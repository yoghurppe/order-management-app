import streamlit as st

# âœ… ãƒšãƒ¼ã‚¸è¨­å®šã‚’è¿½åŠ 
st.set_page_config(
    page_title="ç®¡ç†è£œåŠ©ã‚·ã‚¹ãƒ†ãƒ ",
    layout="wide",                 # æ¨ªå¹…ã‚’æœ€å¤§åŒ–
    initial_sidebar_state="expanded"
)

import pandas as pd
import requests
import datetime
import os
import math
import re
import hashlib
import time
from zoneinfo import ZoneInfo
from streamlit_javascript import st_javascript

# ã“ã“ã« parse_items_fixed ã‚’è¿½åŠ 
def parse_items_fixed(text):
    items = []
    lines = text.strip().splitlines()
    item = {}

    def normalize_number(s):
        table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')
        return s.translate(table).strip()

    for line in lines:
        line = line.strip()

        if "å“ç•ª" in line:
            item = {'å“ç•ª': line.split("å“ç•ª")[-1].strip()}

        elif "JAN" in line:
            item['jan'] = line.split("JAN")[-1].strip()

        elif re.search(r'ï¼ˆ[\d,]+å†† Ã— \d+ç‚¹ï¼‰', line):
            m = re.search(r'ï¼ˆ([\d,]+)å†† Ã— (\d+)ç‚¹ï¼‰', line)
            item['å˜ä¾¡'] = int(m.group(1).replace(',', ''))
            item['ãƒ­ãƒƒãƒˆ'] = int(m.group(2))

        elif normalize_number(line).isdigit() and 'æ•°é‡' not in item:
            item['æ•°é‡'] = int(normalize_number(line))

            if all(k in item for k in ['å“ç•ª', 'jan', 'å˜ä¾¡', 'ãƒ­ãƒƒãƒˆ', 'æ•°é‡']):
                item['ãƒ­ãƒƒãƒˆÃ—æ•°é‡'] = item['ãƒ­ãƒƒãƒˆ'] * item['æ•°é‡']
                items.append(item)
                item = {}

    df = pd.DataFrame(items)

    if not df.empty:
        df['å°è¨ˆ'] = df['å˜ä¾¡'] * df['ãƒ­ãƒƒãƒˆ'] * df['æ•°é‡']
        subtotal = df['å°è¨ˆ'].sum()

        df.loc[len(df)] = {
            'å“ç•ª': 'åˆè¨ˆ',
            'jan': '',
            'å˜ä¾¡': '',
            'ãƒ­ãƒƒãƒˆ': '',
            'æ•°é‡': '',
            'ãƒ­ãƒƒãƒˆÃ—æ•°é‡': '',
            'å°è¨ˆ': subtotal
        }

    return df
    
# ğŸŸ¢ ã“ã“ã‹ã‚‰ã‚¢ãƒ—ãƒªã®ä¸­èº«ï¼ˆè¨€èªé¸æŠãªã©ï¼‰
language = st.sidebar.selectbox("è¨€èª / Language", ["æ—¥æœ¬èª", "ä¸­æ–‡"], key="language")

# ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡¨ç¤ºç”¨ãƒ©ãƒ™ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
TEXT = {
    "æ—¥æœ¬èª": {
        "title_order_ai": "ç®¡ç†è£œåŠ©ã‚·ã‚¹ãƒ†ãƒ ",
        "mode_select": "ãƒ¢ãƒ¼ãƒ‰ã‚’é¸ã‚“ã§ãã ã•ã„",
        "upload_csv": "CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        "order_ai": "ç™ºæ³¨AIåˆ¤å®š",
        "search_item": "å•†å“æƒ…å ±æ¤œç´¢",
        "upload_item": "å•†å“æƒ…å ±CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        "price_improve": "ä»•å…¥ä¾¡æ ¼æ”¹å–„ãƒªã‚¹ãƒˆ",
        "search_keyword": "å•†å“åãƒ»å•†å“ã‚³ãƒ¼ãƒ‰ã§æ¤œç´¢",
        "search_brand": "ãƒ¡ãƒ¼ã‚«ãƒ¼åã§çµã‚Šè¾¼ã¿",
        "search_type": "å–æ‰±åŒºåˆ†ã§çµã‚Šè¾¼ã¿",
        "product_list": "å•†å“ä¸€è¦§",
        "search_keyword": "å•†å“åãƒ»å•†å“ã‚³ãƒ¼ãƒ‰ã§æ¤œç´¢",
        "search_brand": "ãƒ¡ãƒ¼ã‚«ãƒ¼åã§çµã‚Šè¾¼ã¿",
        "search_type": "å–æ‰±åŒºåˆ†ã§çµã‚Šè¾¼ã¿",
        "search_rank": "ãƒ©ãƒ³ã‚¯ã§çµã‚Šè¾¼ã¿",
        "search_code": "å•†å“ã‚³ãƒ¼ãƒ‰ / JAN",
        "all": "ã™ã¹ã¦",
        "loading_data": "ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...",
        "multi_jan": "è¤‡æ•°JANå…¥åŠ›ï¼ˆæ”¹è¡Œã¾ãŸã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰"
    },
    "ä¸­æ–‡": {
        "title_order_ai": "ç®¡ç†æ”¯æŒç³»ç»Ÿ",
        "mode_select": "è¯·é€‰æ‹©æ¨¡å¼",
        "upload_csv": "ä¸Šä¼ CSV",
        "order_ai": "è®¢è´§AIåˆ¤æ–­",
        "search_item": "å•†å“ä¿¡æ¯æŸ¥è¯¢",
        "upload_item": "ä¸Šä¼ å•†å“ä¿¡æ¯CSV",
        "price_improve": "è¿›è´§ä»·æ ¼ä¼˜åŒ–æ¸…å•",
        "search_keyword": "æŒ‰å•†å“åç§°æˆ–ç¼–å·æœç´¢",
        "search_brand": "æŒ‰å“ç‰Œç­›é€‰",
        "search_type": "æŒ‰åˆ†ç±»ç­›é€‰",
        "product_list": "å•†å“åˆ—è¡¨",
        "search_keyword": "æŒ‰å•†å“åç§°æˆ–ç¼–å·æœç´¢",
        "search_brand": "æŒ‰åˆ¶é€ å•†ç­›é€‰",
        "search_type": "æŒ‰åˆ†ç±»ç­›é€‰",
        "search_rank": "æŒ‰ç­‰çº§ç­›é€‰",
        "search_code": "å•†å“ç¼–å· / æ¡ç ",
        "all": "å…¨éƒ¨",
        "loading_data": "ğŸ“Š æ­£åœ¨è¯»å–æ•°æ®...",
        "multi_jan": "æ‰¹é‡è¾“å…¥æ¡ç ï¼ˆæ¢è¡Œæˆ–é€—å·åˆ†éš”ï¼‰"
    }
}

# åˆ—åãƒãƒƒãƒ”ãƒ³ã‚°
COLUMN_NAMES = {
    "æ—¥æœ¬èª": {
        "å•†å“ã‚³ãƒ¼ãƒ‰": "å•†å“ã‚³ãƒ¼ãƒ‰",
        "jan": "JAN",
        "ãƒ©ãƒ³ã‚¯": "ãƒ©ãƒ³ã‚¯",
        "ãƒ¡ãƒ¼ã‚«ãƒ¼å": "ãƒ¡ãƒ¼ã‚«ãƒ¼å",
        "å•†å“å": "å•†å“å",
        "å–æ‰±åŒºåˆ†": "å–æ‰±åŒºåˆ†",
        "åœ¨åº«": "åœ¨åº«",
        "åˆ©ç”¨å¯èƒ½": "åˆ©ç”¨å¯èƒ½",
        "ç™ºæ³¨æ¸ˆ": "ç™ºæ³¨æ¸ˆ",
        "ä»•å…¥ä¾¡æ ¼": "ä»•å…¥ä¾¡æ ¼",
        "ã‚±ãƒ¼ã‚¹å…¥æ•°": "ã‚±ãƒ¼ã‚¹å…¥æ•°",
        "ç™ºæ³¨ãƒ­ãƒƒãƒˆ": "ç™ºæ³¨ãƒ­ãƒƒãƒˆ",
        "é‡é‡": "é‡é‡(g)",
        "å®Ÿç¸¾åŸä¾¡": "å®Ÿç¸¾åŸä¾¡",
        "æœ€å®‰åŸä¾¡": "æœ€å®‰åŸä¾¡"
    },
    "ä¸­æ–‡": {
        "å•†å“ã‚³ãƒ¼ãƒ‰": "å•†å“ç¼–å·",
        "jan": "æ¡ç ",
        "ãƒ©ãƒ³ã‚¯": "ç­‰çº§",
        "ãƒ¡ãƒ¼ã‚«ãƒ¼å": "åˆ¶é€ å•†åç§°",
        "å•†å“å": "å•†å“åç§°",
        "å–æ‰±åŒºåˆ†": "åˆ†ç±»",
        "åœ¨åº«": "åº“å­˜",
        "åˆ©ç”¨å¯èƒ½": "å¯ç”¨åº“å­˜",
        "ç™ºæ³¨æ¸ˆ": "å·²è®¢è´­",
        "ä»•å…¥ä¾¡æ ¼": "è¿›è´§ä»·",
        "ã‚±ãƒ¼ã‚¹å…¥æ•°": "ç®±å…¥æ•°",
        "ç™ºæ³¨ãƒ­ãƒƒãƒˆ": "è®¢è´­å•ä½",
        "é‡é‡": "é‡é‡(g)",
        "å®Ÿç¸¾åŸä¾¡": "å®é™…æˆæœ¬",
        "æœ€å®‰åŸä¾¡": "æœ€ä½æˆæœ¬"
    }
}

# ğŸ” Supabaseæ¥ç¶šè¨­å®š
SUPABASE_URL_PRE = st.secrets["SUPABASE_URL"]
SUPABASE_KEY_PRE = st.secrets["SUPABASE_KEY"]
HEADERS_PRE = {
    "apikey": SUPABASE_KEY_PRE,
    "Authorization": f"Bearer {SUPABASE_KEY_PRE}",
    "Content-Type": "application/json"
}

# ğŸ“… item_master ã®æœ€æ–°æ›´æ–°æ—¥æ™‚ã‚’ JST è¡¨ç¤ºã§å–å¾—
def fetch_latest_item_update():
    url = f"{SUPABASE_URL_PRE}/rest/v1/item_master?select=updated_at&order=updated_at.desc&limit=1"
    res = requests.get(url, headers=HEADERS_PRE)
    if res.status_code == 200 and res.json():
        dt = pd.to_datetime(res.json()[0]["updated_at"], errors="coerce", utc=True)
        if pd.notnull(dt):
            dt_jst = dt.tz_convert(ZoneInfo("Asia/Tokyo"))
            return f"ï¼ˆ{dt_jst.strftime('%-m.%d update')}ï¼‰"
    return ""

def fetch_table(table_name):
    headers = {**HEADERS_PRE, "Prefer": "count=exact"}
    dfs = []
    offset = 0
    limit = 1000
    while True:
        url = f"{SUPABASE_URL_PRE}/rest/v1/{table_name}?select=*&offset={offset}&limit={limit}"
        res = requests.get(url, headers=headers)
        if res.status_code == 416 or not res.json():
            break
        if res.status_code not in [200, 206]:
            st.error(f"{table_name} ã®å–å¾—ã«å¤±æ•—: {res.status_code} / {res.text}")
            return pd.DataFrame()
        dfs.append(pd.DataFrame(res.json()))
        offset += limit
    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

item_master_update_text = fetch_latest_item_update()

# ã‚¿ã‚¤ãƒˆãƒ«è¡¨ç¤º
st.title(TEXT[language]["title_order_ai"])

MODE_KEYS = {
    "home": {
        "æ—¥æœ¬èª": "ğŸ  ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸",
        "ä¸­æ–‡": "ğŸ  ä¸»é¡µ"
    },
    "search_item": {
        "æ—¥æœ¬èª": f"ğŸ” å•†å“æƒ…å ±æ¤œç´¢{item_master_update_text}",
        "ä¸­æ–‡": f"ğŸ” å•†å“ä¿¡æ¯æŸ¥è¯¢{item_master_update_text}"
    },
    "price_improve": {
        "æ—¥æœ¬èª": "ä»•å…¥ä¾¡æ ¼æ”¹å–„ãƒªã‚¹ãƒˆ",
        "ä¸­æ–‡": "è¿›è´§ä»·æ ¼ä¼˜åŒ–æ¸…å•"
    },
    "monthly_sales": {
        "æ—¥æœ¬èª": "è²©å£²å®Ÿç¸¾ï¼ˆç›´è¿‘1ãƒ¶æœˆï¼‰",
        "ä¸­æ–‡": "é”€å”®ä¸šç»©ï¼ˆæœ€è¿‘ä¸€ä¸ªæœˆï¼‰"
    },
    "order_ai": {
        "æ—¥æœ¬èª": "ç™ºæ³¨AIåˆ¤å®š",
        "ä¸­æ–‡": "è®¢è´§AIåˆ¤æ–­"
    },
    "rank_check": {
        "æ—¥æœ¬èª": "ABCãƒ©ãƒ³ã‚¯å•†å“ç¢ºèª",
        "ä¸­æ–‡": "ABCç­‰çº§å•†å“æ£€æŸ¥"
    },
    "purchase_history": {
        "æ—¥æœ¬èª": "ğŸ“œ ç™ºæ³¨å±¥æ­´",
        "ä¸­æ–‡": "ğŸ“œ è®¢è´§è®°å½•"
    },
    "difficult_items": {
        "æ—¥æœ¬èª": "å…¥è·å›°é›£å•†å“",
        "ä¸­æ–‡": "è¿›è´§å›°éš¾å•†å“"
    },
    "csv_upload": {
        "æ—¥æœ¬èª": "CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        "ä¸­æ–‡": "ä¸Šä¼ CSV"
    },
    "order": {
        "æ—¥æœ¬èª": "ğŸ“¦ ç™ºæ³¨æ›¸ä½œæˆãƒ¢ãƒ¼ãƒ‰",
        "ä¸­æ–‡": "ğŸ“¦ è®¢å•ä¹¦ç”Ÿæˆæ¨¡å¼"
    },
}


mode_labels = [v[language] for v in MODE_KEYS.values()]
mode_selection = st.sidebar.radio(TEXT[language]["mode_select"], mode_labels, index=0)
mode = next(key for key, labels in MODE_KEYS.items() if labels[language] == mode_selection)


# å„ãƒ¢ãƒ¼ãƒ‰ã®å‡¦ç†åˆ†å²
if mode == "home":
    st.subheader("ğŸ  " + TEXT[language]["title_order_ai"])

    if language == "æ—¥æœ¬èª":
        st.markdown("""
        #### ã”åˆ©ç”¨ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚
        å·¦ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰æ“ä½œã‚’é¸ã‚“ã§ãã ã•ã„ã€‚
        - ğŸ“¦ ç™ºæ³¨AI
        - ğŸ“¤ CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        - ğŸ” å•†å“æƒ…å ±æ¤œç´¢
        - ğŸ’° ä»•å…¥ä¾¡æ ¼æ”¹å–„ãƒªã‚¹ãƒˆ
        """)
    else:
        st.markdown("""
        #### æ„Ÿè°¢æ‚¨çš„ä½¿ç”¨ã€‚
        è¯·ä»å·¦ä¾§èœå•ä¸­é€‰æ‹©æ“ä½œæ¨¡å¼ã€‚
        - ğŸ“¦ è®¢è´§AI
        - ğŸ“¤ ä¸Šä¼ CSV
        - ğŸ” å•†å“ä¿¡æ¯æŸ¥è¯¢
        - ğŸ’° è¿›è´§ä»·æ ¼ä¼˜åŒ–æ¸…å•
        """)

elif mode == "order_ai":
    st.subheader("ğŸ“¦ ç™ºæ³¨AIãƒ¢ãƒ¼ãƒ‰")

    ai_mode = st.radio("ç™ºæ³¨AIãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠ", ["é€šå¸¸ãƒ¢ãƒ¼ãƒ‰", "JDãƒ¢ãƒ¼ãƒ‰"], index=0)

    if st.button("ğŸ¤– è¨ˆç®—ã‚’é–‹å§‹ã™ã‚‹"):
        SUPABASE_URL = st.secrets["SUPABASE_URL"]
        SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
        HEADERS = {
            "apikey": SUPABASE_KEY,
            "Authorization": f"Bearer {SUPABASE_KEY}",
            "Content-Type": "application/json"
        }

        def fetch_table(table_name):
            headers = {**HEADERS, "Prefer": "count=exact"}
            dfs = []
            offset = 0
            limit = 1000
            while True:
                url = f"{SUPABASE_URL}/rest/v1/{table_name}?select=*&offset={offset}&limit={limit}"
                res = requests.get(url, headers=headers)
                if res.status_code == 416 or not res.json():
                    break
                if res.status_code not in [200, 206]:
                    st.error(f"{table_name} ã®å–å¾—ã«å¤±æ•—: {res.status_code} / {res.text}")
                    return pd.DataFrame()
                dfs.append(pd.DataFrame(res.json()))
                offset += limit
            return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

        def normalize_jan(x):
            try:
                return str(x).strip()
            except:
                return ""

        with st.spinner("ğŸ“¦ ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
            df_sales = fetch_table("sales")
            df_purchase = fetch_table("purchase_data")
            df_master = fetch_table("item_master")
            if ai_mode == "JDãƒ¢ãƒ¼ãƒ‰":
                df_warehouse = fetch_table("warehouse_stock")

        if df_sales.empty or df_purchase.empty or df_master.empty:
            st.warning("å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
            st.stop()
        if ai_mode == "JDãƒ¢ãƒ¼ãƒ‰" and df_warehouse.empty:
            st.warning("JDãƒ¢ãƒ¼ãƒ‰ç”¨ã® warehouse_stock ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
            st.stop()

        df_sales["jan"] = df_sales["jan"].apply(normalize_jan)
        df_purchase["jan"] = df_purchase["jan"].apply(normalize_jan)
        df_master["jan"] = df_master["jan"].apply(normalize_jan)
        if ai_mode == "JDãƒ¢ãƒ¼ãƒ‰":
            df_warehouse["product_code"] = df_warehouse["product_code"].apply(normalize_jan)
            df_warehouse["stock_available"] = pd.to_numeric(df_warehouse["stock_available"], errors="coerce").fillna(0).astype(int)

        df_sales["quantity_sold"] = pd.to_numeric(df_sales["quantity_sold"], errors="coerce").fillna(0).astype(int)
        df_sales["stock_available"] = pd.to_numeric(df_sales["stock_available"], errors="coerce").fillna(0).astype(int)

        df_history = fetch_table("purchase_history")
        df_history["quantity"] = pd.to_numeric(df_history["quantity"], errors="coerce").fillna(0).astype(int)
        df_history["memo"] = df_history["memo"].astype(str).fillna("")
        df_history["jan"] = df_history["jan"].apply(normalize_jan)

        # ğŸ”„ ã€Œä¸Šæµ·ã€ã‚’å«ã‚€ç™ºæ³¨å±¥æ­´ã‚’é™¤å¤–å¯¾è±¡ã¨ã—ã¦é›†è¨ˆ
        df_shanghai = df_history[df_history["memo"].str.contains("ä¸Šæµ·", na=False)]
        df_shanghai_grouped = df_shanghai.groupby("jan")["quantity"].sum().reset_index(name="shanghai_quantity")

        # item_master ã®ç™ºæ³¨æ¸ˆã«ã€Œä¸Šæµ·ã€åˆ†ã‚’å·®ã—å¼•ã„ãŸåˆ—ã‚’è¿½åŠ 
        df_master = df_master.merge(df_shanghai_grouped, on="jan", how="left")
        df_master["shanghai_quantity"] = df_master["shanghai_quantity"].fillna(0).astype(int)
        df_master["ç™ºæ³¨æ¸ˆ_ä¿®æ­£å¾Œ"] = (df_master["ç™ºæ³¨æ¸ˆ"] - df_master["shanghai_quantity"]).clip(lower=0)

        # df_sales å´ã«ç™ºæ³¨æ¸ˆã‚’å†ãƒãƒ¼ã‚¸ï¼ˆå…ƒã‚³ãƒ¼ãƒ‰ã‚’ä¸Šæ›¸ãã™ã‚‹å½¢ã§ï¼‰
        df_sales.drop(columns=["ç™ºæ³¨æ¸ˆ"], errors="ignore", inplace=True)
        df_sales = df_sales.merge(df_master[["jan", "ç™ºæ³¨æ¸ˆ_ä¿®æ­£å¾Œ"]], on="jan", how="left")
        df_sales["ç™ºæ³¨æ¸ˆ"] = df_sales["ç™ºæ³¨æ¸ˆ_ä¿®æ­£å¾Œ"].fillna(0).astype(int)

        df_sales["ç™ºæ³¨æ¸ˆ"] = df_sales["ç™ºæ³¨æ¸ˆ"].fillna(0).astype(int)

        df_purchase["order_lot"] = pd.to_numeric(df_purchase["order_lot"], errors="coerce").fillna(0).astype(int)
        df_purchase["price"] = pd.to_numeric(df_purchase["price"], errors="coerce").fillna(0)

        rank_multiplier = {
            "Aãƒ©ãƒ³ã‚¯": 1.0,
            "Bãƒ©ãƒ³ã‚¯": 1.2,
            "Cãƒ©ãƒ³ã‚¯": 1.0,
            "TEST": 1.5
        }

        from datetime import date, timedelta
        import math

        with st.spinner("ğŸ¤– ç™ºæ³¨AIãŒè¨ˆç®—ã‚’ã—ã¦ã„ã¾ã™..."):
            df_history = fetch_table("purchase_history")
            df_history["order_date"] = pd.to_datetime(df_history["order_date"], errors="coerce").dt.date
            today = date.today()
            yesterday = today - timedelta(days=1)
            recent_jans = df_history[df_history["order_date"].isin([today, yesterday])]["jan"].dropna().astype(str).apply(normalize_jan).unique().tolist()

            results = []
            for _, row in df_sales.iterrows():
                jan = row["jan"]
                sold = row["quantity_sold"]

                if ai_mode == "JDãƒ¢ãƒ¼ãƒ‰":
                    stock_row = df_warehouse[df_warehouse["product_code"] == jan]
                    stock = stock_row["stock_available"].values[0] if not stock_row.empty else 0
                else:
                    stock = row.get("stock_available", 0)

                ordered = row["ç™ºæ³¨æ¸ˆ"]

                rank_row = df_master[df_master["jan"] == jan]
                rank = rank_row["ãƒ©ãƒ³ã‚¯"].values[0] if not rank_row.empty and "ãƒ©ãƒ³ã‚¯" in rank_row else ""
                multiplier = rank_multiplier.get(rank, 1.0)

                if rank == "Aãƒ©ãƒ³ã‚¯":
                    if (stock + ordered) < sold:
                        need_qty_raw = math.ceil(sold * 1.2)
                    else:
                        need_qty_raw = 0
                else:
                    need_qty_raw = math.ceil(sold * multiplier) - stock - ordered

                if stock <= 1 and sold >= 1 and need_qty_raw <= 0:
                    need_qty = 1
                else:
                    need_qty = max(need_qty_raw, 0)

                if jan in recent_jans:
                    continue

                if rank == "Aãƒ©ãƒ³ã‚¯":
                    reorder_point = max(math.floor(sold * 1.0), 1)
                elif rank == "Bãƒ©ãƒ³ã‚¯":
                    reorder_point = max(math.floor(sold * 0.9), 1)
                else:
                    reorder_point = max(math.floor(sold * 0.7), 1)

                current_total = stock + ordered
                if current_total > reorder_point:
                    continue
                if need_qty <= 0:
                    continue

                options = df_purchase[df_purchase["jan"] == jan].copy()
                if options.empty:
                    continue
                options = options[options["order_lot"] > 0]

                if rank == "Aãƒ©ãƒ³ã‚¯":
                    bigger_lots = options[options["order_lot"] >= need_qty]
                    if not bigger_lots.empty:
                        best_option = bigger_lots.sort_values("order_lot", ascending=False).iloc[0]
                    else:
                        best_option = options.sort_values("order_lot", ascending=False).iloc[0]
                else:
                    options["diff"] = (options["order_lot"] - need_qty).abs()
                    smaller_lots = options[options["order_lot"] <= need_qty]
                    if not smaller_lots.empty:
                        best_option = smaller_lots.loc[smaller_lots["diff"].idxmin()]
                    else:
                        near_lots = options[(options["order_lot"] > need_qty) & (options["order_lot"] <= need_qty * 1.5) & (options["order_lot"] != 1)]
                        if not near_lots.empty:
                            best_option = near_lots.loc[near_lots["diff"].idxmin()]
                        else:
                            one_lot = options[options["order_lot"] == 1]
                            if not one_lot.empty:
                                best_option = one_lot.iloc[0]
                            else:
                                best_option = options.sort_values("order_lot").iloc[0]

                if rank == "Aãƒ©ãƒ³ã‚¯":
                    sets = math.ceil(need_qty_raw / best_option["order_lot"])
                else:
                    sets = math.ceil(need_qty / best_option["order_lot"])
                qty = sets * best_option["order_lot"]
                total_cost = qty * best_option["price"]

                results.append({
                    "jan": jan,
                    "è²©å£²å®Ÿç¸¾": sold,
                    "åœ¨åº«": stock,
                    "ç™ºæ³¨æ¸ˆ": ordered,
                    "ç†è«–å¿…è¦æ•°": need_qty_raw if rank == "Aãƒ©ãƒ³ã‚¯" else need_qty,
                    "ç™ºæ³¨æ•°": qty,
                    "ãƒ­ãƒƒãƒˆ": best_option["order_lot"],
                    "æ•°é‡": round(qty / best_option["order_lot"], 2),
                    "å˜ä¾¡": best_option["price"],
                    "ç·é¡": total_cost,
                    "ä»•å…¥å…ˆ": best_option.get("supplier", "ä¸æ˜"),
                    "ãƒ©ãƒ³ã‚¯": rank
                })

            if results:
                result_df = pd.DataFrame(results)
                df_master["å•†å“ã‚³ãƒ¼ãƒ‰"] = df_master["å•†å“ã‚³ãƒ¼ãƒ‰"].astype(str).str.strip()
                result_df["jan"] = result_df["jan"].astype(str).str.strip()
                df_temp = df_master[["å•†å“ã‚³ãƒ¼ãƒ‰", "å•†å“å", "å–æ‰±åŒºåˆ†"]].copy()
                df_temp.rename(columns={"å•†å“ã‚³ãƒ¼ãƒ‰": "jan"}, inplace=True)
                result_df = pd.merge(result_df, df_temp, on="jan", how="left")

                # âœ… å¼å¤©åœ¨åº«ã‚’çµåˆï¼ˆè¡¨ç¤ºã®ã¿ï¼‰
                df_benten = fetch_table("benten_stock")
                df_benten["jan"] = df_benten["jan"].astype(str).str.strip()
                df_benten = df_benten[["jan", "stock"]].rename(columns={"stock": "å¼å¤©åœ¨åº«"})
                result_df = pd.merge(result_df, df_benten, on="jan", how="left")
                result_df["å¼å¤©åœ¨åº«"] = result_df["å¼å¤©åœ¨åº«"].fillna(0).astype(int)

                # âœ… åœ¨åº« â†’ JDåœ¨åº« ã«åˆ—åå¤‰æ›´
                result_df.rename(columns={"åœ¨åº«": "JDåœ¨åº«"}, inplace=True)

                # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if "å•†å“å" in result_df.columns:
                    result_df = result_df[result_df["å•†å“å"].notna()]
                if "å–æ‰±åŒºåˆ†" in result_df.columns:
                    result_df = result_df[result_df["å–æ‰±åŒºåˆ†"] != "å–æ‰±ä¸­æ­¢"]
                else:
                    st.warning("âš ï¸ã€å–æ‰±åŒºåˆ†ã€åˆ—ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")

                # è¡¨ç¤ºé †ã«ä¸¦ã¹æ›¿ãˆï¼ˆJDåœ¨åº«ãƒ»å¼å¤©åœ¨åº«ã‚’å«ã‚€ï¼‰
                column_order = ["jan", "å•†å“å", "ãƒ©ãƒ³ã‚¯", "è²©å£²å®Ÿç¸¾", "JDåœ¨åº«", "å¼å¤©åœ¨åº«", "ç™ºæ³¨æ¸ˆ",
                                "ç†è«–å¿…è¦æ•°", "ç™ºæ³¨æ•°", "ãƒ­ãƒƒãƒˆ", "æ•°é‡", "å˜ä¾¡", "ç·é¡", "ä»•å…¥å…ˆ"]
                result_df = result_df[[col for col in column_order if col in result_df.columns]]

                st.success(f"âœ… ç™ºæ³¨å¯¾è±¡: {len(result_df)} ä»¶")
                st.dataframe(result_df)

                csv = result_df.to_csv(index=False).encode("utf-8-sig")
                st.download_button("ğŸ“¥ ç™ºæ³¨CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv, file_name="orders_available_based.csv", mime="text/csv")

                st.markdown("---")
                st.subheader("ğŸ“¦ ä»•å…¥å…ˆåˆ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
                for supplier, group in result_df.groupby("ä»•å…¥å…ˆ"):
                    supplier_csv = group.to_csv(index=False).encode("utf-8-sig")
                    st.download_button(
                        label=f"ğŸ“¥ {supplier} ç”¨ ç™ºæ³¨CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=supplier_csv,
                        file_name=f"orders_{supplier}.csv",
                        mime="text/csv"
                    )
            else:
                st.info("ç¾åœ¨ã€ç™ºæ³¨ãŒå¿…è¦ãªå•†å“ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")




# ğŸ” å•†å“æƒ…å ±æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ -----------------------------
elif mode == "search_item":
    st.subheader("ğŸ” å•†å“æƒ…å ±æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰")

    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    HEADERS = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json"
    }

    def fetch_item_master():
        headers = {**HEADERS, "Prefer": "count=exact"}
        dfs = []
        offset, limit = 0, 1000
        while True:
            url = f"{SUPABASE_URL}/rest/v1/item_master?select=*&offset={offset}&limit={limit}"
            res = requests.get(url, headers=headers)
            if res.status_code == 416 or not res.json():
                break
            if res.status_code not in [200, 206]:
                st.error(f"item_master ã®å–å¾—ã«å¤±æ•—: {res.status_code} / {res.text}")
                return pd.DataFrame()
            dfs.append(pd.DataFrame(res.json()))
            offset += limit
        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

    def fetch_warehouse_stock():
        headers = {**HEADERS, "Prefer": "count=exact"}
        dfs = []
        offset, limit = 0, 1000
        while True:
            url = f"{SUPABASE_URL}/rest/v1/warehouse_stock?select=*&offset={offset}&limit={limit}"
            res = requests.get(url, headers=headers)
            if res.status_code == 416 or not res.json():
                break
            if res.status_code not in [200, 206]:
                st.error(f"warehouse_stock ã®å–å¾—ã«å¤±æ•—: {res.status_code} / {res.text}")
                return pd.DataFrame()
            dfs.append(pd.DataFrame(res.json()))
            offset += limit
        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

    df_master = fetch_item_master()
    df_warehouse = fetch_warehouse_stock()

    if df_master.empty:
        st.warning("å•†å“æƒ…å ±ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        st.stop()

    df_master["jan"] = df_master["jan"].astype(str)
    df_master["å•†å“ã‚³ãƒ¼ãƒ‰"] = df_master["å•†å“ã‚³ãƒ¼ãƒ‰"].astype(str)
    df_master["å•†å“å"] = df_master["å•†å“å"].astype(str)

    df_warehouse["product_code"] = df_warehouse["product_code"].astype(str)
    df_warehouse["stock_available"] = pd.to_numeric(df_warehouse["stock_available"], errors="coerce").fillna(0).astype(int)
    df_warehouse["stock_total"] = df_warehouse["stock_available"]

    df_master = df_master.merge(
        df_warehouse[["product_code", "stock_total", "stock_available"]],
        left_on="jan", right_on="product_code",
        how="left"
    )
    df_master["åœ¨åº«"] = df_master["stock_total"].fillna(0).astype(int)
    df_master["åˆ©ç”¨å¯èƒ½"] = df_master["stock_available"].fillna(0).astype(int)

    # æ–°ã—ã„ä¾¡æ ¼åˆ—ã®è¿½åŠ 
    df_master["å®Ÿç¸¾åŸä¾¡"] = pd.to_numeric(df_master["average_cost"], errors="coerce").fillna(0).astype(int)
    df_master["æœ€å®‰åŸä¾¡"] = pd.to_numeric(df_master["purchase_cost"], errors="coerce").fillna(0).astype(int)

    col1, col2 = st.columns(2)
    with col1:
        keyword_name = st.text_input(TEXT[language]["search_keyword"], "")
        keyword_code = st.text_input(TEXT[language]["search_code"], "")

    with col2:
        jan_filter_multi = st.text_area(
            TEXT[language]["multi_jan"],
            placeholder="ä¾‹:\n4901234567890\n4987654321098",
            height=120,
        )

    maker_filter = st.selectbox(
        TEXT[language]["search_brand"],
        [TEXT[language]["all"]] + sorted(df_master["ãƒ¡ãƒ¼ã‚«ãƒ¼å"].dropna().unique())
    )
    rank_filter = st.selectbox(
        TEXT[language]["search_rank"],
        [TEXT[language]["all"]] + sorted(df_master["ãƒ©ãƒ³ã‚¯"].dropna().unique())
    )
    type_filter = st.selectbox(
        TEXT[language]["search_type"],
        [TEXT[language]["all"]] + sorted(df_master["å–æ‰±åŒºåˆ†"].dropna().unique())
    )

    jan_list = [j.strip() for j in re.split(r"[,\n\r]+", jan_filter_multi) if j.strip()]
    df_view = df_master.copy()

    if jan_list:
        df_view = df_view[df_view["jan"].isin(jan_list)]
    elif keyword_code:
        df_view = df_view[
            df_view["å•†å“ã‚³ãƒ¼ãƒ‰"].str.contains(keyword_code, case=False, na=False) |
            df_view["jan"].str.contains(keyword_code, case=False, na=False)
        ]
    if keyword_name:
        df_view = df_view[df_view["å•†å“å"].str.contains(keyword_name, case=False, na=False)]
    if maker_filter != TEXT[language]["all"]:
        df_view = df_view[df_view["ãƒ¡ãƒ¼ã‚«ãƒ¼å"] == maker_filter]
    if rank_filter != TEXT[language]["all"]:
        df_view = df_view[df_view["ãƒ©ãƒ³ã‚¯"] == rank_filter]
    if type_filter != TEXT[language]["all"]:
        df_view = df_view[df_view["å–æ‰±åŒºåˆ†"] == type_filter]

    view_cols = [
        "å•†å“ã‚³ãƒ¼ãƒ‰", "jan", "ãƒ©ãƒ³ã‚¯", "ãƒ¡ãƒ¼ã‚«ãƒ¼å", "å•†å“å", "å–æ‰±åŒºåˆ†",
        "åœ¨åº«", "ç™ºæ³¨æ¸ˆ", "å®Ÿç¸¾åŸä¾¡", "æœ€å®‰åŸä¾¡", "ã‚±ãƒ¼ã‚¹å…¥æ•°", "ç™ºæ³¨ãƒ­ãƒƒãƒˆ", "é‡é‡"
    ]
    available_cols = [c for c in view_cols if c in df_view.columns]

    display_df = (
        df_view[available_cols]
        .sort_values(by="å•†å“ã‚³ãƒ¼ãƒ‰")
        .rename(columns=COLUMN_NAMES[language])
    )

    row_count = len(display_df)
    h_left, h_right = st.columns([1, 0.15])
    h_left.subheader(TEXT[language]["product_list"])
    h_right.markdown(
        f"<h4 style='text-align:right; margin-top: 0.6em;'>{row_count:,}ä»¶</h4>",
        unsafe_allow_html=True
    )

    st.dataframe(display_df, use_container_width=True)

    csv = display_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        "ğŸ“… CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv,
        file_name="item_master_filtered.csv",
        mime="text/csv",
    )


elif mode == "purchase_history":
    st.subheader("ğŸ“œ ç™ºæ³¨å±¥æ­´")

    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    HEADERS = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json"
    }

    # ---------- ğŸ” æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ  ----------
    col1, col2 = st.columns(2)

    with col1:
        # å¾“æ¥ã®å˜ä¸€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰
        jan_filter_single = st.text_input("ğŸ” JANã§æ¤œç´¢ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", "")
        order_id_filter   = st.text_input("ğŸ” Orderâ€¯IDã§æ¤œç´¢ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", "")

    with col2:
        jan_filter_multi = st.text_area(
            TEXT[language]["multi_jan"],                # â†å‹•çš„ãƒ©ãƒ™ãƒ«
            placeholder="ä¾‹:\n4901234567890\n4987654321098",
            height=120,
        )

    @st.cache_data(ttl=60)
    def fetch_purchase_history():
        url = f"{SUPABASE_URL}/rest/v1/purchase_history?select=*"
        res = requests.get(url, headers=HEADERS)
        if res.status_code == 200:
            return pd.DataFrame(res.json())
        st.error("âŒ ç™ºæ³¨å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return pd.DataFrame()

    df = fetch_purchase_history()

    if df.empty:
        st.info("ç™ºæ³¨å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        st.stop()

    # ------------- ğŸ§¹ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° -------------
    import re

    df["jan"]        = df["jan"].astype(str)
    df["order_date"] = pd.to_datetime(df["order_date"], errors="coerce").dt.date

    # â‘  è¤‡æ•°â€¯JAN ãƒªã‚¹ãƒˆã‚’æ•´å½¢
    jan_list = [j.strip() for j in re.split(r"[,\n\r]+", jan_filter_multi) if j.strip()]

    if jan_list:  # æœ€å„ªå…ˆ
        df = df[df["jan"].isin(jan_list)]
    elif jan_filter_single:
        df = df[df["jan"].str.contains(jan_filter_single, na=False)]

    if order_id_filter:
        df = df[df["order_id"].astype(str).str.contains(order_id_filter, na=False)]

    # ------------- ğŸ“‹ è¡¨ç¤º -------------
    df_show = df[["jan", "quantity", "order_date", "order_id"]].sort_values("jan")

    st.success(f"âœ… ç™ºæ³¨å±¥æ­´ ä»¶æ•°: {len(df_show)} ä»¶")
    st.dataframe(df_show, use_container_width=True)




elif mode == "price_improve":
    st.subheader("ğŸ’° " + TEXT[language]["price_improve"])

    # èªè¨¼ç”¨ãƒ˜ãƒƒãƒ€ãƒ¼å®šç¾©
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    HEADERS = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json"
    }

    def fetch_table(table_name):
        headers = {**HEADERS, "Prefer": "count=exact"}
        dfs = []
        offset = 0
        limit = 1000
        while True:
            url = f"{SUPABASE_URL}/rest/v1/{table_name}?select=*&offset={offset}&limit={limit}"
            res = requests.get(url, headers=headers)
            if res.status_code == 416 or not res.json():
                break
            if res.status_code not in [200, 206]:
                st.error(f"{table_name} ã®å–å¾—ã«å¤±æ•—: {res.status_code} / {res.text}")
                return pd.DataFrame()
            dfs.append(pd.DataFrame(res.json()))
            offset += limit
        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

    with st.spinner("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        df_sales = fetch_table("sales")
        df_purchase = fetch_table("purchase_data")
        df_item = fetch_table("item_master")

    def normalize_jan(x):
        try:
            if re.fullmatch(r"\d+(\.0+)?", str(x)):
                return str(int(float(x)))
            else:
                return str(x).strip()
        except:
            return ""

    # æ•´å½¢
    df_sales["jan"] = df_sales["jan"].apply(normalize_jan)
    df_purchase["jan"] = df_purchase["jan"].apply(normalize_jan)
    df_item["jan"] = df_item["jan"].apply(normalize_jan)
    df_purchase["price"] = pd.to_numeric(df_purchase["price"], errors="coerce").fillna(0)

    # ç¾åœ¨ä¾¡æ ¼åˆ¤å®š
    current_prices = {}
    for _, row in df_sales.iterrows():
        jan = row["jan"]
        sold = row["quantity_sold"]
        stock = row.get("stock_available", 0)
        ordered = row.get("stock_ordered", 0)
        options = df_purchase[df_purchase["jan"] == jan].copy()
        if options.empty:
            continue

        if stock >= sold:
            need_qty = 0
        else:
            need_qty = sold - stock + math.ceil(sold * 0.5) - ordered
            need_qty = max(need_qty, 0)

        if need_qty <= 0:
            continue

        options = options[options["order_lot"] > 0]
        options["diff"] = (options["order_lot"] - need_qty).abs()

        smaller_lots = options[options["order_lot"] <= need_qty]
        if not smaller_lots.empty:
            best_option = smaller_lots.loc[smaller_lots["diff"].idxmin()]
        else:
            near_lots = options[(options["order_lot"] > need_qty) & (options["order_lot"] <= need_qty * 1.5) & (options["order_lot"] != 1)]
            if not near_lots.empty:
                best_option = near_lots.loc[near_lots["diff"].idxmin()]
            else:
                one_lot = options[options["order_lot"] == 1]
                if not one_lot.empty:
                    best_option = one_lot.iloc[0]
                else:
                    best_option = options.sort_values("order_lot").iloc[0]

        current_prices[jan] = best_option["price"]

    # æœ€å®‰å€¤å–å¾—
    min_prices = df_purchase.groupby("jan")["price"].min().to_dict()

    rows = []
    for jan, current_price in current_prices.items():
        if jan in min_prices and min_prices[jan] < current_price:
            item = df_item[df_item["jan"] == jan].head(1)
            if not item.empty:
                row = {
                    "å•†å“ã‚³ãƒ¼ãƒ‰": item.iloc[0].get("item_code", ""),
                    "JAN": jan,
                    "ãƒ¡ãƒ¼ã‚«ãƒ¼å": item.iloc[0].get("brand", ""),
                    "ç¾åœ¨ã®ä»•å…¥ä¾¡æ ¼": current_price,
                    "æœ€å®‰å€¤ã®ä»•å…¥ä¾¡æ ¼": min_prices[jan],
                    "å·®åˆ†": round(min_prices[jan] - current_price, 2)
                }
                rows.append(row)

    if rows:
        df_result = pd.DataFrame(rows)

        # âœ… å¤šè¨€èªã‚«ãƒ©ãƒ åã«å¤‰æ›
        column_translation = {
            "æ—¥æœ¬èª": {
                "å•†å“ã‚³ãƒ¼ãƒ‰": "å•†å“ã‚³ãƒ¼ãƒ‰",
                "JAN": "JAN",
                "ãƒ¡ãƒ¼ã‚«ãƒ¼å": "ãƒ¡ãƒ¼ã‚«ãƒ¼å",
                "ç¾åœ¨ã®ä»•å…¥ä¾¡æ ¼": "ç¾åœ¨ã®ä»•å…¥ä¾¡æ ¼",
                "æœ€å®‰å€¤ã®ä»•å…¥ä¾¡æ ¼": "æœ€å®‰å€¤ã®ä»•å…¥ä¾¡æ ¼",
                "å·®åˆ†": "å·®åˆ†"
            },
            "ä¸­æ–‡": {
                "å•†å“ã‚³ãƒ¼ãƒ‰": "å•†å“ç¼–å·",
                "JAN": "æ¡ç ",
                "ãƒ¡ãƒ¼ã‚«ãƒ¼å": "åˆ¶é€ å•†åç§°",
                "ç¾åœ¨ã®ä»•å…¥ä¾¡æ ¼": "å½“å‰è¿›è´§ä»·",
                "æœ€å®‰å€¤ã®ä»•å…¥ä¾¡æ ¼": "æœ€ä½è¿›è´§ä»·",
                "å·®åˆ†": "å·®é¢"
            }
        }

        df_result = df_result.rename(columns=column_translation[language])

        st.success(f"âœ… æ”¹å–„å¯¾è±¡å•†å“æ•°: {len(df_result)} ä»¶")
        st.dataframe(df_result)

        csv = df_result.to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            "ğŸ“¥ æ”¹å–„ãƒªã‚¹ãƒˆCSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv,
            file_name="price_improvement_list.csv",
            mime="text/csv",
            key="price_improve_download"  # ğŸ”‘ è¤‡æ•°å‘¼ã³å‡ºã—é˜²æ­¢
        )
    else:
        st.info("æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚‹å•†å“ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")


if mode == "csv_upload":
    st.subheader("ğŸ“„ CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰")

    def normalize_jan(x):
        try:
            return str(x).strip()
        except:
            return ""

    input_password = st.text_input("ğŸ”‘ ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", type="password")
    correct_password = st.secrets.get("UPLOAD_PASSWORD", "pass1234")

    if input_password != correct_password:
        st.warning("æ­£ã—ã„ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    HEADERS = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json"
    }

    def preprocess_csv(df, table):
        df.columns = df.columns.str.replace("ã€€", "").str.replace("\ufeff", "").str.strip()

        if table == "sales":
            st.write("ğŸ“ sales åˆ—å:", df.columns.tolist())
            item_col = None
            for col in df.columns:
                if "ã‚¢ã‚¤ãƒ†ãƒ " in col:
                    item_col = col
                    break
            if item_col:
                df.rename(columns={item_col: "jan"}, inplace=True)
            else:
                raise ValueError(f"âŒ 'ã‚¢ã‚¤ãƒ†ãƒ ' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼åˆ—å: {df.columns.tolist()}")

            df.rename(columns={
                "å–æ‰±åŒºåˆ†": "handling_type",
                "è²©å£²æ•°é‡": "quantity_sold",
                "ç¾åœ¨ã®æ‰‹æŒæ•°é‡": "stock_total",
                "ç¾åœ¨ã®åˆ©ç”¨å¯èƒ½æ•°é‡": "stock_available",
                "ç¾åœ¨ã®æ³¨æ–‡æ¸ˆæ•°é‡": "stock_ordered"
            }, inplace=True)

            for col in ["quantity_sold", "stock_total", "stock_available", "stock_ordered"]:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0).astype(int)

            df["jan"] = df["jan"].apply(normalize_jan)

        elif table == "purchase_data":
            for col in ["order_lot", "price"]:
                if col in df.columns:
                    df[col] = df[col].astype(str).str.replace(",", "")
                    df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0)
                    if col == "order_lot":
                        df[col] = df[col].round().astype(int)
            df["jan"] = df["jan"].apply(normalize_jan)

        elif table == "item_master":
            st.write("ğŸ“ item_master åˆ—å:", df.columns.tolist())
            upc_col = None
            for col in df.columns:
                if "UPC" in col:
                    upc_col = col
                    break

            if upc_col:
                df.rename(columns={upc_col: "jan"}, inplace=True)
            else:
                raise ValueError(f"âŒ 'UPCã‚³ãƒ¼ãƒ‰' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼åˆ—å: {df.columns.tolist()}")

            df.rename(columns={
                "è¡¨ç¤ºå": "å•†å“å",
                "ãƒ¡ãƒ¼ã‚«ãƒ¼å": "ãƒ¡ãƒ¼ã‚«ãƒ¼å",
                "ã‚¢ã‚¤ãƒ†ãƒ å®šç¾©åŸä¾¡": "ä»•å…¥ä¾¡æ ¼",
                "ã‚«ãƒ¼ãƒˆãƒ³å…¥æ•°": "ã‚±ãƒ¼ã‚¹å…¥æ•°",
                "ç™ºæ³¨ãƒ­ãƒƒãƒˆ": "ç™ºæ³¨ãƒ­ãƒƒãƒˆ",
                "ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸é‡é‡(g)": "é‡é‡",
                "æ‰‹æŒ": "åœ¨åº«",
                "åˆ©ç”¨å¯èƒ½": "åˆ©ç”¨å¯èƒ½",
                "æ³¨æ–‡æ¸ˆ": "ç™ºæ³¨æ¸ˆ",
                "åå‰": "å•†å“ã‚³ãƒ¼ãƒ‰",
                "å•†å“ãƒ©ãƒ³ã‚¯": "ãƒ©ãƒ³ã‚¯"
            }, inplace=True)

            df.drop(columns=["å†…éƒ¨ID"], inplace=True, errors="ignore")
            df["jan"] = df["jan"].apply(normalize_jan)

            for col in ["ã‚±ãƒ¼ã‚¹å…¥æ•°", "ç™ºæ³¨ãƒ­ãƒƒãƒˆ", "åœ¨åº«", "åˆ©ç”¨å¯èƒ½", "ç™ºæ³¨æ¸ˆ"]:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0).round().astype(int)

        return df

    def upload_file(file, table_name):
        if not file:
            return
        with st.spinner(f"ğŸ“¤ {file.name} ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­..."):
            temp_path = f"/tmp/{file.name}"
            with open(temp_path, "wb") as f:
                f.write(file.read())
            try:
                df = pd.read_csv(
                    temp_path,
                    sep=",",
                    engine="python",
                    on_bad_lines="skip",
                    encoding="utf-8-sig"
                )
                df = preprocess_csv(df, table_name)

                requests.delete(f"{SUPABASE_URL}/rest/v1/{table_name}?id=gt.0", headers=HEADERS)

                if table_name == "purchase_data":
                    df = df.drop_duplicates(subset=["jan", "supplier", "order_lot"], keep="last")
                elif table_name == "item_master":
                    df = df.drop_duplicates(subset=["å•†å“ã‚³ãƒ¼ãƒ‰"], keep="last")
                    if "id" not in df.columns:
                        df.insert(0, "id", range(1, len(df) + 1))
                else:
                    df = df.drop_duplicates(subset=["jan"], keep="last")

                df = df.replace({pd.NA: None, pd.NaT: None, float("nan"): None}).where(pd.notnull(df), None)

                for i in range(0, len(df), 500):
                    batch = df.iloc[i:i+500].to_dict(orient="records")
                    res = requests.post(
                        f"{SUPABASE_URL}/rest/v1/{table_name}",
                        headers={**HEADERS, "Prefer": "resolution=merge-duplicates"},
                        json=batch
                    )
                    if res.status_code not in [200, 201]:
                        st.error(f"âŒ {table_name} ãƒãƒƒãƒPOSTå¤±æ•—: {res.status_code} {res.text}")
                        return

                st.success(f"âœ… {table_name} ã« {len(df)} ä»¶ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")

            except Exception as e:
                st.error(f"âŒ {table_name} ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    sales_file = st.file_uploader("ğŸ“ sales.csv ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type="csv")
    if sales_file:
        upload_file(sales_file, "sales")

    purchase_file = st.file_uploader("ğŸ“¦ purchase_data.csv ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type="csv")
    if purchase_file:
        upload_file(purchase_file, "purchase_data")

    item_file = st.file_uploader("ğŸ“‹ item_master.csv ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type="csv")
    if item_file:
        upload_file(item_file, "item_master")

    # âœ… ã“ã‚Œã‚‚ãƒ¢ãƒ¼ãƒ‰å†…ã«å…¥ã‚Œã‚‹ï¼
    warehouse_file = st.file_uploader("ğŸ¢ å€‰åº«åœ¨åº«.xlsx ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["xlsx"])
    if warehouse_file:
        def preprocess_warehouse_stock(file):
            df = pd.read_excel(file, sheet_name="å€‰åº«åœ¨åº«")
            df_upload = df.iloc[:, [9, 13, 22]].copy()  # J, N, W
            df_upload.columns = ["product_code", "stock_available", "jan"]
            df_upload["product_code"] = df_upload["product_code"].astype(str).str.strip()
            df_upload["jan"] = df_upload["jan"].astype(str).str.strip()
            df_upload["stock_available"] = pd.to_numeric(df_upload["stock_available"], errors="coerce").fillna(0).round().astype(int)
            return df_upload

        def upload_warehouse_stock(df):
            try:
                requests.delete(f"{SUPABASE_URL}/rest/v1/warehouse_stock?product_code=neq.null", headers=HEADERS)
                df = df.drop_duplicates(subset=["product_code"], keep="last")
                df = df.replace({pd.NA: None, pd.NaT: None, float("nan"): None}).where(pd.notnull(df), None)

                for i in range(0, len(df), 500):
                    batch = df.iloc[i:i+500].to_dict(orient="records")
                    res = requests.post(
                        f"{SUPABASE_URL}/rest/v1/warehouse_stock",
                        headers={**HEADERS, "Prefer": "resolution=merge-duplicates"},
                        json=batch
                    )
                    if res.status_code not in [200, 201]:
                        st.error(f"âŒ warehouse_stock ãƒãƒƒãƒPOSTå¤±æ•—: {res.status_code} {res.text}")
                        return

                st.success(f"âœ… warehouse_stock ã« {len(df)} ä»¶ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")

            except Exception as e:
                st.error(f"âŒ warehouse_stock ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

        with st.spinner("ğŸ“¤ å€‰åº«åœ¨åº«.xlsx ã‚’å‡¦ç†ä¸­..."):
            df_warehouse = preprocess_warehouse_stock(warehouse_file)
            upload_warehouse_stock(df_warehouse)

    benten_file = st.file_uploader("ğŸ­ BENTENå€‰åº«åœ¨åº«ï¼ˆCSVï¼‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["csv"])
    if benten_file:
        def preprocess_benten_stock(file):
            df = pd.read_csv(file)
            df.columns = df.columns.str.replace("ã€€", "").str.replace("\ufeff", "").str.strip()

            upc_col = None
            stock_col = None
            for col in df.columns:
                if "UPC" in col:
                    upc_col = col
                if "åˆ©ç”¨å¯èƒ½" in col:
                    stock_col = col

            if not upc_col or not stock_col:
                raise ValueError(f"âŒ 'UPCã‚³ãƒ¼ãƒ‰' ã¾ãŸã¯ 'åˆ©ç”¨å¯èƒ½' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼åˆ—å: {df.columns.tolist()}")

            df = df[[upc_col, stock_col]].copy()
            df.rename(columns={upc_col: "jan", stock_col: "stock"}, inplace=True)
            df["jan"] = df["jan"].astype(str).str.strip()
            df["stock"] = pd.to_numeric(df["stock"], errors="coerce").fillna(0).round().astype(int)
            df["updated_at"] = pd.Timestamp.now().isoformat()
            return df

        def upload_benten_stock(df):
            try:
                requests.delete(f"{SUPABASE_URL}/rest/v1/benten_stock?jan=neq.null", headers=HEADERS)
                df = df.drop_duplicates(subset=["jan"], keep="last")
                df = df.replace({pd.NA: None, pd.NaT: None, float("nan"): None}).where(pd.notnull(df), None)

                for i in range(0, len(df), 500):
                    batch = df.iloc[i:i+500].to_dict(orient="records")
                    res = requests.post(
                        f"{SUPABASE_URL}/rest/v1/benten_stock",
                        headers={**HEADERS, "Prefer": "resolution=merge-duplicates"},
                        json=batch
                    )
                    if res.status_code not in [200, 201]:
                        st.error(f"âŒ benten_stock ãƒãƒƒãƒPOSTå¤±æ•—: {res.status_code} {res.text}")
                        return

                st.success(f"âœ… benten_stock ã« {len(df)} ä»¶ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")

            except Exception as e:
                st.error(f"âŒ benten_stock ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

        with st.spinner("ğŸ“¤ BENTENå€‰åº«CSV ã‚’å‡¦ç†ä¸­..."):
            df_benten = preprocess_benten_stock(benten_file)
            upload_benten_stock(df_benten)



# ğŸ†• è²©å£²å®Ÿç¸¾ï¼ˆç›´è¿‘1ãƒ¶æœˆï¼‰ãƒ¢ãƒ¼ãƒ‰ -----------------------------
elif mode == "monthly_sales":
    st.subheader("ğŸ“Š è²©å£²å®Ÿç¸¾ï¼ˆç›´è¿‘1ãƒ¶æœˆï¼‰")

    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    HEADERS = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json"
    }

    # âœ… ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•°
    def fetch_data(table_name):
        headers = {**HEADERS, "Prefer": "count=exact"}
        dfs = []
        offset, limit = 0, 1000
        while True:
            url = f"{SUPABASE_URL}/rest/v1/{table_name}?select=*&offset={offset}&limit={limit}"
            res = requests.get(url, headers=headers)
            if res.status_code == 416 or not res.json():
                break
            if res.status_code not in [200, 206]:
                st.error(f"{table_name} ã®å–å¾—ã«å¤±æ•—: {res.status_code} / {res.text}")
                return pd.DataFrame()
            dfs.append(pd.DataFrame(res.json()))
            offset += limit
        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    df_master = fetch_data("item_master")
    df_sales = fetch_data("sales")
    df_warehouse = fetch_data("warehouse_stock")

    if df_master.empty or df_sales.empty or df_warehouse.empty:
        st.warning("å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        st.stop()

    # item_master æ•´å½¢
    df_master["jan"] = df_master["jan"].astype(str)
    df_master["å•†å“ã‚³ãƒ¼ãƒ‰"] = df_master["å•†å“ã‚³ãƒ¼ãƒ‰"].astype(str)
    df_master = df_master.rename(columns={"jan": "JAN"})

    # sales æ•´å½¢
    df_sales["å•†å“ã‚³ãƒ¼ãƒ‰"] = df_sales["jan"].astype(str)
    df_sales.rename(columns={"quantity_sold": "è²©å£²æ•°"}, inplace=True)

    # warehouse_stock æ•´å½¢
    df_warehouse["product_code"] = df_warehouse["product_code"].astype(str)
    df_warehouse = df_warehouse.rename(columns={
        "product_code": "å•†å“ã‚³ãƒ¼ãƒ‰",
        "stock_available": "åˆ©ç”¨å¯èƒ½åœ¨åº«"
    })

    # --- ãƒãƒ¼ã‚¸ ---
    df_joined = pd.merge(df_sales, df_master, on="å•†å“ã‚³ãƒ¼ãƒ‰", how="left")
    df_joined = pd.merge(df_joined, df_warehouse[["å•†å“ã‚³ãƒ¼ãƒ‰", "åˆ©ç”¨å¯èƒ½åœ¨åº«"]], on="å•†å“ã‚³ãƒ¼ãƒ‰", how="left")

    # --- JAN ---
    if "JAN" in df_joined.columns:
        df_joined["jan"] = df_joined["JAN"]
    else:
        st.warning("âš ï¸ item_master å´ã‹ã‚‰JANãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    # --- æ•°å€¤åˆ— ---
    df_joined["è²©å£²æ•°"] = pd.to_numeric(df_joined["è²©å£²æ•°"], errors="coerce").fillna(0).astype(int)
    df_joined["ç™ºæ³¨æ¸ˆ"] = pd.to_numeric(df_joined.get("stock_ordered", 0), errors="coerce").fillna(0).astype(int)
    df_joined["åˆ©ç”¨å¯èƒ½"] = df_joined["åˆ©ç”¨å¯èƒ½åœ¨åº«"].fillna(0).astype(int)
    df_joined.drop(columns=["åˆ©ç”¨å¯èƒ½åœ¨åº«"], inplace=True)

    # è²©å£²æ•° > 0 ã®ã¿
    df_joined = df_joined[df_joined["è²©å£²æ•°"] > 0]

    # ---------- ğŸ” æ¤œç´¢ UI ----------
    col1, col2 = st.columns(2)

    with col1:
        keyword_name = st.text_input(TEXT[language]["search_keyword"], "")
        keyword_code = st.text_input(TEXT[language]["search_code"], "")

    with col2:
        jan_filter_multi = st.text_area(
            TEXT[language]["multi_jan"],
            placeholder="ä¾‹:\n4901234567890\n4987654321098",
            height=120,
        )

    maker_filter = st.selectbox(
        TEXT[language]["search_brand"],
        [TEXT[language]["all"]] + sorted(df_joined["ãƒ¡ãƒ¼ã‚«ãƒ¼å"].dropna().unique())
    )
    rank_filter = st.selectbox(
        TEXT[language]["search_rank"],
        [TEXT[language]["all"]] + sorted(df_joined["ãƒ©ãƒ³ã‚¯"].dropna().unique())
    )
    type_filter = st.selectbox(
        TEXT[language]["search_type"],
        [TEXT[language]["all"]] + sorted(df_joined["å–æ‰±åŒºåˆ†"].dropna().unique())
    )

    import re
    jan_list = [j.strip() for j in re.split(r"[,\n\r]+", jan_filter_multi) if j.strip()]

    df_view = df_joined.copy()

    if jan_list:
        df_view = df_view[df_view["jan"].isin(jan_list)]
    elif keyword_code:
        df_view = df_view[
            df_view["å•†å“ã‚³ãƒ¼ãƒ‰"].str.contains(keyword_code, case=False, na=False) |
            df_view["jan"].str.contains(keyword_code, case=False, na=False)
        ]

    if keyword_name:
        df_view = df_view[df_view["å•†å“å"].str.contains(keyword_name, case=False, na=False)]

    if maker_filter != TEXT[language]["all"]:
        df_view = df_view[df_view["ãƒ¡ãƒ¼ã‚«ãƒ¼å"] == maker_filter]

    if rank_filter != TEXT[language]["all"]:
        df_view = df_view[df_view["ãƒ©ãƒ³ã‚¯"] == rank_filter]

    if type_filter != TEXT[language]["all"]:
        df_view = df_view[df_view["å–æ‰±åŒºåˆ†"] == type_filter]

    # ---------- ğŸ“‹ è¡¨ç¤º ----------
    view_cols = [
        "å•†å“ã‚³ãƒ¼ãƒ‰", "jan", "ãƒ©ãƒ³ã‚¯", "ãƒ¡ãƒ¼ã‚«ãƒ¼å",
        "å•†å“å", "å–æ‰±åŒºåˆ†", "è²©å£²æ•°", "åˆ©ç”¨å¯èƒ½", "ç™ºæ³¨æ¸ˆ"
    ]
    available_cols = [c for c in view_cols if c in df_view.columns]

    display_df = (
        df_view[available_cols]
        .sort_values(by="å•†å“ã‚³ãƒ¼ãƒ‰")
        .rename(columns=COLUMN_NAMES[language])
    )

    row_count = len(display_df)
    h_left, h_right = st.columns([1, 0.15])
    h_left.subheader(TEXT[language]["product_list"])
    h_right.markdown(
        f"<h4 style='text-align:right; margin-top: 0.6em;'>{row_count:,}ä»¶</h4>",
        unsafe_allow_html=True
    )

    st.dataframe(display_df, use_container_width=True)

    csv = display_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        "ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv,
        file_name="monthly_sales_filtered.csv",
        mime="text/csv",
    )


elif mode == "rank_check":
    st.subheader("ğŸ“Œ ãƒ©ãƒ³ã‚¯å•†å“ç¢ºèªãƒ¢ãƒ¼ãƒ‰")

    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    df_item = fetch_table("item_master")
    df_sales = fetch_table("sales")
    df_stock = fetch_table("warehouse_stock")
    df_benten = fetch_table("benten_stock")
    df_history = fetch_table("purchase_history")

    if df_item.empty or df_sales.empty or df_stock.empty or df_benten.empty or df_history.empty:
        st.warning("å¿…è¦ãªãƒ†ãƒ¼ãƒ–ãƒ«ãŒç©ºã§ã™")
        st.stop()

    # ğŸ”‘ ãƒ‡ãƒ¼ã‚¿æ•´å½¢
    df_item["jan"] = df_item["jan"].astype(str).str.strip()
    df_item.loc[df_item["jan"].isin(["", "nan", "None", "NULL"]), "jan"] = None
    df_item["ãƒ©ãƒ³ã‚¯"] = df_item["ãƒ©ãƒ³ã‚¯"].astype(str).str.strip()

    # ç™ºæ³¨æ¸ˆï¼ˆä¸Šæµ·é™¤å¤–ï¼‰
    df_history["quantity"] = pd.to_numeric(df_history["quantity"], errors="coerce").fillna(0).astype(int)
    df_history["memo"] = df_history["memo"].astype(str).fillna("")
    df_history["jan"] = df_history["jan"].astype(str).str.strip()
    df_shanghai = df_history[df_history["memo"].str.contains("ä¸Šæµ·", na=False)]
    df_shanghai_grouped = df_shanghai.groupby("jan")["quantity"].sum().reset_index(name="shanghai_quantity")

    df_item["ç™ºæ³¨æ¸ˆ"] = pd.to_numeric(df_item["ç™ºæ³¨æ¸ˆ"], errors="coerce").fillna(0).astype(int)
    df_item = df_item.merge(df_shanghai_grouped, on="jan", how="left")
    df_item["shanghai_quantity"] = df_item["shanghai_quantity"].fillna(0).astype(int)
    df_item["ç™ºæ³¨æ¸ˆ"] = (df_item["ç™ºæ³¨æ¸ˆ"] - df_item["shanghai_quantity"]).clip(lower=0)

    # A/B/Cãƒ©ãƒ³ã‚¯å•†å“ã®ã¿ï¼ˆJANã‚ã‚Šï¼‰
    df_ab = df_item[
        df_item["ãƒ©ãƒ³ã‚¯"].isin(["Aãƒ©ãƒ³ã‚¯", "Bãƒ©ãƒ³ã‚¯", "Cãƒ©ãƒ³ã‚¯"]) & df_item["jan"].notnull()
    ].copy()
    df_ab["JAN"] = df_ab["jan"]
    df_ab = df_ab.drop_duplicates(subset=["JAN"])

    # ãƒ©ãƒ³ã‚¯ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    selected_ranks = st.multiselect(
        "ğŸ“Œ è¡¨ç¤ºã™ã‚‹ãƒ©ãƒ³ã‚¯ã‚’é¸æŠ",
        ["Aãƒ©ãƒ³ã‚¯", "Bãƒ©ãƒ³ã‚¯", "Cãƒ©ãƒ³ã‚¯"],
        default=["Aãƒ©ãƒ³ã‚¯", "Bãƒ©ãƒ³ã‚¯", "Cãƒ©ãƒ³ã‚¯"]
    )

    # sales â†’ JAN
    df_sales["JAN"] = df_sales["jan"].astype(str).str.strip()

    # JDåœ¨åº«
    df_stock["JAN"] = df_stock["jan"].astype(str).str.strip()
    df_stock = df_stock.rename(columns={"stock_available": "JDåœ¨åº«"})

    # å¼å¤©åœ¨åº«
    df_benten["JAN"] = df_benten["jan"].astype(str).str.strip()
    df_benten = df_benten.rename(columns={"stock": "å¼å¤©åœ¨åº«"})

    # å®Ÿç¸¾ï¼ˆ30æ—¥ï¼‰
    df_sales_30 = (
        df_sales.groupby("JAN", as_index=False)["quantity_sold"]
        .sum()
        .rename(columns={"quantity_sold": "å®Ÿç¸¾ï¼ˆ30æ—¥ï¼‰"})
    )

    # ç™ºæ³¨æ¸ˆ
    df_item_sub = df_item[["jan", "ç™ºæ³¨æ¸ˆ"]].copy()
    df_item_sub["JAN"] = df_item_sub["jan"].astype(str).str.strip()
    df_item_sub = df_item_sub[["JAN", "ç™ºæ³¨æ¸ˆ"]]

    # ãƒãƒ¼ã‚¸
    df_merged = (
        df_ab[["JAN", "å•†å“å", "ãƒ©ãƒ³ã‚¯"]]
        .merge(df_sales_30, on="JAN", how="left")
        .merge(df_item_sub, on="JAN", how="left")
        .merge(df_stock[["JAN", "JDåœ¨åº«"]], on="JAN", how="left")
        .merge(df_benten[["JAN", "å¼å¤©åœ¨åº«"]], on="JAN", how="left")
    )

    # æ¬ æè£œå®Œ
    df_merged["ç™ºæ³¨æ¸ˆ"] = df_merged["ç™ºæ³¨æ¸ˆ"].fillna(0).astype(int)
    df_merged["å®Ÿç¸¾ï¼ˆ30æ—¥ï¼‰"] = df_merged["å®Ÿç¸¾ï¼ˆ30æ—¥ï¼‰"].fillna(0)
    df_merged["JDåœ¨åº«"] = df_merged["JDåœ¨åº«"].fillna(0)
    df_merged["å¼å¤©åœ¨åº«"] = df_merged["å¼å¤©åœ¨åº«"].fillna(0)
    df_merged["å®Ÿç¸¾ï¼ˆ7æ—¥ï¼‰"] = None

    # ã‚¢ãƒ©ãƒ¼ãƒˆè¨ˆç®—
    df_merged["ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.0"] = df_merged["å®Ÿç¸¾ï¼ˆ30æ—¥ï¼‰"] > (
        df_merged["JDåœ¨åº«"] + df_merged["å¼å¤©åœ¨åº«"] + df_merged["ç™ºæ³¨æ¸ˆ"]
    )
    df_merged["ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.2"] = (df_merged["å®Ÿç¸¾ï¼ˆ30æ—¥ï¼‰"] * 1.2) > (
        df_merged["JDåœ¨åº«"] + df_merged["å¼å¤©åœ¨åº«"] + df_merged["ç™ºæ³¨æ¸ˆ"]
    )

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    check_1_0 = st.checkbox("âœ… ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.0ã®ã¿è¡¨ç¤º", value=False)
    check_1_2 = st.checkbox("âœ… ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.2ã®ã¿è¡¨ç¤º", value=False)

    df_result = df_merged[df_merged["ãƒ©ãƒ³ã‚¯"].isin(selected_ranks)].copy()
    if check_1_0:
        df_result = df_result[df_result["ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.0"]]
    if check_1_2:
        df_result = df_result[df_result["ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.2"]]

    # å‡ºåŠ›
    st.dataframe(df_result[[
        "JAN",
        "å•†å“å",
        "ãƒ©ãƒ³ã‚¯",
        "å®Ÿç¸¾ï¼ˆ30æ—¥ï¼‰",
        "å®Ÿç¸¾ï¼ˆ7æ—¥ï¼‰",
        "JDåœ¨åº«",
        "å¼å¤©åœ¨åº«",
        "ç™ºæ³¨æ¸ˆ",
        "ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.0",
        "ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.2"
    ]])



elif mode == "difficult_items":
    st.subheader("ğŸš« å…¥è·å›°é›£å•†å“ãƒ¢ãƒ¼ãƒ‰")

    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    HEADERS = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json"
    }

    df = fetch_table("difficult_items")
    if not df.empty:
        df["created_at"] = pd.to_datetime(df["created_at"]).dt.strftime("%Y-%m-%d %H:%M:%S")
        df["updated_at"] = pd.to_datetime(df["updated_at"]).dt.strftime("%Y-%m-%d %H:%M:%S")
        df["é¸æŠ"] = False

        cols = ["é¸æŠ", "item_key", "reason", "note", "created_at", "updated_at", "id"]
        df = df[cols]

        st.write("### ğŸ“‹ ç¾åœ¨ã®å…¥è·å›°é›£ãƒªã‚¹ãƒˆ")

        edited_df = st.data_editor(
            df,
            use_container_width=True,
            num_rows="dynamic",
            column_config={
                "é¸æŠ": st.column_config.CheckboxColumn("é¸æŠ")
            },
            disabled=[
                "item_key", "reason", "note", "created_at", "updated_at"
            ]
        )

        selected_df = edited_df[edited_df["é¸æŠ"]]
        selected_ids = selected_df["id"].tolist()

        # âœ… ãƒœã‚¿ãƒ³ç„¡åŠ¹åŒ–ç®¡ç†
        delete_btn_disabled = False

        if st.button("âœ… é¸æŠã—ãŸè¡Œã‚’å‰Šé™¤"):
            if selected_ids:
                for _id in selected_ids:
                    record = df[df["id"] == _id].copy().to_dict(orient="records")[0]
                    record.pop("é¸æŠ", None)
                    record.pop("created_at", None)
                    record.pop("updated_at", None)
                    record["item_id"] = record["id"]
                    record.pop("id")
                    record["action"] = "delete"
                    record["action_at"] = datetime.datetime.now(ZoneInfo("Asia/Tokyo")).isoformat()
        
                    res1 = requests.post(
                        f"{SUPABASE_URL}/rest/v1/difficult_items_history",
                        headers={**HEADERS, "Prefer": "return=representation"},
                        json=record
                    )
        
                    res2 = requests.delete(
                        f"{SUPABASE_URL}/rest/v1/difficult_items?id=eq.{_id}",
                        headers=HEADERS
                    )
        
                st.success("âœ… å‰Šé™¤å®Œäº†ï¼")
                st.rerun()
            else:
                st.warning("âš ï¸ è¡ŒãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“")

    with st.form("add_difficult_item"):
        item_key = st.text_input("ãƒ–ãƒ©ãƒ³ãƒ‰ / å•†å“å / JAN ãªã©")
        reason = st.text_input("å…¥è·å›°é›£ç†ç”±")
        note = st.text_area("å‚™è€ƒ")

        submitted = st.form_submit_button("ç™»éŒ²ã™ã‚‹")
        if submitted:
            payload = {
                "item_key": item_key,
                "reason": reason,
                "note": note
            }

            res = requests.post(
                f"{SUPABASE_URL}/rest/v1/difficult_items",
                headers={**HEADERS, "Prefer": "return=representation"},
                json=payload
            )
            st.write("ç™»éŒ²POST:", res.status_code, res.text)

            if res.status_code in [200, 201]:
                record = res.json()[0]
                record["item_id"] = record["id"]
                record.pop("id")
                record.pop("created_at", None)  # â† å¿˜ã‚Œãšè¿½åŠ ï¼
                record.pop("updated_at", None)  # â† å¿˜ã‚Œãšè¿½åŠ ï¼
                record["action"] = "insert"
                record["action_at"] = datetime.datetime.now(ZoneInfo("Asia/Tokyo")).isoformat()

                res2 = requests.post(
                    f"{SUPABASE_URL}/rest/v1/difficult_items_history",
                    headers={**HEADERS, "Prefer": "return=representation"},
                    json=record
                )

                st.success("âœ… ç™»éŒ²ã—ã¾ã—ãŸï¼")
                st.rerun()
            else:
                st.error(f"ç™»éŒ²å¤±æ•—: {res.text}")

    df_history = fetch_table("difficult_items_history")
    
    if not df_history.empty:
        one_week_ago = datetime.datetime.now(ZoneInfo("Asia/Tokyo")) - datetime.timedelta(days=7)
        df_history["action_at"] = pd.to_datetime(df_history["action_at"], utc=True)
        df_history = df_history[df_history["action_at"] >= one_week_ago]
    
        # ğŸ”¥ JSTã«å¤‰æ›
        df_history["action_at"] = df_history["action_at"].dt.tz_convert("Asia/Tokyo")
        df_history["action_at"] = df_history["action_at"].dt.strftime("%Y-%m-%d %H:%M:%S")
    
        st.write("ğŸ“œ **å±¥æ­´ï¼ˆç›´è¿‘7æ—¥åˆ†ï¼‰**")
        st.dataframe(df_history, use_container_width=True)
    else:
        st.write("ğŸ“œ **å±¥æ­´ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“**")


# parse_items_fixed ã¯ä»Šã®ã¾ã¾åˆ©ç”¨OK

elif mode == "order":
    st.subheader("ğŸ“¦ ç™ºæ³¨æ›¸ä½œæˆãƒ¢ãƒ¼ãƒ‰")

    option = st.radio("å…¥åŠ›æ–¹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„", ["ãƒ†ã‚­ã‚¹ãƒˆè²¼ã‚Šä»˜ã‘", "CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"])
    df_order = None

    # ---------- CSVãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé…å¸ƒ ----------
    def provide_template():
        template = pd.DataFrame({
            "jan": [],
            "æ•°é‡": [],
            "å˜ä¾¡": []
        })
        csv_temp = template.to_csv(index=False, encoding="utf-8-sig")
        st.download_button(
            "ğŸ“ å…¥åŠ›ç”¨CSVãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_temp,
            mime="text/csv",
            file_name="order_template.csv",
            help="å¿…é ˆåˆ—: jan, æ•°é‡, å˜ä¾¡ï¼ˆãƒ­ãƒƒãƒˆÃ—æ•°é‡ã§ã‚‚å¯ï¼‰"
        )

    provide_template()

    # ---------- å…¥åŠ› ----------
    if option == "ãƒ†ã‚­ã‚¹ãƒˆè²¼ã‚Šä»˜ã‘":
        input_text = st.text_area("æ³¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’è²¼ã‚Šä»˜ã‘", height=300)
        if st.button("ãƒ†ã‚­ã‚¹ãƒˆã‚’å¤‰æ›"):
            if not input_text.strip():
                st.warning("âš  ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            else:
                df_order = parse_items_fixed(input_text)
                if df_order is not None and not df_order.empty and "å“ç•ª" in df_order.columns:
                    df_order = df_order[df_order["å“ç•ª"] != "åˆè¨ˆ"]
                else:
                    st.warning("âš  å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’æ­£ã—ãå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

elif mode == "order":
    st.subheader("ğŸ“¦ ç™ºæ³¨æ›¸ä½œæˆãƒ¢ãƒ¼ãƒ‰")

    option = st.radio("å…¥åŠ›æ–¹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„", ["ãƒ†ã‚­ã‚¹ãƒˆè²¼ã‚Šä»˜ã‘", "CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"])
    df_order = None

    # ---------- CSVãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé…å¸ƒ ----------
    def provide_template():
        template = pd.DataFrame({
            "jan": [],
            "æ•°é‡": [],
            "å˜ä¾¡": []
        })
        csv_temp = template.to_csv(index=False, encoding="utf-8-sig")
        st.download_button(
            "ğŸ“ å…¥åŠ›ç”¨CSVãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_temp,
            mime="text/csv",
            file_name="order_template.csv",
            help="å¿…é ˆåˆ—: jan, æ•°é‡, å˜ä¾¡ï¼ˆãƒ­ãƒƒãƒˆÃ—æ•°é‡ã§ã‚‚å¯ï¼‰"
        )

    provide_template()

    # ---------- å…¥åŠ› ----------
    if option == "ãƒ†ã‚­ã‚¹ãƒˆè²¼ã‚Šä»˜ã‘":
        input_text = st.text_area("æ³¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’è²¼ã‚Šä»˜ã‘", height=300)
        if st.button("ãƒ†ã‚­ã‚¹ãƒˆã‚’å¤‰æ›"):
            if not input_text.strip():
                st.warning("âš  ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            else:
                df_order = parse_items_fixed(input_text)
                if df_order is not None and not df_order.empty and "å“ç•ª" in df_order.columns:
                    df_order = df_order[df_order["å“ç•ª"] != "åˆè¨ˆ"]
                else:
                    st.warning("âš  å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’æ­£ã—ãå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

    if option == "CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
        uploaded_file = st.file_uploader("æ³¨æ–‡CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["csv"])
        if uploaded_file is not None:
            try:
                df_order = pd.read_csv(uploaded_file, encoding="utf-8-sig")
            except UnicodeDecodeError:
                df_order = pd.read_csv(uploaded_file, encoding="shift_jis")

            st.success("âœ… CSVã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼")
            st.dataframe(df_order.head())

            # ã‚«ãƒ©ãƒ åã‚’æ¨™æº–åŒ–
            df_order.columns = df_order.columns.str.strip().str.lower()
            st.write("ğŸ“Œ CSVã‚«ãƒ©ãƒ å:", df_order.columns.tolist())

            # ã‚ˆãã‚ã‚‹è¡¨è¨˜ã‚†ã‚Œã‚’ä¿®æ­£
            rename_map = {
                "janã‚³ãƒ¼ãƒ‰": "jan", "ï¼ªï¼¡ï¼®": "jan", "JAN": "jan",
                "æ•°é‡": "æ•°é‡", "æ•°": "æ•°é‡", "qty": "æ•°é‡",
                "ãƒ­ãƒƒãƒˆÃ—æ•°é‡": "ãƒ­ãƒƒãƒˆÃ—æ•°é‡",
                "å˜ä¾¡": "å˜ä¾¡", "ä¾¡æ ¼": "å˜ä¾¡", "price": "å˜ä¾¡"
            }
            df_order.rename(columns={k.lower(): v for k, v in rename_map.items() if k.lower() in df_order.columns}, inplace=True)

            # å¿…é ˆåˆ—ãƒã‚§ãƒƒã‚¯
            if "jan" not in df_order.columns:
                st.error("âŒ CSVã« 'jan' åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚åˆ—åã‚’ 'jan' ã«ã—ã¦ãã ã•ã„ã€‚")
                df_order = None

    # ---------- åˆæœŸè¨­å®š ----------
    suppliers = [
        "0402 ãƒãƒªãƒå…±å’Œç‰©ç”£æ ªå¼ä¼šç¤¾","0077 å¤§åˆ†å…±å’Œæ ªå¼ä¼šç¤¾","0025 æ ªå¼ä¼šç¤¾ã‚ªãƒ³ãƒ€",
        "0029 Kãƒ»BLUEæ ªå¼ä¼šç¤¾","0072 æ–°å¯Œå£«ãƒãƒ¼ãƒŠãƒ¼æ ªå¼ä¼šç¤¾","0073 æ ªå¼ä¼šç¤¾ã€€ã‚¨ã‚£ãƒãƒ»ã‚±ã‚¤",
        "0085 ä¸­å¤®ç‰©ç”£æ ªå¼ä¼šç¤¾","0106 è¥¿å·æ ªå¼ä¼šç¤¾","0197 å¤§æœ¨åŒ–ç²§å“æ ªå¼ä¼šç¤¾","0201 ç¾é‡‘ä»•å…¥ã‚Œ",
        "0202 ãƒˆãƒ©ã‚¹ã‚³ä¸­å±±æ ªå¼ä¼šç¤¾","0256 æ ªå¼ä¼šç¤¾ã€€ã‚°ãƒ©ãƒ³ã‚¸ã‚§","0258 æ ªå¼ä¼šç¤¾ã€€ãƒ•ã‚¡ã‚¤ãƒ³",
        "0263 æ ªå¼ä¼šç¤¾ãƒ¡ãƒ‡ã‚£ãƒ•ã‚¡ã‚¤ãƒ³","0285 æœ‰é™ä¼šç¤¾ã‚ªãƒ¼ã‚¶ã‚¤é¦–è—¤","0343 æ ªå¼ä¼šç¤¾æ£®ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ",
        "0376 è…é‡æ ªå¼ä¼šç¤¾","0411 æ ªå¼ä¼šç¤¾ãƒ©ã‚¯ãƒ¼ãƒ³ã‚³ãƒãƒ¼ã‚¹ï¼ˆã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒ‡ãƒªãƒãƒªãƒ¼ï¼‰",
        "0435 æ ªå¼ä¼šç¤¾ æµä¹…å•†äº‹","0444 ãƒãƒŠãƒ¢ãƒ³ãƒ¯ãƒ¼ã‚¯ã‚¹ åˆåŒä¼šç¤¾","0445 å¯Œæ£®å•†äº‹ æ ªå¼ä¼šç¤¾",
        "0457 ã‚«ãƒã‚¤ã‚·æ ªå¼ä¼šç¤¾","0468 ç‹å­å›½éš›è²¿æ˜“æ ªå¼ä¼šç¤¾","0469 æ ªå¼ä¼šç¤¾ã€€æ–°æ—¥é…è–¬å“",
        "0474 æ ªå¼ä¼šç¤¾ã€€äº”æ´²","0475 æ ªå¼ä¼šç¤¾ã‚·ã‚²ãƒãƒ„","0476 ã‚«ãƒ¼ãƒ‰ä»•å…¥ã‚Œ",
        "0479 ã‚¹ã‚±ãƒ¼ã‚¿ãƒ¼æ ªå¼ä¼šç¤¾","0482 é¢¨é›²å•†äº‹æ ªå¼ä¼šç¤¾","0484 ZSAå•†äº‹æ ªå¼ä¼šç¤¾",
        "0486 Maple Internationalæ ªå¼ä¼šç¤¾","0490 NEW WINDæ ªå¼ä¼šç¤¾","0491 ã‚¢ãƒ—ãƒ©ã‚¤ãƒ‰æ ªå¼ä¼šç¤¾"
    ]
    employees = ["031 æ–è—¤è£•å²","037 ç±³æ¾¤å’Œæ•","043 å¾è¶Š","079 éš‹è‰¶å‰"]
    departments = ["è¼¸å‡ºäº‹æ¥­éƒ¨ : è¼¸å‡ºï¼ˆASEANï¼‰","è¼¸å‡ºäº‹æ¥­éƒ¨ : è¼¸å‡ºï¼ˆä¸­å›½ï¼‰","è¼¸å‡ºäº‹æ¥­éƒ¨"]
    locations = ["JD-ç‰©æµ-åƒè‘‰","å¼å¤©å€‰åº«"]

    col1, col2, col3 = st.columns(3)
    with col1:
        from datetime import datetime, date
        external_id = datetime.now().strftime("%Y%m%d%H%M%S")
        st.text_input("å¤–éƒ¨IDï¼ˆè‡ªå‹•ï¼‰", value=external_id, disabled=True)
        supplier = st.selectbox("ä»•å…¥å…ˆ", suppliers)
    with col2:
        order_date = st.date_input("æ—¥ä»˜", value=date.today())
        employee = st.selectbox("å¾“æ¥­å“¡", employees)
    with col3:
        department = st.selectbox("éƒ¨é–€", departments)
        location = st.selectbox("å ´æ‰€", locations)

    memo = st.text_input("ãƒ¡ãƒ¢", "BCãƒ©ãƒ³ã‚¯")

    # ---------- ç™ºæ³¨æ›¸ç”Ÿæˆ ----------
    if df_order is not None and not df_order.empty:
        df_item = fetch_table("item_master")
        df_item.columns = df_item.columns.str.strip().str.lower()
        st.write("ğŸ“Œ item_master ã‚«ãƒ©ãƒ å:", df_item.columns.tolist())

        if "jan" not in df_item.columns:
            st.error("âŒ item_master ã« 'jan' åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚Supabase å´ã®åˆ—åã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        # å‹ã‚’çµ±ä¸€
        df_order["jan"] = df_order["jan"].astype(str).str.strip()
        df_item["jan"]  = df_item["jan"].astype(str).str.strip()

        # ç¨ç‡åˆ¤å®š
        def get_tax_rate(schedule: str) -> float:
            if not schedule:
                return 0.0
            if any(key in schedule for key in ["æ¶ˆè²»ç¨10", "ä»•å…¥10"]):
                return 0.10
            if any(key in schedule for key in ["æ¶ˆè²»ç¨8", "ä»•å…¥8"]):
                return 0.08
            return 0.0

        if "ç´ç¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«" in df_item.columns:
            df_item["tax_rate"] = df_item["ç´ç¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«"].apply(get_tax_rate)
        else:
            df_item["tax_rate"] = 0.0

        # ãƒãƒ¼ã‚¸
        df = df_order.merge(df_item, on="jan", how="left")

        # ä¸æ˜JANè­¦å‘Š
        missing = df[df["å•†å“å"].isna()]
        if not missing.empty:
            st.warning(f"âš  {len(missing)} ä»¶ãŒ item_master ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

        qty_col = "ãƒ­ãƒƒãƒˆÃ—æ•°é‡" if "ãƒ­ãƒƒãƒˆÃ—æ•°é‡" in df.columns else "æ•°é‡"
        df["æ•°é‡"] = pd.to_numeric(df[qty_col], errors="coerce").fillna(0).astype(int)
        df["å˜ä¾¡"] = pd.to_numeric(df["å˜ä¾¡"], errors="coerce").fillna(0).astype(int)

        import numpy as np
        df["é‡‘é¡"] = df["å˜ä¾¡"] * df["æ•°é‡"]
        df["ç¨é¡"] = np.floor(df["é‡‘é¡"] * df["tax_rate"]).astype(int)
        df["ç·é¡"] = df["é‡‘é¡"] + df["ç¨é¡"]

        order_date_str = order_date.strftime("%Y/%m/%d")

        df_out = pd.DataFrame({
            "å¤–éƒ¨ID": external_id,
            "ä»•å…¥å…ˆ": supplier,
            "æ—¥ä»˜": order_date_str,
            "å¾“æ¥­å“¡": employee,
            "éƒ¨é–€": department,
            "ãƒ¡ãƒ¢": memo,
            "å ´æ‰€": location,
            "ã‚¢ã‚¤ãƒ†ãƒ ": df.get("å•†å“ã‚³ãƒ¼ãƒ‰", "").astype(str) + " " + df.get("å•†å“å", ""),
            "æ•°é‡": df["æ•°é‡"],
            "å˜ä¾¡/ç‡": df["å˜ä¾¡"],
            "é‡‘é¡": df["é‡‘é¡"],
            "ç¨é¡": df["ç¨é¡"],
            "ç·é¡": df["ç·é¡"]
        })

        st.subheader("ğŸ“‘ ç™ºæ³¨æ›¸ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        st.dataframe(df_out)

        csv_out = df_out.to_csv(index=False, encoding="utf-8-sig")
        st.download_button(
            label="ğŸ“¥ ç™ºæ³¨æ›¸CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_out,
            file_name=f"ç™ºæ³¨æ›¸_{external_id}.csv",
            mime="text/csv"
        )
