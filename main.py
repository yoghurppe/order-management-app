import streamlit as st
import pandas as pd
import requests
import os
import math
import re

# è¨€èªžé¸æŠž
language = st.sidebar.selectbox("\ud83c\udf10 è¨€èªž / Language", ["æ—¥æœ¬èªž", "ä¸­æ–‡"], key="language")

# ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡¨ç¤ºç”¨ãƒ©ãƒ™ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
TEXT = {
    "æ—¥æœ¬èªž": {
        "title_order_ai": "\ud83d\udce6 ç™ºæ³¨AIï¼ˆåˆ©ç”¨å¯èƒ½åœ¨åº«ã§åˆ¤æ–­ï¼‰",
        "mode_select": "ãƒ¢ãƒ¼ãƒ‰ã‚’é¸ã‚“ã§ãã ã•ã„",
        "upload_csv": "\ud83d\udcc4 CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        "order_ai": "\ud83d\udce6 ç™ºæ³¨AIåˆ¤å®š",
        "search_item": "\ud83d\udd0d å•†å“æƒ…å ±æ¤œç´¢",
        "upload_item": "\ud83d\udcc4 å•†å“æƒ…å ±CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        "price_improve": "\ud83d\udcb0 ä»•å…¥ä¾¡æ ¼æ”¹å–„ãƒªã‚¹ãƒˆ",
        "search_keyword": "å•†å“åãƒ»å•†å“ã‚³ãƒ¼ãƒ‰ã§æ¤œç´¢",
        "search_brand": "ãƒ–ãƒ©ãƒ³ãƒ‰ã§çµžã‚Šè¾¼ã¿",
        "search_type": "å–æ‰±åŒºåˆ†ã§çµžã‚Šè¾¼ã¿",
        "product_list": "\ud83d\udccb å•†å“ä¸€è¦§"
    },
    "ä¸­æ–‡": {
        "title_order_ai": "\ud83d\udce6 è®¢è´§AIï¼ˆæ ¹æ®å¯ç”¨åº“å­˜ï¼‰",
        "mode_select": "è¯·é€‰æ‹©æ¨¡å¼",
        "upload_csv": "\ud83d\udcc4 ä¸Šä¼ CSV",
        "order_ai": "\ud83d\udce6 è®¢è´§AIåˆ¤æ–­",
        "search_item": "\ud83d\udd0d å•†å“ä¿¡æ¯æŸ¥è¯¢",
        "upload_item": "\ud83d\udcc4 ä¸Šä¼ å•†å“ä¿¡æ¯CSV",
        "price_improve": "\ud83d\udcb0 è¿›è´§ä»·æ ¼ä¼˜åŒ–æ¸…å•",
        "search_keyword": "æŒ‰å•†å“åç§°æˆ–ç¼–å·æœç´¢",
        "search_brand": "æŒ‰å“ç‰Œç­›é€‰",
        "search_type": "æŒ‰åˆ†ç±»ç­›é€‰",
        "product_list": "\ud83d\udccb å•†å“åˆ—è¡¨"
    }
}

# åˆ—åãƒžãƒƒãƒ”ãƒ³ã‚°
COLUMN_NAMES = {
    "æ—¥æœ¬èªž": {
        "å•†å“ã‚³ãƒ¼ãƒ‰": "å•†å“ã‚³ãƒ¼ãƒ‰",
        "jan": "JAN",
        "ãƒ©ãƒ³ã‚¯": "ãƒ©ãƒ³ã‚¯",
        "ãƒ–ãƒ©ãƒ³ãƒ‰": "ãƒ–ãƒ©ãƒ³ãƒ‰",
        "å•†å“å": "å•†å“å",
        "å–æ‰±åŒºåˆ†": "å–æ‰±åŒºåˆ†",
        "åœ¨åº«": "åœ¨åº«",
        "åˆ©ç”¨å¯èƒ½": "åˆ©ç”¨å¯èƒ½",
        "ç™ºæ³¨æ¸ˆ": "ç™ºæ³¨æ¸ˆ",
        "ä»•å…¥ä¾¡æ ¼": "ä»•å…¥ä¾¡æ ¼",
        "ã‚±ãƒ¼ã‚¹å…¥æ•°": "ã‚±ãƒ¼ã‚¹å…¥æ•°",
        "ç™ºæ³¨ãƒ­ãƒƒãƒˆ": "ç™ºæ³¨ãƒ­ãƒƒãƒˆ",
        "é‡é‡": "é‡é‡(g)"
    },
    "ä¸­æ–‡": {
        "å•†å“ã‚³ãƒ¼ãƒ‰": "å•†å“ç¼–å·",
        "jan": "æ¡ç ",
        "ãƒ©ãƒ³ã‚¯": "ç­‰çº§",
        "ãƒ–ãƒ©ãƒ³ãƒ‰": "å“ç‰Œ",
        "å•†å“å": "å•†å“åç§°",
        "å–æ‰±åŒºåˆ†": "åˆ†ç±»",
        "åœ¨åº«": "åº“å­˜",
        "åˆ©ç”¨å¯èƒ½": "å¯ç”¨åº“å­˜",
        "ç™ºæ³¨æ¸ˆ": "å·²è®¢è´­",
        "ä»•å…¥ä¾¡æ ¼": "è¿›è´§ä»·",
        "ã‚±ãƒ¼ã‚¹å…¥æ•°": "ç®±å…¥æ•°",
        "ç™ºæ³¨ãƒ­ãƒƒãƒˆ": "è®¢è´­å•ä½",
        "é‡é‡": "é‡é‡(g)"
    }
}

# ã‚¿ã‚¤ãƒˆãƒ«
st.set_page_config(page_title=TEXT[language]["title_order_ai"])
st.title(TEXT[language]["title_order_ai"])

# ãƒ¢ãƒ¼ãƒ‰é¸æŠž
mode = st.sidebar.radio(TEXT[language]["mode_select"], [
    TEXT[language]["upload_csv"],
    TEXT[language]["order_ai"],
    TEXT[language]["search_item"],
    TEXT[language]["upload_item"],
    TEXT[language]["price_improve"]
])


def fetch_table_cached(table_name):
    if table_name not in st.session_state:
        headers = {**HEADERS, "Prefer": "count=exact"}
        dfs = []
        offset = 0
        limit = 1000
        while True:
            url = f"{SUPABASE_URL}/rest/v1/{table_name}?select=*&offset={offset}&limit={limit}"
            res = requests.get(url, headers=headers)
            if res.status_code == 416:
                break
            if res.status_code not in [200, 206]:
                st.error(f"{table_name} ã®å–å¾—ã«å¤±æ•—: {res.status_code} / {res.text}")
                st.session_state[table_name] = pd.DataFrame()
                return
            data = res.json()
            if not data:
                break
            dfs.append(pd.DataFrame(data))
            offset += limit
        df = pd.concat(dfs, ignore_index=True)
        st.session_state[table_name] = df
    st.write(f"ðŸ“¦ {table_name} ä»¶æ•°: {len(st.session_state[table_name])}")
    return st.session_state[table_name]

def normalize_jan(x):
    try:
        if re.fullmatch(r"\d+(\.0+)?", str(x)):
            return str(int(float(x)))
        else:
            return str(x).strip()
    except:
        return ""

SUPABASE_URL = st.secrets["SUPABASE_URL"]
SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
HEADERS = {
    "apikey": SUPABASE_KEY,
    "Authorization": f"Bearer {SUPABASE_KEY}",
    "Content-Type": "application/json"
}

st.set_page_config(page_title="ç™ºæ³¨AIï¼ˆç´å“ã‚¿ã‚¤ãƒŸãƒ³ã‚° + åˆ©ç”¨å¯èƒ½åœ¨åº«ï¼‰", layout="wide")
st.title("ðŸ“¦ ç™ºæ³¨AIï¼ˆåˆ©ç”¨å¯èƒ½åœ¨åº«ã§åˆ¤æ–­ï¼‰")

mode = st.sidebar.radio("ãƒ¢ãƒ¼ãƒ‰ã‚’é¸ã‚“ã§ãã ã•ã„", ["ðŸ“¤ CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", "ðŸ“¦ ç™ºæ³¨AIåˆ¤å®š", "ðŸ” å•†å“æƒ…å ±æ¤œç´¢", "ðŸ“¤ å•†å“æƒ…å ±CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", "ðŸ’° ä»•å…¥ä¾¡æ ¼æ”¹å–„ãƒªã‚¹ãƒˆ"])



if mode == "ðŸ“¤ CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
    st.header("ðŸ“¤ CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")

    def preprocess_csv(df, table):
        df.columns = df.columns.str.strip()

        if table == "sales":
            df.rename(columns={
                "ã‚¢ã‚¤ãƒ†ãƒ ": "jan",
                "å–æ‰±åŒºåˆ†": "handling_type",
                "è²©å£²æ•°é‡": "quantity_sold",
                "ç¾åœ¨ã®æ‰‹æŒæ•°é‡": "stock_total",
                "ç¾åœ¨ã®åˆ©ç”¨å¯èƒ½æ•°é‡": "stock_available",
                "ç¾åœ¨ã®æ³¨æ–‡æ¸ˆæ•°é‡": "stock_ordered"
            }, inplace=True)

            for col in ["quantity_sold", "stock_total", "stock_available", "stock_ordered"]:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0).astype(int)
            df["jan"] = df["jan"].apply(normalize_jan)

        if table == "purchase_data":
            for col in ["order_lot", "price"]:
                if col in df.columns:
                    df[col] = df[col].astype(str).str.replace(",", "")
                    df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0)
                    if col == "order_lot":
                        df[col] = df[col].round().astype(int)
            df["jan"] = df["jan"].apply(normalize_jan)

        return df

    def batch_upload_csv_to_supabase(file_path, table):
        try:
            df = pd.read_csv(file_path)
            df = preprocess_csv(df, table)

            url = f"{SUPABASE_URL}/rest/v1/{table}?id=gt.0"
            requests.delete(url, headers=HEADERS)

            if table == "purchase_data":
                df = df.drop_duplicates(subset=["jan", "supplier", "order_lot"], keep="last")
            else:
                df = df.drop_duplicates(subset=["jan"], keep="last")

            batch_size = 500
            for i in range(0, len(df), batch_size):
                batch = df.iloc[i:i+batch_size].where(pd.notnull(df.iloc[i:i+batch_size]), None).to_dict(orient="records")
                res = requests.post(
                    f"{SUPABASE_URL}/rest/v1/{table}",
                    headers={**HEADERS, "Prefer": "resolution=merge-duplicates"},
                    json=batch
                )
                if res.status_code not in [200, 201]:
                    st.error(f"âŒ ãƒãƒƒãƒPOSTå¤±æ•—: {res.status_code} {res.text}")
                    return
            st.success(f"âœ… {table} ã« {len(df)} ä»¶ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        except Exception as e:
            st.error(f"âŒ {table} ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    sales_file = st.file_uploader("ðŸ§¾ sales.csv ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type="csv")
    if sales_file:
        with st.spinner("ðŸ“¤ sales.csv ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­..."):
            temp_path = "/tmp/sales.csv"
            with open(temp_path, "wb") as f:
                f.write(sales_file.read())
            batch_upload_csv_to_supabase(temp_path, "sales")

    purchase_file = st.file_uploader("ðŸ“¦ purchase_data.csv ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type="csv")
    if purchase_file:
        with st.spinner("ðŸ“¤ purchase_data.csv ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­..."):
            temp_path = "/tmp/purchase_data.csv"
            with open(temp_path, "wb") as f:
                f.write(purchase_file.read())
            batch_upload_csv_to_supabase(temp_path, "purchase_data")

if mode == "ðŸ“¦ ç™ºæ³¨AIåˆ¤å®š":
    with st.spinner("ðŸ“¦ ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        st.header("ðŸ“¦ ç™ºæ³¨AIï¼ˆåˆ©ç”¨å¯èƒ½åœ¨åº«ãƒ™ãƒ¼ã‚¹ï¼‰")

        df_sales = fetch_table_cached("sales")
        df_purchase = fetch_table_cached("purchase_data")

    if df_sales.empty or df_purchase.empty:
        st.warning("è²©å£²å®Ÿç¸¾ã¾ãŸã¯ä»•å…¥ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
        st.stop()

    

    if df_sales.empty or df_purchase.empty:
        st.warning("è²©å£²å®Ÿç¸¾ã¾ãŸã¯ä»•å…¥ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
        st.stop()

    df_sales["jan"] = df_sales["jan"].apply(normalize_jan)
    df_purchase["jan"] = df_purchase["jan"].apply(normalize_jan)

    df_sales["quantity_sold"] = pd.to_numeric(df_sales["quantity_sold"], errors="coerce").fillna(0).astype(int)
    df_sales["stock_available"] = pd.to_numeric(df_sales["stock_available"], errors="coerce").fillna(0).astype(int)
    df_sales["stock_ordered"] = pd.to_numeric(df_sales["stock_ordered"], errors="coerce").fillna(0).astype(int)
    df_purchase["order_lot"] = pd.to_numeric(df_purchase["order_lot"], errors="coerce").fillna(0).astype(int)
    df_purchase["price"] = pd.to_numeric(df_purchase["price"], errors="coerce").fillna(0)

    with st.spinner("ðŸ¤– ç™ºæ³¨AIã‚’è¨ˆç®—ä¸­..."):
        results = []
        for _, row in df_sales.iterrows():
            jan = row["jan"]
            sold = row["quantity_sold"]
            stock = row.get("stock_available", 0)
            ordered = row.get("stock_ordered", 0)
            options = df_purchase[df_purchase["jan"] == jan].copy()
            if options.empty:
                continue

            if stock >= sold:
                need_qty = 0
            else:
                need_qty = sold - stock + math.ceil(sold * 0.5) - ordered
                need_qty = max(need_qty, 0)

            if need_qty <= 0:
                continue

            options = options[options["order_lot"] > 0]
            options["diff"] = (options["order_lot"] - need_qty).abs()

            smaller_lots = options[options["order_lot"] <= need_qty]

            if not smaller_lots.empty:
                best_option = smaller_lots.loc[smaller_lots["diff"].idxmin()]
            else:
                near_lots = options[(options["order_lot"] > need_qty) & (options["order_lot"] <= need_qty * 1.5) & (options["order_lot"] != 1)]
                if not near_lots.empty:
                    best_option = near_lots.loc[near_lots["diff"].idxmin()]
                else:
                    one_lot = options[options["order_lot"] == 1]
                    if not one_lot.empty:
                        best_option = one_lot.iloc[0]
                    else:
                        best_option = options.sort_values("order_lot").iloc[0]  # æœ€å°ãƒ­ãƒƒãƒˆã‚’é¸ã¶

            sets = math.ceil(need_qty / best_option["order_lot"])
            qty = sets * best_option["order_lot"]
            total_cost = qty * best_option["price"]

            best_plan = {
                "jan": jan,
                "è²©å£²å®Ÿç¸¾": sold,
                "åœ¨åº«": stock,
                "ç™ºæ³¨æ¸ˆ": ordered,
                "ç†è«–å¿…è¦æ•°": need_qty,
                "ç™ºæ³¨æ•°": qty,
                "ãƒ­ãƒƒãƒˆ": best_option["order_lot"],
                "æ•°é‡": round(qty / best_option["order_lot"], 2),
                "å˜ä¾¡": best_option["price"],
                "ç·é¡": total_cost,
                "ä»•å…¥å…ˆ": best_option.get("supplier", "ä¸æ˜Ž")
            }
            results.append(best_plan)

    if results:
        result_df = pd.DataFrame(results)
        column_order = ["jan", "è²©å£²å®Ÿç¸¾", "åœ¨åº«", "ç™ºæ³¨æ¸ˆ", "ç†è«–å¿…è¦æ•°", "ç™ºæ³¨æ•°", "ãƒ­ãƒƒãƒˆ", "æ•°é‡", "å˜ä¾¡", "ç·é¡", "ä»•å…¥å…ˆ"]
        result_df = result_df[[col for col in column_order if col in result_df.columns]]
        st.success(f"âœ… ç™ºæ³¨å¯¾è±¡: {len(result_df)} ä»¶")
        st.dataframe(result_df)
        csv = result_df.to_csv(index=False).encode("utf-8-sig")
        st.download_button("ðŸ“¥ ç™ºæ³¨CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv, file_name="orders_available_based.csv", mime="text/csv")

        st.markdown("---")
        st.subheader("ðŸ“¦ ä»•å…¥å…ˆåˆ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        for supplier, group in result_df.groupby("ä»•å…¥å…ˆ"):
            supplier_csv = group.to_csv(index=False).encode("utf-8-sig")
            st.download_button(
                label=f"ðŸ“¥ {supplier} ç”¨ ç™ºæ³¨CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=supplier_csv,
                file_name=f"orders_{supplier}.csv",
                mime="text/csv"
            )
    else:
        st.info("ç¾åœ¨ã€ç™ºæ³¨ãŒå¿…è¦ãªå•†å“ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

# å•†å“æƒ…å ±æ¤œç´¢
if mode == "ðŸ” å•†å“æƒ…å ±æ¤œç´¢":
    st.header("ðŸ” å•†å“æƒ…å ±DBæ¤œç´¢")

    @st.cache_resource
    def fetch_item_master():
        url = f"{SUPABASE_URL}/rest/v1/item_master?select=*"
        res = requests.get(url, headers=HEADERS)
        if res.status_code == 200:
            return pd.DataFrame(res.json())
        st.error("item_master ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return pd.DataFrame()

    if "df_master" not in st.session_state:
        st.session_state.df_master = fetch_item_master()

    df_master = st.session_state.df_master

    if df_master.empty:
        st.warning("å•†å“æƒ…å ±ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        st.stop()

    df_master["jan"] = df_master["jan"].astype(str)
    df_master["å•†å“ã‚³ãƒ¼ãƒ‰"] = df_master["å•†å“ã‚³ãƒ¼ãƒ‰"].astype(str)
    df_master["å•†å“å"] = df_master["å•†å“å"].astype(str)

    st.subheader("ðŸ”Ž æ¤œç´¢æ¡ä»¶")

    keyword = st.text_input("å•†å“åãƒ»å•†å“ã‚³ãƒ¼ãƒ‰ã§æ¤œç´¢", "")
    brand_filter = st.selectbox("ãƒ–ãƒ©ãƒ³ãƒ‰ã§çµžã‚Šè¾¼ã¿", ["ã™ã¹ã¦"] + sorted(df_master["ãƒ–ãƒ©ãƒ³ãƒ‰"].dropna().unique()))
    type_filter = st.selectbox("å–æ‰±åŒºåˆ†ã§çµžã‚Šè¾¼ã¿", ["ã™ã¹ã¦"] + sorted(df_master["å–æ‰±åŒºåˆ†"].dropna().unique()))

    df_view = df_master.copy()

    if keyword:
        df_view = df_view[
            df_view["å•†å“å"].str.contains(keyword, case=False, na=False) |
            df_view["å•†å“ã‚³ãƒ¼ãƒ‰"].str.contains(keyword, case=False, na=False)
        ]
    if brand_filter != "ã™ã¹ã¦":
        df_view = df_view[df_view["ãƒ–ãƒ©ãƒ³ãƒ‰"] == brand_filter]
    if type_filter != "ã™ã¹ã¦":
        df_view = df_view[df_view["å–æ‰±åŒºåˆ†"] == type_filter]

    view_cols = [
        "å•†å“ã‚³ãƒ¼ãƒ‰", "jan", "ãƒ©ãƒ³ã‚¯", "ãƒ–ãƒ©ãƒ³ãƒ‰", "å•†å“å", "å–æ‰±åŒºåˆ†",
        "åœ¨åº«", "åˆ©ç”¨å¯èƒ½", "ç™ºæ³¨æ¸ˆ", "ä»•å…¥ä¾¡æ ¼", "ã‚±ãƒ¼ã‚¹å…¥æ•°", "ç™ºæ³¨ãƒ­ãƒƒãƒˆ", "é‡é‡"
    ]
    rename_map = {
        "å•†å“ã‚³ãƒ¼ãƒ‰": "å•†å“ã‚³ãƒ¼ãƒ‰/å•†å“ç¼–å·",
        "jan": "JAN",
        "ãƒ©ãƒ³ã‚¯": "ãƒ©ãƒ³ã‚¯/ç­‰çº§",  # â†â˜…è¿½åŠ 
        "ãƒ–ãƒ©ãƒ³ãƒ‰": "ãƒ–ãƒ©ãƒ³ãƒ‰/å“ç‰Œ",
        "å•†å“å": "å•†å“å/å•†å“åç§°",
        "å–æ‰±åŒºåˆ†": "å–æ‰±åŒºåˆ†/åˆ†ç±»",
        "åœ¨åº«": "åœ¨åº«/åº“å­˜",
        "åˆ©ç”¨å¯èƒ½": "åˆ©ç”¨å¯èƒ½/å¯ç”¨åº“å­˜",
        "ç™ºæ³¨æ¸ˆ": "ç™ºæ³¨æ¸ˆ/å·²è®¢è´­",
        "ä»•å…¥ä¾¡æ ¼": "ä»•å…¥ä¾¡æ ¼/è¿›è´§ä»·",
        "ã‚±ãƒ¼ã‚¹å…¥æ•°": "ã‚±ãƒ¼ã‚¹å…¥æ•°/ç®±å…¥æ•°",
        "ç™ºæ³¨ãƒ­ãƒƒãƒˆ": "ç™ºæ³¨ãƒ­ãƒƒãƒˆ/è®¢è´­å•ä½",
        "é‡é‡": "é‡é‡/é‡é‡(g)"
    }
    available_cols = [col for col in view_cols if col in df_view.columns]

    st.subheader("ðŸ“‹ å•†å“ä¸€è¦§")
    display_df = df_view[available_cols].sort_values(by="å•†å“ã‚³ãƒ¼ãƒ‰")
    display_df = display_df.rename(columns=rename_map)
    st.dataframe(display_df)

    csv = display_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button("ðŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv, file_name="item_master_filtered.csv", mime="text/csv")



# å•†å“æƒ…å ±CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
if mode == "ðŸ“¤ å•†å“æƒ…å ±CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
    st.header("ðŸ“¤ å•†å“æƒ…å ±CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")

    def preprocess_item_master(df):
        df.rename(columns={
            "UPCã‚³ãƒ¼ãƒ‰": "jan",
            "è¡¨ç¤ºå": "å•†å“å",
            "ãƒ¡ãƒ¼ã‚«ãƒ¼å": "ãƒ–ãƒ©ãƒ³ãƒ‰",
            "ã‚¢ã‚¤ãƒ†ãƒ å®šç¾©åŽŸä¾¡": "ä»•å…¥ä¾¡æ ¼",
            "ã‚«ãƒ¼ãƒˆãƒ³å…¥æ•°": "ã‚±ãƒ¼ã‚¹å…¥æ•°",
            "ç™ºæ³¨ãƒ­ãƒƒãƒˆ": "ç™ºæ³¨ãƒ­ãƒƒãƒˆ",
            "ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸é‡é‡(g)": "é‡é‡",
            "æ‰‹æŒ": "åœ¨åº«",
            "åˆ©ç”¨å¯èƒ½": "åˆ©ç”¨å¯èƒ½",
            "æ³¨æ–‡æ¸ˆ": "ç™ºæ³¨æ¸ˆ",
            "åå‰": "å•†å“ã‚³ãƒ¼ãƒ‰",
            "å•†å“ãƒ©ãƒ³ã‚¯": "ãƒ©ãƒ³ã‚¯"  # â†â˜…è¿½åŠ 
        }, inplace=True)
        # ä¸è¦ãªåˆ—ã‚’å‰Šé™¤
        df.drop(columns=["å†…éƒ¨ID"], inplace=True, errors="ignore")

        for col in ["ã‚±ãƒ¼ã‚¹å…¥æ•°", "ç™ºæ³¨ãƒ­ãƒƒãƒˆ", "åœ¨åº«", "åˆ©ç”¨å¯èƒ½", "ç™ºæ³¨æ¸ˆ"]:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0).round().astype(int)
    
        df["jan"] = df["jan"].apply(normalize_jan)
        return df

    item_file = st.file_uploader("ðŸ§¾ item_master.csv ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type="csv")
    if item_file:
        temp_path = "/tmp/item_master.csv"
        with open(temp_path, "wb") as f:
            f.write(item_file.read())

        try:
            df = pd.read_csv(temp_path)
            df = preprocess_item_master(df)
        
            # Supabaseãƒ†ãƒ¼ãƒ–ãƒ«åˆæœŸåŒ–ï¼ˆæ—¢å­˜å‰Šé™¤ï¼‰
            requests.delete(f"{SUPABASE_URL}/rest/v1/item_master?id=gt.0", headers=HEADERS)
        
            # å•†å“ã‚³ãƒ¼ãƒ‰ã‚’ã‚­ãƒ¼ã«é‡è¤‡æŽ’é™¤
            df = df.drop_duplicates(subset=["å•†å“ã‚³ãƒ¼ãƒ‰"], keep="last")
        
            # ðŸ”½ IDä»˜ä¸Ž
            if "id" not in df.columns:
                df.insert(0, "id", range(1, len(df) + 1))
        
            # NaNãƒ»inf ã‚’ JSONäº’æ›ãª None ã«å¤‰æ›
            df = df.replace({pd.NA: None, pd.NaT: None, float('nan'): None, float('inf'): None, -float('inf'): None})
            df = df.where(pd.notnull(df), None)
        
            # ãƒãƒƒãƒPOST
            batch_size = 500
            for i in range(0, len(df), batch_size):
                batch = df.iloc[i:i+batch_size].to_dict(orient="records")
                res = requests.post(
                    f"{SUPABASE_URL}/rest/v1/item_master",
                    headers={**HEADERS, "Prefer": "resolution=merge-duplicates"},
                    json=batch
                )
                if res.status_code not in [200, 201]:
                    st.error(f"âŒ ãƒãƒƒãƒPOSTå¤±æ•—: {res.status_code} {res.text}")
                    break
            else:
                st.success(f"âœ… item_master ã« {len(df)} ä»¶ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")
        except Exception as e:
            st.error(f"âŒ item_master ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

if mode == "ðŸ’° ä»•å…¥ä¾¡æ ¼æ”¹å–„ãƒªã‚¹ãƒˆ":
    with st.spinner("ðŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        df_sales = fetch_table_cached("sales")
        df_purchase = fetch_table_cached("purchase_data")
        df_item = fetch_table_cached("item_master")
        df_sales["jan"] = df_sales["jan"].apply(normalize_jan)
        df_purchase["jan"] = df_purchase["jan"].apply(normalize_jan)
        df_item["jan"] = df_item["jan"].apply(normalize_jan)
        df_purchase["price"] = pd.to_numeric(df_purchase["price"], errors="coerce").fillna(0)

    # ç™ºæ³¨AIã‹ã‚‰ç¾åœ¨ã®ä»•å…¥ä¾¡æ ¼ã‚’å†ç¾
    current_prices = {}
    for _, row in df_sales.iterrows():
        jan = row["jan"]
        sold = row["quantity_sold"]
        stock = row.get("stock_available", 0)
        ordered = row.get("stock_ordered", 0)
        options = df_purchase[df_purchase["jan"] == jan].copy()
        if options.empty:
            continue

        if stock >= sold:
            need_qty = 0
        else:
            need_qty = sold - stock + math.ceil(sold * 0.5) - ordered
            need_qty = max(need_qty, 0)

        if need_qty <= 0:
            continue

        options = options[options["order_lot"] > 0]
        options["diff"] = (options["order_lot"] - need_qty).abs()

        smaller_lots = options[options["order_lot"] <= need_qty]

        if not smaller_lots.empty:
            best_option = smaller_lots.loc[smaller_lots["diff"].idxmin()]
        else:
            near_lots = options[(options["order_lot"] > need_qty) & (options["order_lot"] <= need_qty * 1.5) & (options["order_lot"] != 1)]
            if not near_lots.empty:
                best_option = near_lots.loc[near_lots["diff"].idxmin()]
            else:
                one_lot = options[options["order_lot"] == 1]
                if not one_lot.empty:
                    best_option = one_lot.iloc[0]
                else:
                    best_option = options.sort_values("order_lot").iloc[0]

        current_prices[jan] = best_option["price"]

    # æœ€å®‰å€¤å–å¾—
    min_prices = df_purchase.groupby("jan")["price"].min().to_dict()

    rows = []
    for jan, current_price in current_prices.items():
        if jan in min_prices and min_prices[jan] < current_price:
            item = df_item[df_item["jan"] == jan].head(1)
            if not item.empty:
                row = {
                    "å•†å“ã‚³ãƒ¼ãƒ‰": item.iloc[0].get("item_code", ""),
                    "JAN": jan,
                    "ãƒ–ãƒ©ãƒ³ãƒ‰": item.iloc[0].get("brand", ""),
                    "ç¾åœ¨ã®ä»•å…¥ä¾¡æ ¼": current_price,
                    "æœ€å®‰å€¤ã®ä»•å…¥ä¾¡æ ¼": min_prices[jan],
                    "å·®åˆ†": round(min_prices[jan] - current_price, 2)
                }
                rows.append(row)

    if rows:
        df_result = pd.DataFrame(rows)
        st.success(f"âœ… æ”¹å–„å¯¾è±¡å•†å“æ•°: {len(df_result)} ä»¶")
        st.dataframe(df_result)
        csv = df_result.to_csv(index=False).encode("utf-8-sig")
        st.download_button("ðŸ“¥ æ”¹å–„ãƒªã‚¹ãƒˆCSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv, file_name="price_improvement_list.csv", mime="text/csv")
    else:
        st.info("æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚‹å•†å“ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
