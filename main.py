import streamlit as st
import pandas as pd
import requests
import datetime
import os
import math
import re
import hashlib
import time
from zoneinfo import ZoneInfo
from streamlit_javascript import st_javascript

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="ç®¡ç†è£œåŠ©ã‚·ã‚¹ãƒ†ãƒ ", layout="wide")

# ğŸ”‘ ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ï¼ˆMD5ãƒãƒƒã‚·ãƒ¥åŒ–æ¸ˆï¼‰: ä¾‹ã€Œadmin123ã€
PASSWORD_HASH = "0f754d47528b6393d510866d26f508de"  # MD5("smikie0826")

# ğŸ§  ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

# ğŸª ã‚¯ãƒƒã‚­ãƒ¼ç¢ºèª
cookie = st_javascript("document.cookie")

# âœ… èªè¨¼æ¸ˆã¿ or ã‚¯ãƒƒã‚­ãƒ¼æœ‰åŠ¹ãªã‚‰ã‚¹ãƒ«ãƒ¼
if st.session_state.authenticated or ("auth_token=valid" in str(cookie)):
    st.session_state.authenticated = True

    # ğŸ”’ ãƒ­ã‚°ã‚¢ã‚¦ãƒˆæ©Ÿèƒ½ï¼ˆã‚¯ãƒƒã‚­ãƒ¼å‰Šé™¤ + ãƒªãƒ­ãƒ¼ãƒ‰ï¼‰
    if st.sidebar.button("ğŸ”’ ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ"):
        st.session_state.authenticated = False
        st_javascript("document.cookie = 'auth_token=; Max-Age=0'; location.reload();")

else:
    st.title("ğŸ” èªè¨¼ãŒå¿…è¦ã§ã™")

    # âœ… ã‚¨ãƒ³ã‚¿ãƒ¼ã‚­ãƒ¼å¯¾å¿œãƒ•ã‚©ãƒ¼ãƒ 
    with st.form("login_form"):
        password = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›", type="password")
        submitted = st.form_submit_button("ãƒ­ã‚°ã‚¤ãƒ³")

    if submitted:
        hashed = hashlib.md5(password.encode()).hexdigest()
        if hashed == PASSWORD_HASH:
            st.session_state.authenticated = True
            st_javascript("document.cookie = 'auth_token=valid; Max-Age=86400'")
            st.success("âœ… èªè¨¼æˆåŠŸã€ãƒªãƒ­ãƒ¼ãƒ‰ã—ã¾ã™")
            time.sleep(1)
            st.experimental_rerun()
        else:
            st.error("âŒ ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé•ã„ã¾ã™")

    st.stop()
    
# ğŸŸ¢ ã“ã“ã‹ã‚‰ã‚¢ãƒ—ãƒªã®ä¸­èº«ï¼ˆè¨€èªé¸æŠãªã©ï¼‰
language = st.sidebar.selectbox("è¨€èª / Language", ["æ—¥æœ¬èª", "ä¸­æ–‡"], key="language")

# ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡¨ç¤ºç”¨ãƒ©ãƒ™ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
TEXT = {
    "æ—¥æœ¬èª": {
        "title_order_ai": "ç®¡ç†è£œåŠ©ã‚·ã‚¹ãƒ†ãƒ ",
        "mode_select": "ãƒ¢ãƒ¼ãƒ‰ã‚’é¸ã‚“ã§ãã ã•ã„",
        "upload_csv": "CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        "order_ai": "ç™ºæ³¨AIåˆ¤å®š",
        "search_item": "å•†å“æƒ…å ±æ¤œç´¢",
        "upload_item": "å•†å“æƒ…å ±CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        "price_improve": "ä»•å…¥ä¾¡æ ¼æ”¹å–„ãƒªã‚¹ãƒˆ",
        "search_keyword": "å•†å“åãƒ»å•†å“ã‚³ãƒ¼ãƒ‰ã§æ¤œç´¢",
        "search_brand": "ãƒ¡ãƒ¼ã‚«ãƒ¼åã§çµã‚Šè¾¼ã¿",
        "search_type": "å–æ‰±åŒºåˆ†ã§çµã‚Šè¾¼ã¿",
        "product_list": "å•†å“ä¸€è¦§",
        "search_keyword": "å•†å“åãƒ»å•†å“ã‚³ãƒ¼ãƒ‰ã§æ¤œç´¢",
        "search_brand": "ãƒ¡ãƒ¼ã‚«ãƒ¼åã§çµã‚Šè¾¼ã¿",
        "search_type": "å–æ‰±åŒºåˆ†ã§çµã‚Šè¾¼ã¿",
        "search_rank": "ãƒ©ãƒ³ã‚¯ã§çµã‚Šè¾¼ã¿",
        "search_code": "å•†å“ã‚³ãƒ¼ãƒ‰ / JAN",
        "all": "ã™ã¹ã¦",
        "loading_data": "ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...",
        "multi_jan": "è¤‡æ•°JANå…¥åŠ›ï¼ˆæ”¹è¡Œã¾ãŸã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰"
    },
    "ä¸­æ–‡": {
        "title_order_ai": "ç®¡ç†æ”¯æŒç³»ç»Ÿ",
        "mode_select": "è¯·é€‰æ‹©æ¨¡å¼",
        "upload_csv": "ä¸Šä¼ CSV",
        "order_ai": "è®¢è´§AIåˆ¤æ–­",
        "search_item": "å•†å“ä¿¡æ¯æŸ¥è¯¢",
        "upload_item": "ä¸Šä¼ å•†å“ä¿¡æ¯CSV",
        "price_improve": "è¿›è´§ä»·æ ¼ä¼˜åŒ–æ¸…å•",
        "search_keyword": "æŒ‰å•†å“åç§°æˆ–ç¼–å·æœç´¢",
        "search_brand": "æŒ‰å“ç‰Œç­›é€‰",
        "search_type": "æŒ‰åˆ†ç±»ç­›é€‰",
        "product_list": "å•†å“åˆ—è¡¨",
        "search_keyword": "æŒ‰å•†å“åç§°æˆ–ç¼–å·æœç´¢",
        "search_brand": "æŒ‰åˆ¶é€ å•†ç­›é€‰",
        "search_type": "æŒ‰åˆ†ç±»ç­›é€‰",
        "search_rank": "æŒ‰ç­‰çº§ç­›é€‰",
        "search_code": "å•†å“ç¼–å· / æ¡ç ",
        "all": "å…¨éƒ¨",
        "loading_data": "ğŸ“Š æ­£åœ¨è¯»å–æ•°æ®...",
        "multi_jan": "æ‰¹é‡è¾“å…¥æ¡ç ï¼ˆæ¢è¡Œæˆ–é€—å·åˆ†éš”ï¼‰"
    }
}

# åˆ—åãƒãƒƒãƒ”ãƒ³ã‚°
COLUMN_NAMES = {
    "æ—¥æœ¬èª": {
        "å•†å“ã‚³ãƒ¼ãƒ‰": "å•†å“ã‚³ãƒ¼ãƒ‰",
        "jan": "JAN",
        "ãƒ©ãƒ³ã‚¯": "ãƒ©ãƒ³ã‚¯",
        "ãƒ¡ãƒ¼ã‚«ãƒ¼å": "ãƒ¡ãƒ¼ã‚«ãƒ¼å",
        "å•†å“å": "å•†å“å",
        "å–æ‰±åŒºåˆ†": "å–æ‰±åŒºåˆ†",
        "åœ¨åº«": "åœ¨åº«",
        "åˆ©ç”¨å¯èƒ½": "åˆ©ç”¨å¯èƒ½",
        "ç™ºæ³¨æ¸ˆ": "ç™ºæ³¨æ¸ˆ",
        "ä»•å…¥ä¾¡æ ¼": "ä»•å…¥ä¾¡æ ¼",
        "ã‚±ãƒ¼ã‚¹å…¥æ•°": "ã‚±ãƒ¼ã‚¹å…¥æ•°",
        "ç™ºæ³¨ãƒ­ãƒƒãƒˆ": "ç™ºæ³¨ãƒ­ãƒƒãƒˆ",
        "é‡é‡": "é‡é‡(g)",
    },
    "ä¸­æ–‡": {
        "å•†å“ã‚³ãƒ¼ãƒ‰": "å•†å“ç¼–å·",
        "jan": "æ¡ç ",
        "ãƒ©ãƒ³ã‚¯": "ç­‰çº§",
        "ãƒ¡ãƒ¼ã‚«ãƒ¼å": "åˆ¶é€ å•†åç§°",
        "å•†å“å": "å•†å“åç§°",
        "å–æ‰±åŒºåˆ†": "åˆ†ç±»",
        "åœ¨åº«": "åº“å­˜",
        "åˆ©ç”¨å¯èƒ½": "å¯ç”¨åº“å­˜",
        "ç™ºæ³¨æ¸ˆ": "å·²è®¢è´­",
        "ä»•å…¥ä¾¡æ ¼": "è¿›è´§ä»·",
        "ã‚±ãƒ¼ã‚¹å…¥æ•°": "ç®±å…¥æ•°",
        "ç™ºæ³¨ãƒ­ãƒƒãƒˆ": "è®¢è´­å•ä½",
        "é‡é‡": "é‡é‡(g)"
    }
}

# ğŸ” Supabaseæ¥ç¶šè¨­å®š
SUPABASE_URL_PRE = st.secrets["SUPABASE_URL"]
SUPABASE_KEY_PRE = st.secrets["SUPABASE_KEY"]
HEADERS_PRE = {
    "apikey": SUPABASE_KEY_PRE,
    "Authorization": f"Bearer {SUPABASE_KEY_PRE}",
    "Content-Type": "application/json"
}

# ğŸ“… item_master ã®æœ€æ–°æ›´æ–°æ—¥æ™‚ã‚’ JST è¡¨ç¤ºã§å–å¾—
def fetch_latest_item_update():
    url = f"{SUPABASE_URL_PRE}/rest/v1/item_master?select=updated_at&order=updated_at.desc&limit=1"
    res = requests.get(url, headers=HEADERS_PRE)
    if res.status_code == 200 and res.json():
        dt = pd.to_datetime(res.json()[0]["updated_at"], errors="coerce", utc=True)
        if pd.notnull(dt):
            dt_jst = dt.tz_convert(ZoneInfo("Asia/Tokyo"))
            return f"ï¼ˆ{dt_jst.strftime('%-m.%d update')}ï¼‰"
    return ""

def fetch_table(table_name):
    headers = {**HEADERS_PRE, "Prefer": "count=exact"}
    dfs = []
    offset = 0
    limit = 1000
    while True:
        url = f"{SUPABASE_URL_PRE}/rest/v1/{table_name}?select=*&offset={offset}&limit={limit}"
        res = requests.get(url, headers=headers)
        if res.status_code == 416 or not res.json():
            break
        if res.status_code not in [200, 206]:
            st.error(f"{table_name} ã®å–å¾—ã«å¤±æ•—: {res.status_code} / {res.text}")
            return pd.DataFrame()
        dfs.append(pd.DataFrame(res.json()))
        offset += limit
    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

item_master_update_text = fetch_latest_item_update()

# ã‚¿ã‚¤ãƒˆãƒ«è¡¨ç¤º
st.title(TEXT[language]["title_order_ai"])

MODE_KEYS = {
    "home": {
        "æ—¥æœ¬èª": "ğŸ  ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸",
        "ä¸­æ–‡": "ğŸ  ä¸»é¡µ"
    },
    "search_item": {
        "æ—¥æœ¬èª": f"ğŸ” å•†å“æƒ…å ±æ¤œç´¢{item_master_update_text}",
        "ä¸­æ–‡": f"ğŸ” å•†å“ä¿¡æ¯æŸ¥è¯¢{item_master_update_text}"
    },
    "price_improve": {
        "æ—¥æœ¬èª": "ä»•å…¥ä¾¡æ ¼æ”¹å–„ãƒªã‚¹ãƒˆ",
        "ä¸­æ–‡": "è¿›è´§ä»·æ ¼ä¼˜åŒ–æ¸…å•"
    },
    "monthly_sales": {
        "æ—¥æœ¬èª": "è²©å£²å®Ÿç¸¾ï¼ˆç›´è¿‘1ãƒ¶æœˆï¼‰",
        "ä¸­æ–‡": "é”€å”®ä¸šç»©ï¼ˆæœ€è¿‘ä¸€ä¸ªæœˆï¼‰"
    },
    "order_ai": {
        "æ—¥æœ¬èª": "ç™ºæ³¨AIåˆ¤å®š",
        "ä¸­æ–‡": "è®¢è´§AIåˆ¤æ–­"
    },
    "rank_a_check": {
        "æ—¥æœ¬èª": "ğŸ…°ï¸ Aãƒ©ãƒ³ã‚¯å•†å“ç¢ºèª",
        "ä¸­æ–‡": "ğŸ…°ï¸ Aç­‰çº§å•†å“æ£€æŸ¥"
    },
    "purchase_history": {
        "æ—¥æœ¬èª": "ğŸ“œ ç™ºæ³¨å±¥æ­´",
        "ä¸­æ–‡": "ğŸ“œ è®¢è´§è®°å½•"
    },
    "difficult_items": {
        "æ—¥æœ¬èª": "å…¥è·å›°é›£å•†å“",
        "ä¸­æ–‡": "è¿›è´§å›°éš¾å•†å“"
    },
    "csv_upload": {
        "æ—¥æœ¬èª": "CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        "ä¸­æ–‡": "ä¸Šä¼ CSV"
    },
}


mode_labels = [v[language] for v in MODE_KEYS.values()]
mode_selection = st.sidebar.radio(TEXT[language]["mode_select"], mode_labels, index=0)
mode = next(key for key, labels in MODE_KEYS.items() if labels[language] == mode_selection)


# å„ãƒ¢ãƒ¼ãƒ‰ã®å‡¦ç†åˆ†å²
if mode == "home":
    st.subheader("ğŸ  " + TEXT[language]["title_order_ai"])

    if language == "æ—¥æœ¬èª":
        st.markdown("""
        #### ã”åˆ©ç”¨ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚
        å·¦ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰æ“ä½œã‚’é¸ã‚“ã§ãã ã•ã„ã€‚
        - ğŸ“¦ ç™ºæ³¨AI
        - ğŸ“¤ CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        - ğŸ” å•†å“æƒ…å ±æ¤œç´¢
        - ğŸ’° ä»•å…¥ä¾¡æ ¼æ”¹å–„ãƒªã‚¹ãƒˆ
        """)
    else:
        st.markdown("""
        #### æ„Ÿè°¢æ‚¨çš„ä½¿ç”¨ã€‚
        è¯·ä»å·¦ä¾§èœå•ä¸­é€‰æ‹©æ“ä½œæ¨¡å¼ã€‚
        - ğŸ“¦ è®¢è´§AI
        - ğŸ“¤ ä¸Šä¼ CSV
        - ğŸ” å•†å“ä¿¡æ¯æŸ¥è¯¢
        - ğŸ’° è¿›è´§ä»·æ ¼ä¼˜åŒ–æ¸…å•
        """)

elif mode == "order_ai":
    st.subheader("ğŸ“¦ ç™ºæ³¨AIãƒ¢ãƒ¼ãƒ‰")

    ai_mode = st.radio("ç™ºæ³¨AIãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠ", ["é€šå¸¸ãƒ¢ãƒ¼ãƒ‰", "JDãƒ¢ãƒ¼ãƒ‰"], index=0)

    if st.button("ğŸ¤– è¨ˆç®—ã‚’é–‹å§‹ã™ã‚‹"):
        SUPABASE_URL = st.secrets["SUPABASE_URL"]
        SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
        HEADERS = {
            "apikey": SUPABASE_KEY,
            "Authorization": f"Bearer {SUPABASE_KEY}",
            "Content-Type": "application/json"
        }

        def fetch_table(table_name):
            headers = {**HEADERS, "Prefer": "count=exact"}
            dfs = []
            offset = 0
            limit = 1000
            while True:
                url = f"{SUPABASE_URL}/rest/v1/{table_name}?select=*&offset={offset}&limit={limit}"
                res = requests.get(url, headers=headers)
                if res.status_code == 416 or not res.json():
                    break
                if res.status_code not in [200, 206]:
                    st.error(f"{table_name} ã®å–å¾—ã«å¤±æ•—: {res.status_code} / {res.text}")
                    return pd.DataFrame()
                dfs.append(pd.DataFrame(res.json()))
                offset += limit
            return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

        def normalize_jan(x):
            try:
                return str(x).strip()
            except:
                return ""

        with st.spinner("ğŸ“¦ ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
            df_sales = fetch_table("sales")
            df_purchase = fetch_table("purchase_data")
            df_master = fetch_table("item_master")
            if ai_mode == "JDãƒ¢ãƒ¼ãƒ‰":
                df_warehouse = fetch_table("warehouse_stock")

        if df_sales.empty or df_purchase.empty or df_master.empty:
            st.warning("å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
            st.stop()
        if ai_mode == "JDãƒ¢ãƒ¼ãƒ‰" and df_warehouse.empty:
            st.warning("JDãƒ¢ãƒ¼ãƒ‰ç”¨ã® warehouse_stock ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
            st.stop()

        df_sales["jan"] = df_sales["jan"].apply(normalize_jan)
        df_purchase["jan"] = df_purchase["jan"].apply(normalize_jan)
        df_master["jan"] = df_master["jan"].apply(normalize_jan)
        if ai_mode == "JDãƒ¢ãƒ¼ãƒ‰":
            df_warehouse["product_code"] = df_warehouse["product_code"].apply(normalize_jan)
            df_warehouse["stock_available"] = pd.to_numeric(df_warehouse["stock_available"], errors="coerce").fillna(0).astype(int)

        df_sales["quantity_sold"] = pd.to_numeric(df_sales["quantity_sold"], errors="coerce").fillna(0).astype(int)
        df_sales["stock_available"] = pd.to_numeric(df_sales["stock_available"], errors="coerce").fillna(0).astype(int)

        # ğŸ”„ item_master ã‹ã‚‰ç™ºæ³¨æ¸ˆã‚’ãƒãƒ¼ã‚¸
        df_master["ç™ºæ³¨æ¸ˆ"] = pd.to_numeric(df_master["ç™ºæ³¨æ¸ˆ"], errors="coerce").fillna(0).astype(int)
        
        # ğŸ”„ ğŸ”¥ ã“ã“ã‹ã‚‰è¿½åŠ ï¼šã€Œä¸Šæµ·å‘ã‘ã€ç™ºæ³¨åˆ†ã‚’å·®ã—å¼•ãå‡¦ç†
        df_history["quantity"] = pd.to_numeric(df_history["quantity"], errors="coerce").fillna(0).astype(int)
        df_history["memo"] = df_history["memo"].astype(str).fillna("")
        df_history["jan"] = df_history["jan"].apply(normalize_jan)
        
        df_shanghai = df_history[df_history["memo"].str.contains("ä¸Šæµ·", na=False)]
        df_shanghai_grouped = df_shanghai.groupby("jan")["quantity"].sum().reset_index(name="shanghai_quantity")
        
        df_master = df_master.merge(df_shanghai_grouped, on="jan", how="left")
        df_master["shanghai_quantity"] = df_master["shanghai_quantity"].fillna(0).astype(int)
        df_master["ç™ºæ³¨æ¸ˆ_ä¿®æ­£å¾Œ"] = (df_master["ç™ºæ³¨æ¸ˆ"] - df_master["shanghai_quantity"]).clip(lower=0)
        
        # ä¿®æ­£å¾Œã®ç™ºæ³¨æ¸ˆã‚’ df_sales ã«åæ˜ 
        df_sales = df_sales.merge(
            df_master[["jan", "ç™ºæ³¨æ¸ˆ_ä¿®æ­£å¾Œ"]],
            on="jan",
            how="left"
        )
        df_sales["ç™ºæ³¨æ¸ˆ"] = df_sales["ç™ºæ³¨æ¸ˆ_ä¿®æ­£å¾Œ"].fillna(0).astype(int)

        df_purchase["order_lot"] = pd.to_numeric(df_purchase["order_lot"], errors="coerce").fillna(0).astype(int)
        df_purchase["price"] = pd.to_numeric(df_purchase["price"], errors="coerce").fillna(0)

        rank_multiplier = {
            "Aãƒ©ãƒ³ã‚¯": 1.0,
            "Bãƒ©ãƒ³ã‚¯": 1.2,
            "Cãƒ©ãƒ³ã‚¯": 1.0,
            "TEST": 1.5
        }

        from datetime import date, timedelta
        import math

        with st.spinner("ğŸ¤– ç™ºæ³¨AIãŒè¨ˆç®—ã‚’ã—ã¦ã„ã¾ã™..."):
            df_history = fetch_table("purchase_history")
            df_history["order_date"] = pd.to_datetime(df_history["order_date"], errors="coerce").dt.date
            today = date.today()
            yesterday = today - timedelta(days=1)
            recent_jans = df_history[df_history["order_date"].isin([today, yesterday])]["jan"].dropna().astype(str).apply(normalize_jan).unique().tolist()

            results = []
            for _, row in df_sales.iterrows():
                jan = row["jan"]
                sold = row["quantity_sold"]

                if ai_mode == "JDãƒ¢ãƒ¼ãƒ‰":
                    stock_row = df_warehouse[df_warehouse["product_code"] == jan]
                    stock = stock_row["stock_available"].values[0] if not stock_row.empty else 0
                else:
                    stock = row.get("stock_available", 0)

                ordered = row["ç™ºæ³¨æ¸ˆ"]

                rank_row = df_master[df_master["jan"] == jan]
                rank = rank_row["ãƒ©ãƒ³ã‚¯"].values[0] if not rank_row.empty and "ãƒ©ãƒ³ã‚¯" in rank_row else ""
                multiplier = rank_multiplier.get(rank, 1.0)

                if rank == "Aãƒ©ãƒ³ã‚¯":
                    if (stock + ordered) < sold:
                        need_qty_raw = math.ceil(sold * 1.2)
                    else:
                        need_qty_raw = 0
                else:
                    need_qty_raw = math.ceil(sold * multiplier) - stock - ordered

                if stock <= 1 and sold >= 1 and need_qty_raw <= 0:
                    need_qty = 1
                else:
                    need_qty = max(need_qty_raw, 0)

                if jan in recent_jans:
                    continue

                if rank == "Aãƒ©ãƒ³ã‚¯":
                    reorder_point = max(math.floor(sold * 1.0), 1)
                elif rank == "Bãƒ©ãƒ³ã‚¯":
                    reorder_point = max(math.floor(sold * 0.9), 1)
                else:
                    reorder_point = max(math.floor(sold * 0.7), 1)

                current_total = stock + ordered
                if current_total > reorder_point:
                    continue
                if need_qty <= 0:
                    continue

                options = df_purchase[df_purchase["jan"] == jan].copy()
                if options.empty:
                    continue
                options = options[options["order_lot"] > 0]

                if rank == "Aãƒ©ãƒ³ã‚¯":
                    bigger_lots = options[options["order_lot"] >= need_qty]
                    if not bigger_lots.empty:
                        best_option = bigger_lots.sort_values("order_lot", ascending=False).iloc[0]
                    else:
                        best_option = options.sort_values("order_lot", ascending=False).iloc[0]
                else:
                    options["diff"] = (options["order_lot"] - need_qty).abs()
                    smaller_lots = options[options["order_lot"] <= need_qty]
                    if not smaller_lots.empty:
                        best_option = smaller_lots.loc[smaller_lots["diff"].idxmin()]
                    else:
                        near_lots = options[(options["order_lot"] > need_qty) & (options["order_lot"] <= need_qty * 1.5) & (options["order_lot"] != 1)]
                        if not near_lots.empty:
                            best_option = near_lots.loc[near_lots["diff"].idxmin()]
                        else:
                            one_lot = options[options["order_lot"] == 1]
                            if not one_lot.empty:
                                best_option = one_lot.iloc[0]
                            else:
                                best_option = options.sort_values("order_lot").iloc[0]

                if rank == "Aãƒ©ãƒ³ã‚¯":
                    sets = math.ceil(need_qty_raw / best_option["order_lot"])
                else:
                    sets = math.ceil(need_qty / best_option["order_lot"])
                qty = sets * best_option["order_lot"]
                total_cost = qty * best_option["price"]

                results.append({
                    "jan": jan,
                    "è²©å£²å®Ÿç¸¾": sold,
                    "åœ¨åº«": stock,
                    "ç™ºæ³¨æ¸ˆ": ordered,
                    "ç†è«–å¿…è¦æ•°": need_qty_raw if rank == "Aãƒ©ãƒ³ã‚¯" else need_qty,
                    "ç™ºæ³¨æ•°": qty,
                    "ãƒ­ãƒƒãƒˆ": best_option["order_lot"],
                    "æ•°é‡": round(qty / best_option["order_lot"], 2),
                    "å˜ä¾¡": best_option["price"],
                    "ç·é¡": total_cost,
                    "ä»•å…¥å…ˆ": best_option.get("supplier", "ä¸æ˜"),
                    "ãƒ©ãƒ³ã‚¯": rank
                })

            if results:
                result_df = pd.DataFrame(results)
                df_master["å•†å“ã‚³ãƒ¼ãƒ‰"] = df_master["å•†å“ã‚³ãƒ¼ãƒ‰"].astype(str).str.strip()
                result_df["jan"] = result_df["jan"].astype(str).str.strip()
                df_temp = df_master[["å•†å“ã‚³ãƒ¼ãƒ‰", "å•†å“å", "å–æ‰±åŒºåˆ†"]].copy()
                df_temp.rename(columns={"å•†å“ã‚³ãƒ¼ãƒ‰": "jan"}, inplace=True)
                result_df = pd.merge(result_df, df_temp, on="jan", how="left")
                if "å•†å“å" in result_df.columns:
                    result_df = result_df[result_df["å•†å“å"].notna()]
                if "å–æ‰±åŒºåˆ†" in result_df.columns:
                    result_df = result_df[result_df["å–æ‰±åŒºåˆ†"] != "å–æ‰±ä¸­æ­¢"]
                else:
                    st.warning("âš ï¸ã€å–æ‰±åŒºåˆ†ã€åˆ—ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
                column_order = ["jan", "å•†å“å", "ãƒ©ãƒ³ã‚¯", "è²©å£²å®Ÿç¸¾", "åœ¨åº«", "ç™ºæ³¨æ¸ˆ", "ç†è«–å¿…è¦æ•°", "ç™ºæ³¨æ•°", "ãƒ­ãƒƒãƒˆ", "æ•°é‡", "å˜ä¾¡", "ç·é¡", "ä»•å…¥å…ˆ"]
                result_df = result_df[[col for col in column_order if col in result_df.columns]]
                st.success(f"âœ… ç™ºæ³¨å¯¾è±¡: {len(result_df)} ä»¶")
                st.dataframe(result_df)

                csv = result_df.to_csv(index=False).encode("utf-8-sig")
                st.download_button("ğŸ“¥ ç™ºæ³¨CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv, file_name="orders_available_based.csv", mime="text/csv")

                st.markdown("---")
                st.subheader("ğŸ“¦ ä»•å…¥å…ˆåˆ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
                for supplier, group in result_df.groupby("ä»•å…¥å…ˆ"):
                    supplier_csv = group.to_csv(index=False).encode("utf-8-sig")
                    st.download_button(
                        label=f"ğŸ“¥ {supplier} ç”¨ ç™ºæ³¨CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=supplier_csv,
                        file_name=f"orders_{supplier}.csv",
                        mime="text/csv"
                    )
            else:
                st.info("ç¾åœ¨ã€ç™ºæ³¨ãŒå¿…è¦ãªå•†å“ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")




# ğŸ” å•†å“æƒ…å ±æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ -----------------------------
elif mode == "search_item":
    st.subheader("ğŸ” å•†å“æƒ…å ±æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰")

    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    HEADERS = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json"
    }

    def fetch_item_master():
        headers = {**HEADERS, "Prefer": "count=exact"}
        dfs = []
        offset, limit = 0, 1000
        while True:
            url = f"{SUPABASE_URL}/rest/v1/item_master?select=*&offset={offset}&limit={limit}"
            res = requests.get(url, headers=headers)
            if res.status_code == 416 or not res.json():
                break
            if res.status_code not in [200, 206]:
                st.error(f"item_master ã®å–å¾—ã«å¤±æ•—: {res.status_code} / {res.text}")
                return pd.DataFrame()
            dfs.append(pd.DataFrame(res.json()))
            offset += limit
        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

    def fetch_warehouse_stock():
        headers = {**HEADERS, "Prefer": "count=exact"}
        dfs = []
        offset, limit = 0, 1000
        while True:
            url = f"{SUPABASE_URL}/rest/v1/warehouse_stock?select=*&offset={offset}&limit={limit}"
            res = requests.get(url, headers=headers)
            if res.status_code == 416 or not res.json():
                break
            if res.status_code not in [200, 206]:
                st.error(f"warehouse_stock ã®å–å¾—ã«å¤±æ•—: {res.status_code} / {res.text}")
                return pd.DataFrame()
            dfs.append(pd.DataFrame(res.json()))
            offset += limit
        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

    df_master = fetch_item_master()
    df_warehouse = fetch_warehouse_stock()

    if df_master.empty:
        st.warning("å•†å“æƒ…å ±ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        st.stop()

    df_master["jan"] = df_master["jan"].astype(str)
    df_master["å•†å“ã‚³ãƒ¼ãƒ‰"] = df_master["å•†å“ã‚³ãƒ¼ãƒ‰"].astype(str)
    df_master["å•†å“å"] = df_master["å•†å“å"].astype(str)

    df_warehouse["product_code"] = df_warehouse["product_code"].astype(str)
    df_warehouse["stock_available"] = pd.to_numeric(df_warehouse["stock_available"], errors="coerce").fillna(0).astype(int)
    df_warehouse["stock_total"] = df_warehouse["stock_available"]  # stock_total ãŒãªã‘ã‚Œã°åŒã˜å€¤ã‚’ä»£å…¥

    df_master = df_master.merge(
        df_warehouse[["product_code", "stock_total", "stock_available"]],
        left_on="jan", right_on="product_code",
        how="left"
    )
    df_master["åœ¨åº«"] = df_master["stock_total"].fillna(0).astype(int)
    df_master["åˆ©ç”¨å¯èƒ½"] = df_master["stock_available"].fillna(0).astype(int)

    col1, col2 = st.columns(2)
    with col1:
        keyword_name = st.text_input(TEXT[language]["search_keyword"], "")
        keyword_code = st.text_input(TEXT[language]["search_code"], "")

    with col2:
        jan_filter_multi = st.text_area(
            TEXT[language]["multi_jan"],
            placeholder="ä¾‹:\n4901234567890\n4987654321098",
            height=120,
        )

    maker_filter = st.selectbox(
        TEXT[language]["search_brand"],
        [TEXT[language]["all"]] + sorted(df_master["ãƒ¡ãƒ¼ã‚«ãƒ¼å"].dropna().unique())
    )
    rank_filter = st.selectbox(
        TEXT[language]["search_rank"],
        [TEXT[language]["all"]] + sorted(df_master["ãƒ©ãƒ³ã‚¯"].dropna().unique())
    )
    type_filter = st.selectbox(
        TEXT[language]["search_type"],
        [TEXT[language]["all"]] + sorted(df_master["å–æ‰±åŒºåˆ†"].dropna().unique())
    )

    import re
    jan_list = [j.strip() for j in re.split(r"[,\n\r]+", jan_filter_multi) if j.strip()]
    df_view = df_master.copy()

    if jan_list:
        df_view = df_view[df_view["jan"].isin(jan_list)]
    elif keyword_code:
        df_view = df_view[
            df_view["å•†å“ã‚³ãƒ¼ãƒ‰"].str.contains(keyword_code, case=False, na=False) |
            df_view["jan"].str.contains(keyword_code, case=False, na=False)
        ]
    if keyword_name:
        df_view = df_view[df_view["å•†å“å"].str.contains(keyword_name, case=False, na=False)]
    if maker_filter != TEXT[language]["all"]:
        df_view = df_view[df_view["ãƒ¡ãƒ¼ã‚«ãƒ¼å"] == maker_filter]
    if rank_filter != TEXT[language]["all"]:
        df_view = df_view[df_view["ãƒ©ãƒ³ã‚¯"] == rank_filter]
    if type_filter != TEXT[language]["all"]:
        df_view = df_view[df_view["å–æ‰±åŒºåˆ†"] == type_filter]

    view_cols = [
        "å•†å“ã‚³ãƒ¼ãƒ‰", "jan", "ãƒ©ãƒ³ã‚¯", "ãƒ¡ãƒ¼ã‚«ãƒ¼å", "å•†å“å", "å–æ‰±åŒºåˆ†",
        "åœ¨åº«", "ç™ºæ³¨æ¸ˆ", "ä»•å…¥ä¾¡æ ¼", "ã‚±ãƒ¼ã‚¹å…¥æ•°", "ç™ºæ³¨ãƒ­ãƒƒãƒˆ", "é‡é‡"
    ]
    available_cols = [c for c in view_cols if c in df_view.columns]

    display_df = (
        df_view[available_cols]
        .sort_values(by="å•†å“ã‚³ãƒ¼ãƒ‰")
        .rename(columns=COLUMN_NAMES[language])
    )

    row_count = len(display_df)
    h_left, h_right = st.columns([1, 0.15])
    h_left.subheader(TEXT[language]["product_list"])
    h_right.markdown(
        f"<h4 style='text-align:right; margin-top: 0.6em;'>{row_count:,}ä»¶</h4>",
        unsafe_allow_html=True
    )

    st.dataframe(display_df, use_container_width=True)

    csv = display_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        "ğŸ“… CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv,
        file_name="item_master_filtered.csv",
        mime="text/csv",
    )

elif mode == "purchase_history":
    st.subheader("ğŸ“œ ç™ºæ³¨å±¥æ­´")

    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    HEADERS = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json"
    }

    # ---------- ğŸ” æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ  ----------
    col1, col2 = st.columns(2)

    with col1:
        # å¾“æ¥ã®å˜ä¸€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰
        jan_filter_single = st.text_input("ğŸ” JANã§æ¤œç´¢ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", "")
        order_id_filter   = st.text_input("ğŸ” Orderâ€¯IDã§æ¤œç´¢ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", "")

    with col2:
        jan_filter_multi = st.text_area(
            TEXT[language]["multi_jan"],                # â†å‹•çš„ãƒ©ãƒ™ãƒ«
            placeholder="ä¾‹:\n4901234567890\n4987654321098",
            height=120,
        )

    @st.cache_data(ttl=60)
    def fetch_purchase_history():
        url = f"{SUPABASE_URL}/rest/v1/purchase_history?select=*"
        res = requests.get(url, headers=HEADERS)
        if res.status_code == 200:
            return pd.DataFrame(res.json())
        st.error("âŒ ç™ºæ³¨å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return pd.DataFrame()

    df = fetch_purchase_history()

    if df.empty:
        st.info("ç™ºæ³¨å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        st.stop()

    # ------------- ğŸ§¹ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° -------------
    import re

    df["jan"]        = df["jan"].astype(str)
    df["order_date"] = pd.to_datetime(df["order_date"], errors="coerce").dt.date

    # â‘  è¤‡æ•°â€¯JAN ãƒªã‚¹ãƒˆã‚’æ•´å½¢
    jan_list = [j.strip() for j in re.split(r"[,\n\r]+", jan_filter_multi) if j.strip()]

    if jan_list:  # æœ€å„ªå…ˆ
        df = df[df["jan"].isin(jan_list)]
    elif jan_filter_single:
        df = df[df["jan"].str.contains(jan_filter_single, na=False)]

    if order_id_filter:
        df = df[df["order_id"].astype(str).str.contains(order_id_filter, na=False)]

    # ------------- ğŸ“‹ è¡¨ç¤º -------------
    df_show = df[["jan", "quantity", "order_date", "order_id"]].sort_values("jan")

    st.success(f"âœ… ç™ºæ³¨å±¥æ­´ ä»¶æ•°: {len(df_show)} ä»¶")
    st.dataframe(df_show, use_container_width=True)




elif mode == "price_improve":
    st.subheader("ğŸ’° " + TEXT[language]["price_improve"])

    # èªè¨¼ç”¨ãƒ˜ãƒƒãƒ€ãƒ¼å®šç¾©
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    HEADERS = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json"
    }

    def fetch_table(table_name):
        headers = {**HEADERS, "Prefer": "count=exact"}
        dfs = []
        offset = 0
        limit = 1000
        while True:
            url = f"{SUPABASE_URL}/rest/v1/{table_name}?select=*&offset={offset}&limit={limit}"
            res = requests.get(url, headers=headers)
            if res.status_code == 416 or not res.json():
                break
            if res.status_code not in [200, 206]:
                st.error(f"{table_name} ã®å–å¾—ã«å¤±æ•—: {res.status_code} / {res.text}")
                return pd.DataFrame()
            dfs.append(pd.DataFrame(res.json()))
            offset += limit
        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

    with st.spinner("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        df_sales = fetch_table("sales")
        df_purchase = fetch_table("purchase_data")
        df_item = fetch_table("item_master")

    def normalize_jan(x):
        try:
            if re.fullmatch(r"\d+(\.0+)?", str(x)):
                return str(int(float(x)))
            else:
                return str(x).strip()
        except:
            return ""

    # æ•´å½¢
    df_sales["jan"] = df_sales["jan"].apply(normalize_jan)
    df_purchase["jan"] = df_purchase["jan"].apply(normalize_jan)
    df_item["jan"] = df_item["jan"].apply(normalize_jan)
    df_purchase["price"] = pd.to_numeric(df_purchase["price"], errors="coerce").fillna(0)

    # ç¾åœ¨ä¾¡æ ¼åˆ¤å®š
    current_prices = {}
    for _, row in df_sales.iterrows():
        jan = row["jan"]
        sold = row["quantity_sold"]
        stock = row.get("stock_available", 0)
        ordered = row.get("stock_ordered", 0)
        options = df_purchase[df_purchase["jan"] == jan].copy()
        if options.empty:
            continue

        if stock >= sold:
            need_qty = 0
        else:
            need_qty = sold - stock + math.ceil(sold * 0.5) - ordered
            need_qty = max(need_qty, 0)

        if need_qty <= 0:
            continue

        options = options[options["order_lot"] > 0]
        options["diff"] = (options["order_lot"] - need_qty).abs()

        smaller_lots = options[options["order_lot"] <= need_qty]
        if not smaller_lots.empty:
            best_option = smaller_lots.loc[smaller_lots["diff"].idxmin()]
        else:
            near_lots = options[(options["order_lot"] > need_qty) & (options["order_lot"] <= need_qty * 1.5) & (options["order_lot"] != 1)]
            if not near_lots.empty:
                best_option = near_lots.loc[near_lots["diff"].idxmin()]
            else:
                one_lot = options[options["order_lot"] == 1]
                if not one_lot.empty:
                    best_option = one_lot.iloc[0]
                else:
                    best_option = options.sort_values("order_lot").iloc[0]

        current_prices[jan] = best_option["price"]

    # æœ€å®‰å€¤å–å¾—
    min_prices = df_purchase.groupby("jan")["price"].min().to_dict()

    rows = []
    for jan, current_price in current_prices.items():
        if jan in min_prices and min_prices[jan] < current_price:
            item = df_item[df_item["jan"] == jan].head(1)
            if not item.empty:
                row = {
                    "å•†å“ã‚³ãƒ¼ãƒ‰": item.iloc[0].get("item_code", ""),
                    "JAN": jan,
                    "ãƒ¡ãƒ¼ã‚«ãƒ¼å": item.iloc[0].get("brand", ""),
                    "ç¾åœ¨ã®ä»•å…¥ä¾¡æ ¼": current_price,
                    "æœ€å®‰å€¤ã®ä»•å…¥ä¾¡æ ¼": min_prices[jan],
                    "å·®åˆ†": round(min_prices[jan] - current_price, 2)
                }
                rows.append(row)

    if rows:
        df_result = pd.DataFrame(rows)

        # âœ… å¤šè¨€èªã‚«ãƒ©ãƒ åã«å¤‰æ›
        column_translation = {
            "æ—¥æœ¬èª": {
                "å•†å“ã‚³ãƒ¼ãƒ‰": "å•†å“ã‚³ãƒ¼ãƒ‰",
                "JAN": "JAN",
                "ãƒ¡ãƒ¼ã‚«ãƒ¼å": "ãƒ¡ãƒ¼ã‚«ãƒ¼å",
                "ç¾åœ¨ã®ä»•å…¥ä¾¡æ ¼": "ç¾åœ¨ã®ä»•å…¥ä¾¡æ ¼",
                "æœ€å®‰å€¤ã®ä»•å…¥ä¾¡æ ¼": "æœ€å®‰å€¤ã®ä»•å…¥ä¾¡æ ¼",
                "å·®åˆ†": "å·®åˆ†"
            },
            "ä¸­æ–‡": {
                "å•†å“ã‚³ãƒ¼ãƒ‰": "å•†å“ç¼–å·",
                "JAN": "æ¡ç ",
                "ãƒ¡ãƒ¼ã‚«ãƒ¼å": "åˆ¶é€ å•†åç§°",
                "ç¾åœ¨ã®ä»•å…¥ä¾¡æ ¼": "å½“å‰è¿›è´§ä»·",
                "æœ€å®‰å€¤ã®ä»•å…¥ä¾¡æ ¼": "æœ€ä½è¿›è´§ä»·",
                "å·®åˆ†": "å·®é¢"
            }
        }

        df_result = df_result.rename(columns=column_translation[language])

        st.success(f"âœ… æ”¹å–„å¯¾è±¡å•†å“æ•°: {len(df_result)} ä»¶")
        st.dataframe(df_result)

        csv = df_result.to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            "ğŸ“¥ æ”¹å–„ãƒªã‚¹ãƒˆCSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv,
            file_name="price_improvement_list.csv",
            mime="text/csv",
            key="price_improve_download"  # ğŸ”‘ è¤‡æ•°å‘¼ã³å‡ºã—é˜²æ­¢
        )
    else:
        st.info("æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚‹å•†å“ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")


if mode == "csv_upload":
    st.subheader("ğŸ“„ CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰")

    def normalize_jan(x):
        try:
            return str(x).strip()
        except:
            return ""

    input_password = st.text_input("ğŸ”‘ ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", type="password")
    correct_password = st.secrets.get("UPLOAD_PASSWORD", "pass1234")

    if input_password != correct_password:
        st.warning("æ­£ã—ã„ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    HEADERS = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json"
    }

    def preprocess_csv(df, table):
        df.columns = df.columns.str.replace("ã€€", "").str.replace("\ufeff", "").str.strip()

        if table == "sales":
            st.write("ğŸ“ sales åˆ—å:", df.columns.tolist())
            item_col = None
            for col in df.columns:
                if "ã‚¢ã‚¤ãƒ†ãƒ " in col:
                    item_col = col
                    break
            if item_col:
                df.rename(columns={item_col: "jan"}, inplace=True)
            else:
                raise ValueError(f"âŒ 'ã‚¢ã‚¤ãƒ†ãƒ ' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼åˆ—å: {df.columns.tolist()}")

            df.rename(columns={
                "å–æ‰±åŒºåˆ†": "handling_type",
                "è²©å£²æ•°é‡": "quantity_sold",
                "ç¾åœ¨ã®æ‰‹æŒæ•°é‡": "stock_total",
                "ç¾åœ¨ã®åˆ©ç”¨å¯èƒ½æ•°é‡": "stock_available",
                "ç¾åœ¨ã®æ³¨æ–‡æ¸ˆæ•°é‡": "stock_ordered"
            }, inplace=True)

            for col in ["quantity_sold", "stock_total", "stock_available", "stock_ordered"]:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0).astype(int)

            df["jan"] = df["jan"].apply(normalize_jan)

        elif table == "purchase_data":
            for col in ["order_lot", "price"]:
                if col in df.columns:
                    df[col] = df[col].astype(str).str.replace(",", "")
                    df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0)
                    if col == "order_lot":
                        df[col] = df[col].round().astype(int)
            df["jan"] = df["jan"].apply(normalize_jan)

        elif table == "item_master":
            st.write("ğŸ“ item_master åˆ—å:", df.columns.tolist())
            upc_col = None
            for col in df.columns:
                if "UPC" in col:
                    upc_col = col
                    break

            if upc_col:
                df.rename(columns={upc_col: "jan"}, inplace=True)
            else:
                raise ValueError(f"âŒ 'UPCã‚³ãƒ¼ãƒ‰' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼åˆ—å: {df.columns.tolist()}")

            df.rename(columns={
                "è¡¨ç¤ºå": "å•†å“å",
                "ãƒ¡ãƒ¼ã‚«ãƒ¼å": "ãƒ¡ãƒ¼ã‚«ãƒ¼å",
                "ã‚¢ã‚¤ãƒ†ãƒ å®šç¾©åŸä¾¡": "ä»•å…¥ä¾¡æ ¼",
                "ã‚«ãƒ¼ãƒˆãƒ³å…¥æ•°": "ã‚±ãƒ¼ã‚¹å…¥æ•°",
                "ç™ºæ³¨ãƒ­ãƒƒãƒˆ": "ç™ºæ³¨ãƒ­ãƒƒãƒˆ",
                "ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸é‡é‡(g)": "é‡é‡",
                "æ‰‹æŒ": "åœ¨åº«",
                "åˆ©ç”¨å¯èƒ½": "åˆ©ç”¨å¯èƒ½",
                "æ³¨æ–‡æ¸ˆ": "ç™ºæ³¨æ¸ˆ",
                "åå‰": "å•†å“ã‚³ãƒ¼ãƒ‰",
                "å•†å“ãƒ©ãƒ³ã‚¯": "ãƒ©ãƒ³ã‚¯"
            }, inplace=True)

            df.drop(columns=["å†…éƒ¨ID"], inplace=True, errors="ignore")
            df["jan"] = df["jan"].apply(normalize_jan)

            for col in ["ã‚±ãƒ¼ã‚¹å…¥æ•°", "ç™ºæ³¨ãƒ­ãƒƒãƒˆ", "åœ¨åº«", "åˆ©ç”¨å¯èƒ½", "ç™ºæ³¨æ¸ˆ"]:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0).round().astype(int)

        return df

    def upload_file(file, table_name):
        if not file:
            return
        with st.spinner(f"ğŸ“¤ {file.name} ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­..."):
            temp_path = f"/tmp/{file.name}"
            with open(temp_path, "wb") as f:
                f.write(file.read())
            try:
                df = pd.read_csv(
                    temp_path,
                    sep=",",
                    engine="python",
                    on_bad_lines="skip",
                    encoding="utf-8-sig"
                )
                df = preprocess_csv(df, table_name)

                requests.delete(f"{SUPABASE_URL}/rest/v1/{table_name}?id=gt.0", headers=HEADERS)

                if table_name == "purchase_data":
                    df = df.drop_duplicates(subset=["jan", "supplier", "order_lot"], keep="last")
                elif table_name == "item_master":
                    df = df.drop_duplicates(subset=["å•†å“ã‚³ãƒ¼ãƒ‰"], keep="last")
                    if "id" not in df.columns:
                        df.insert(0, "id", range(1, len(df) + 1))
                else:
                    df = df.drop_duplicates(subset=["jan"], keep="last")

                df = df.replace({pd.NA: None, pd.NaT: None, float("nan"): None}).where(pd.notnull(df), None)

                for i in range(0, len(df), 500):
                    batch = df.iloc[i:i+500].to_dict(orient="records")
                    res = requests.post(
                        f"{SUPABASE_URL}/rest/v1/{table_name}",
                        headers={**HEADERS, "Prefer": "resolution=merge-duplicates"},
                        json=batch
                    )
                    if res.status_code not in [200, 201]:
                        st.error(f"âŒ {table_name} ãƒãƒƒãƒPOSTå¤±æ•—: {res.status_code} {res.text}")
                        return

                st.success(f"âœ… {table_name} ã« {len(df)} ä»¶ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")

            except Exception as e:
                st.error(f"âŒ {table_name} ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    sales_file = st.file_uploader("ğŸ“ sales.csv ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type="csv")
    if sales_file:
        upload_file(sales_file, "sales")

    purchase_file = st.file_uploader("ğŸ“¦ purchase_data.csv ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type="csv")
    if purchase_file:
        upload_file(purchase_file, "purchase_data")

    item_file = st.file_uploader("ğŸ“‹ item_master.csv ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type="csv")
    if item_file:
        upload_file(item_file, "item_master")

    # âœ… ã“ã‚Œã‚‚ãƒ¢ãƒ¼ãƒ‰å†…ã«å…¥ã‚Œã‚‹ï¼
    warehouse_file = st.file_uploader("ğŸ¢ å€‰åº«åœ¨åº«.xlsx ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["xlsx"])
    if warehouse_file:
        def preprocess_warehouse_stock(file):
            df = pd.read_excel(file, sheet_name="å€‰åº«åœ¨åº«")
            df_upload = df.iloc[:, [9, 13, 22]].copy()  # J, N, W
            df_upload.columns = ["product_code", "stock_available", "jan"]
            df_upload["product_code"] = df_upload["product_code"].astype(str).str.strip()
            df_upload["jan"] = df_upload["jan"].astype(str).str.strip()
            df_upload["stock_available"] = pd.to_numeric(df_upload["stock_available"], errors="coerce").fillna(0).round().astype(int)
            return df_upload

        def upload_warehouse_stock(df):
            try:
                requests.delete(f"{SUPABASE_URL}/rest/v1/warehouse_stock?product_code=neq.null", headers=HEADERS)
                df = df.drop_duplicates(subset=["product_code"], keep="last")
                df = df.replace({pd.NA: None, pd.NaT: None, float("nan"): None}).where(pd.notnull(df), None)

                for i in range(0, len(df), 500):
                    batch = df.iloc[i:i+500].to_dict(orient="records")
                    res = requests.post(
                        f"{SUPABASE_URL}/rest/v1/warehouse_stock",
                        headers={**HEADERS, "Prefer": "resolution=merge-duplicates"},
                        json=batch
                    )
                    if res.status_code not in [200, 201]:
                        st.error(f"âŒ warehouse_stock ãƒãƒƒãƒPOSTå¤±æ•—: {res.status_code} {res.text}")
                        return

                st.success(f"âœ… warehouse_stock ã« {len(df)} ä»¶ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")

            except Exception as e:
                st.error(f"âŒ warehouse_stock ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

        with st.spinner("ğŸ“¤ å€‰åº«åœ¨åº«.xlsx ã‚’å‡¦ç†ä¸­..."):
            df_warehouse = preprocess_warehouse_stock(warehouse_file)
            upload_warehouse_stock(df_warehouse)




# ğŸ†• è²©å£²å®Ÿç¸¾ï¼ˆç›´è¿‘1ãƒ¶æœˆï¼‰ãƒ¢ãƒ¼ãƒ‰ -----------------------------
elif mode == "monthly_sales":
    st.subheader("ğŸ“Š è²©å£²å®Ÿç¸¾ï¼ˆç›´è¿‘1ãƒ¶æœˆï¼‰")

    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    HEADERS = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json"
    }

    # âœ… ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•°
    def fetch_data(table_name):
        headers = {**HEADERS, "Prefer": "count=exact"}
        dfs = []
        offset, limit = 0, 1000
        while True:
            url = f"{SUPABASE_URL}/rest/v1/{table_name}?select=*&offset={offset}&limit={limit}"
            res = requests.get(url, headers=headers)
            if res.status_code == 416 or not res.json():
                break
            if res.status_code not in [200, 206]:
                st.error(f"{table_name} ã®å–å¾—ã«å¤±æ•—: {res.status_code} / {res.text}")
                return pd.DataFrame()
            dfs.append(pd.DataFrame(res.json()))
            offset += limit
        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    df_master = fetch_data("item_master")
    df_sales = fetch_data("sales")
    df_warehouse = fetch_data("warehouse_stock")

    if df_master.empty or df_sales.empty or df_warehouse.empty:
        st.warning("å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        st.stop()

    # item_master æ•´å½¢
    df_master["jan"] = df_master["jan"].astype(str)
    df_master["å•†å“ã‚³ãƒ¼ãƒ‰"] = df_master["å•†å“ã‚³ãƒ¼ãƒ‰"].astype(str)
    df_master = df_master.rename(columns={"jan": "JAN"})

    # sales æ•´å½¢
    df_sales["å•†å“ã‚³ãƒ¼ãƒ‰"] = df_sales["jan"].astype(str)
    df_sales.rename(columns={"quantity_sold": "è²©å£²æ•°"}, inplace=True)

    # warehouse_stock æ•´å½¢
    df_warehouse["product_code"] = df_warehouse["product_code"].astype(str)
    df_warehouse = df_warehouse.rename(columns={
        "product_code": "å•†å“ã‚³ãƒ¼ãƒ‰",
        "stock_available": "åˆ©ç”¨å¯èƒ½åœ¨åº«"
    })

    # --- ãƒãƒ¼ã‚¸ ---
    df_joined = pd.merge(df_sales, df_master, on="å•†å“ã‚³ãƒ¼ãƒ‰", how="left")
    df_joined = pd.merge(df_joined, df_warehouse[["å•†å“ã‚³ãƒ¼ãƒ‰", "åˆ©ç”¨å¯èƒ½åœ¨åº«"]], on="å•†å“ã‚³ãƒ¼ãƒ‰", how="left")

    # --- JAN ---
    if "JAN" in df_joined.columns:
        df_joined["jan"] = df_joined["JAN"]
    else:
        st.warning("âš ï¸ item_master å´ã‹ã‚‰JANãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    # --- æ•°å€¤åˆ— ---
    df_joined["è²©å£²æ•°"] = pd.to_numeric(df_joined["è²©å£²æ•°"], errors="coerce").fillna(0).astype(int)
    df_joined["ç™ºæ³¨æ¸ˆ"] = pd.to_numeric(df_joined.get("stock_ordered", 0), errors="coerce").fillna(0).astype(int)
    df_joined["åˆ©ç”¨å¯èƒ½"] = df_joined["åˆ©ç”¨å¯èƒ½åœ¨åº«"].fillna(0).astype(int)
    df_joined.drop(columns=["åˆ©ç”¨å¯èƒ½åœ¨åº«"], inplace=True)

    # è²©å£²æ•° > 0 ã®ã¿
    df_joined = df_joined[df_joined["è²©å£²æ•°"] > 0]

    # ---------- ğŸ” æ¤œç´¢ UI ----------
    col1, col2 = st.columns(2)

    with col1:
        keyword_name = st.text_input(TEXT[language]["search_keyword"], "")
        keyword_code = st.text_input(TEXT[language]["search_code"], "")

    with col2:
        jan_filter_multi = st.text_area(
            TEXT[language]["multi_jan"],
            placeholder="ä¾‹:\n4901234567890\n4987654321098",
            height=120,
        )

    maker_filter = st.selectbox(
        TEXT[language]["search_brand"],
        [TEXT[language]["all"]] + sorted(df_joined["ãƒ¡ãƒ¼ã‚«ãƒ¼å"].dropna().unique())
    )
    rank_filter = st.selectbox(
        TEXT[language]["search_rank"],
        [TEXT[language]["all"]] + sorted(df_joined["ãƒ©ãƒ³ã‚¯"].dropna().unique())
    )
    type_filter = st.selectbox(
        TEXT[language]["search_type"],
        [TEXT[language]["all"]] + sorted(df_joined["å–æ‰±åŒºåˆ†"].dropna().unique())
    )

    import re
    jan_list = [j.strip() for j in re.split(r"[,\n\r]+", jan_filter_multi) if j.strip()]

    df_view = df_joined.copy()

    if jan_list:
        df_view = df_view[df_view["jan"].isin(jan_list)]
    elif keyword_code:
        df_view = df_view[
            df_view["å•†å“ã‚³ãƒ¼ãƒ‰"].str.contains(keyword_code, case=False, na=False) |
            df_view["jan"].str.contains(keyword_code, case=False, na=False)
        ]

    if keyword_name:
        df_view = df_view[df_view["å•†å“å"].str.contains(keyword_name, case=False, na=False)]

    if maker_filter != TEXT[language]["all"]:
        df_view = df_view[df_view["ãƒ¡ãƒ¼ã‚«ãƒ¼å"] == maker_filter]

    if rank_filter != TEXT[language]["all"]:
        df_view = df_view[df_view["ãƒ©ãƒ³ã‚¯"] == rank_filter]

    if type_filter != TEXT[language]["all"]:
        df_view = df_view[df_view["å–æ‰±åŒºåˆ†"] == type_filter]

    # ---------- ğŸ“‹ è¡¨ç¤º ----------
    view_cols = [
        "å•†å“ã‚³ãƒ¼ãƒ‰", "jan", "ãƒ©ãƒ³ã‚¯", "ãƒ¡ãƒ¼ã‚«ãƒ¼å",
        "å•†å“å", "å–æ‰±åŒºåˆ†", "è²©å£²æ•°", "åˆ©ç”¨å¯èƒ½", "ç™ºæ³¨æ¸ˆ"
    ]
    available_cols = [c for c in view_cols if c in df_view.columns]

    display_df = (
        df_view[available_cols]
        .sort_values(by="å•†å“ã‚³ãƒ¼ãƒ‰")
        .rename(columns=COLUMN_NAMES[language])
    )

    row_count = len(display_df)
    h_left, h_right = st.columns([1, 0.15])
    h_left.subheader(TEXT[language]["product_list"])
    h_right.markdown(
        f"<h4 style='text-align:right; margin-top: 0.6em;'>{row_count:,}ä»¶</h4>",
        unsafe_allow_html=True
    )

    st.dataframe(display_df, use_container_width=True)

    csv = display_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        "ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv,
        file_name="monthly_sales_filtered.csv",
        mime="text/csv",
    )


elif mode == "rank_a_check":
    st.subheader("ğŸ…°ï¸ Aãƒ©ãƒ³ã‚¯å•†å“ç¢ºèªãƒ¢ãƒ¼ãƒ‰")

    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    df_item = fetch_table("item_master")
    df_sales = fetch_table("sales")
    df_stock = fetch_table("warehouse_stock")

    if df_item.empty or df_sales.empty or df_stock.empty:
        st.warning("å¿…è¦ãªãƒ†ãƒ¼ãƒ–ãƒ«ãŒç©ºã§ã™")
        st.stop()

    # 1ï¸âƒ£ Aãƒ©ãƒ³ã‚¯ã®ã¿ + JAN å¿…é ˆ
    df_a = df_item[(df_item["ãƒ©ãƒ³ã‚¯"] == "Aãƒ©ãƒ³ã‚¯") & (df_item["jan"].notnull())].copy()
    df_a["JAN"] = df_a["jan"].astype(str).str.strip()

    # 2ï¸âƒ£ sales â†’ JAN ã«çµ±ä¸€
    df_sales["JAN"] = df_sales["jan"].astype(str).str.strip()

    # 3ï¸âƒ£ åœ¨åº«ãƒ†ãƒ¼ãƒ–ãƒ« â†’ JAN ã«çµ±ä¸€
    df_stock["JAN"] = df_stock["jan"].astype(str).str.strip()
    df_stock = df_stock.rename(columns={"stock_available": "åœ¨åº«æ•°"})

    # 4ï¸âƒ£ å®Ÿç¸¾ï¼ˆ30æ—¥ï¼‰
    df_sales_30 = (
        df_sales.groupby("JAN", as_index=False)["quantity_sold"]
        .sum()
        .rename(columns={"quantity_sold": "å®Ÿç¸¾ï¼ˆ30æ—¥ï¼‰"})
    )

    # 5ï¸âƒ£ ç™ºæ³¨æ¸ˆã‚‚ item_master ã‹ã‚‰
    df_item_sub = df_item[["jan", "ç™ºæ³¨æ¸ˆ"]].copy()
    df_item_sub["JAN"] = df_item_sub["jan"].astype(str).str.strip()
    df_item_sub = df_item_sub[["JAN", "ç™ºæ³¨æ¸ˆ"]]

    # 6ï¸âƒ£ ãƒãƒ¼ã‚¸
    df_merged = (
        df_a[["JAN", "å•†å“å", "ãƒ©ãƒ³ã‚¯"]]
        .merge(df_sales_30, on="JAN", how="left")
        .merge(df_item_sub, on="JAN", how="left")
        .merge(df_stock[["JAN", "åœ¨åº«æ•°"]], on="JAN", how="left")
    )

    # 7ï¸âƒ£ æ¬ æè£œå®Œ
    df_merged["ç™ºæ³¨æ¸ˆ"] = df_merged["ç™ºæ³¨æ¸ˆ"].fillna(0).astype(int)
    df_merged["å®Ÿç¸¾ï¼ˆ30æ—¥ï¼‰"] = df_merged["å®Ÿç¸¾ï¼ˆ30æ—¥ï¼‰"].fillna(0)
    df_merged["åœ¨åº«æ•°"] = df_merged["åœ¨åº«æ•°"].fillna(0)
    df_merged["å®Ÿç¸¾ï¼ˆ7æ—¥ï¼‰"] = None

    # 8ï¸âƒ£ ã‚¢ãƒ©ãƒ¼ãƒˆ
    df_merged["ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.0"] = df_merged["å®Ÿç¸¾ï¼ˆ30æ—¥ï¼‰"] > (df_merged["åœ¨åº«æ•°"] + df_merged["ç™ºæ³¨æ¸ˆ"])
    df_merged["ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.2"] = (df_merged["å®Ÿç¸¾ï¼ˆ30æ—¥ï¼‰"] * 1.2) > (df_merged["åœ¨åº«æ•°"] + df_merged["ç™ºæ³¨æ¸ˆ"])

    # 9ï¸âƒ£ ãƒ•ã‚£ãƒ«ã‚¿
    check_1_0 = st.checkbox("âœ… ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.0ã®ã¿è¡¨ç¤º", value=False)
    check_1_2 = st.checkbox("âœ… ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.2ã®ã¿è¡¨ç¤º", value=False)

    df_result = df_merged.copy()
    if check_1_0:
        df_result = df_result[df_result["ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.0"]]
    if check_1_2:
        df_result = df_result[df_result["ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.2"]]

    # ğŸ”Ÿ å‡ºåŠ›
    st.dataframe(df_result[[
        "JAN",
        "å•†å“å",
        "ãƒ©ãƒ³ã‚¯",
        "å®Ÿç¸¾ï¼ˆ30æ—¥ï¼‰",
        "å®Ÿç¸¾ï¼ˆ7æ—¥ï¼‰",
        "åœ¨åº«æ•°",
        "ç™ºæ³¨æ¸ˆ",
        "ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.0",
        "ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.2"
    ]])


elif mode == "difficult_items":
    st.subheader("ğŸš« å…¥è·å›°é›£å•†å“ãƒ¢ãƒ¼ãƒ‰")

    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    HEADERS = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json"
    }

    df = fetch_table("difficult_items")
    if not df.empty:
        df["created_at"] = pd.to_datetime(df["created_at"]).dt.strftime("%Y-%m-%d %H:%M:%S")
        df["updated_at"] = pd.to_datetime(df["updated_at"]).dt.strftime("%Y-%m-%d %H:%M:%S")
        df["é¸æŠ"] = False

        cols = ["é¸æŠ", "item_key", "reason", "note", "created_at", "updated_at", "id"]
        df = df[cols]

        st.write("### ğŸ“‹ ç¾åœ¨ã®å…¥è·å›°é›£ãƒªã‚¹ãƒˆ")

        edited_df = st.data_editor(
            df,
            use_container_width=True,
            num_rows="dynamic",
            column_config={
                "é¸æŠ": st.column_config.CheckboxColumn("é¸æŠ")
            },
            disabled=[
                "item_key", "reason", "note", "created_at", "updated_at"
            ]
        )

        selected_df = edited_df[edited_df["é¸æŠ"]]
        selected_ids = selected_df["id"].tolist()

        # âœ… ãƒœã‚¿ãƒ³ç„¡åŠ¹åŒ–ç®¡ç†
        delete_btn_disabled = False

        if st.button("âœ… é¸æŠã—ãŸè¡Œã‚’å‰Šé™¤"):
            if selected_ids:
                for _id in selected_ids:
                    record = df[df["id"] == _id].copy().to_dict(orient="records")[0]
                    record.pop("é¸æŠ", None)
                    record.pop("created_at", None)
                    record.pop("updated_at", None)
                    record["item_id"] = record["id"]
                    record.pop("id")
                    record["action"] = "delete"
                    record["action_at"] = datetime.datetime.now(ZoneInfo("Asia/Tokyo")).isoformat()
        
                    res1 = requests.post(
                        f"{SUPABASE_URL}/rest/v1/difficult_items_history",
                        headers={**HEADERS, "Prefer": "return=representation"},
                        json=record
                    )
        
                    res2 = requests.delete(
                        f"{SUPABASE_URL}/rest/v1/difficult_items?id=eq.{_id}",
                        headers=HEADERS
                    )
        
                st.success("âœ… å‰Šé™¤å®Œäº†ï¼")
                st.rerun()
            else:
                st.warning("âš ï¸ è¡ŒãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“")

    with st.form("add_difficult_item"):
        item_key = st.text_input("ãƒ–ãƒ©ãƒ³ãƒ‰ / å•†å“å / JAN ãªã©")
        reason = st.text_input("å…¥è·å›°é›£ç†ç”±")
        note = st.text_area("å‚™è€ƒ")

        submitted = st.form_submit_button("ç™»éŒ²ã™ã‚‹")
        if submitted:
            payload = {
                "item_key": item_key,
                "reason": reason,
                "note": note
            }

            res = requests.post(
                f"{SUPABASE_URL}/rest/v1/difficult_items",
                headers={**HEADERS, "Prefer": "return=representation"},
                json=payload
            )
            st.write("ç™»éŒ²POST:", res.status_code, res.text)

            if res.status_code in [200, 201]:
                record = res.json()[0]
                record["item_id"] = record["id"]
                record.pop("id")
                record.pop("created_at", None)  # â† å¿˜ã‚Œãšè¿½åŠ ï¼
                record.pop("updated_at", None)  # â† å¿˜ã‚Œãšè¿½åŠ ï¼
                record["action"] = "insert"
                record["action_at"] = datetime.datetime.now(ZoneInfo("Asia/Tokyo")).isoformat()

                res2 = requests.post(
                    f"{SUPABASE_URL}/rest/v1/difficult_items_history",
                    headers={**HEADERS, "Prefer": "return=representation"},
                    json=record
                )

                st.success("âœ… ç™»éŒ²ã—ã¾ã—ãŸï¼")
                st.rerun()
            else:
                st.error(f"ç™»éŒ²å¤±æ•—: {res.text}")

    df_history = fetch_table("difficult_items_history")
    
    if not df_history.empty:
        one_week_ago = datetime.datetime.now(ZoneInfo("Asia/Tokyo")) - datetime.timedelta(days=7)
        df_history["action_at"] = pd.to_datetime(df_history["action_at"], utc=True)
        df_history = df_history[df_history["action_at"] >= one_week_ago]
    
        # ğŸ”¥ JSTã«å¤‰æ›
        df_history["action_at"] = df_history["action_at"].dt.tz_convert("Asia/Tokyo")
        df_history["action_at"] = df_history["action_at"].dt.strftime("%Y-%m-%d %H:%M:%S")
    
        st.write("ğŸ“œ **å±¥æ­´ï¼ˆç›´è¿‘7æ—¥åˆ†ï¼‰**")
        st.dataframe(df_history, use_container_width=True)
    else:
        st.write("ğŸ“œ **å±¥æ­´ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“**")
