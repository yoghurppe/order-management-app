import streamlit as st

st.markdown("""
<style>
/* ===== multiselect ã®é¸æŠã‚¿ã‚°ï¼ˆé»’æ–‡å­—ãƒ»ç›®ã«å„ªã—ã„ï¼‰ ===== */

/* ã‚¿ã‚°æœ¬ä½“ */
[data-baseweb="tag"] {
    background-color: #D1D5DB !important;   /* è–„ã„ã‚°ãƒ¬ãƒ¼ */
    color: #111827 !important;              /* é»’ï¼ˆå°‘ã—æŸ”ã‚‰ã‹ã‚ï¼‰ */
    border-radius: 6px !important;
    font-weight: 500 !important;
}

/* ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ï¼ˆå¿µã®ãŸã‚æ˜ç¤ºï¼‰ */
[data-baseweb="tag"] span {
    color: #111827 !important;
}

/* Ã— ãƒœã‚¿ãƒ³ */
[data-baseweb="tag"] svg {
    color: #374151 !important;              /* æ¿ƒã„ã‚°ãƒ¬ãƒ¼ */
}

/* hover */
[data-baseweb="tag"]:hover {
    background-color: #9CA3AF !important;   /* å°‘ã—æ¿ƒã„ã‚°ãƒ¬ãƒ¼ */
}
</style>
""", unsafe_allow_html=True)


# âœ… ãƒšãƒ¼ã‚¸è¨­å®šã‚’è¿½åŠ 
st.set_page_config(
    page_title="ã€ASEANã€‘ä¸€å…ƒç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ",
    layout="wide",                 # æ¨ªå¹…ã‚’æœ€å¤§åŒ–
    initial_sidebar_state="expanded"
)

import pandas as pd
import requests
import datetime
import os
import math
import re
import hashlib
import time
from zoneinfo import ZoneInfo
from streamlit_javascript import st_javascript

# ã“ã“ã« parse_items_fixed ã‚’è¿½åŠ 
def parse_items_fixed(text):
    items = []
    lines = text.strip().splitlines()
    item = {}

    def normalize_number(s):
        table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')
        return s.translate(table).strip()

    for line in lines:
        line = line.strip()

        if "å“ç•ª" in line:
            item = {'å“ç•ª': line.split("å“ç•ª")[-1].strip()}

        elif "JAN" in line:
            item['jan'] = line.split("JAN")[-1].strip()

        elif re.search(r'ï¼ˆ[\d,]+å†† Ã— \d+ç‚¹ï¼‰', line):
            m = re.search(r'ï¼ˆ([\d,]+)å†† Ã— (\d+)ç‚¹ï¼‰', line)
            item['å˜ä¾¡'] = int(m.group(1).replace(',', ''))
            item['ãƒ­ãƒƒãƒˆ'] = int(m.group(2))

        elif normalize_number(line).isdigit() and 'æ•°é‡' not in item:
            item['æ•°é‡'] = int(normalize_number(line))

            if all(k in item for k in ['å“ç•ª', 'jan', 'å˜ä¾¡', 'ãƒ­ãƒƒãƒˆ', 'æ•°é‡']):
                item['ãƒ­ãƒƒãƒˆÃ—æ•°é‡'] = item['ãƒ­ãƒƒãƒˆ'] * item['æ•°é‡']
                items.append(item)
                item = {}

    df = pd.DataFrame(items)

    if not df.empty:
        df['å°è¨ˆ'] = df['å˜ä¾¡'] * df['ãƒ­ãƒƒãƒˆ'] * df['æ•°é‡']
        subtotal = df['å°è¨ˆ'].sum()

        df.loc[len(df)] = {
            'å“ç•ª': 'åˆè¨ˆ',
            'jan': '',
            'å˜ä¾¡': '',
            'ãƒ­ãƒƒãƒˆ': '',
            'æ•°é‡': '',
            'ãƒ­ãƒƒãƒˆÃ—æ•°é‡': '',
            'å°è¨ˆ': subtotal
        }

    return df
    
# ğŸŸ¢ ã“ã“ã‹ã‚‰ã‚¢ãƒ—ãƒªã®ä¸­èº«ï¼ˆè¨€èªé¸æŠãªã©ï¼‰
language = st.sidebar.selectbox("è¨€èª / Language", ["æ—¥æœ¬èª", "ä¸­æ–‡"], key="language")

# ãƒ¦ãƒ¼ã‚¶ãƒ¼è¡¨ç¤ºç”¨ãƒ©ãƒ™ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
TEXT = {
    "æ—¥æœ¬èª": {
        "title_order_ai": "ã€ASEANã€‘ä¸€å…ƒç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ",
        "mode_select": "ãƒ¢ãƒ¼ãƒ‰ã‚’é¸ã‚“ã§ãã ã•ã„",
        "upload_csv": "CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        "order_ai": "ç™ºæ³¨AIåˆ¤å®š",
        "search_item": "å•†å“æƒ…å ±æ¤œç´¢",
        "upload_item": "å•†å“æƒ…å ±CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        "price_improve": "ä»•å…¥ä¾¡æ ¼æ”¹å–„ãƒªã‚¹ãƒˆ",
        "search_keyword": "å•†å“åãƒ»å•†å“ã‚³ãƒ¼ãƒ‰ã§æ¤œç´¢",
        "search_brand": "ãƒ¡ãƒ¼ã‚«ãƒ¼åã§çµã‚Šè¾¼ã¿",
        "search_type": "å–æ‰±åŒºåˆ†ã§çµã‚Šè¾¼ã¿",
        "product_list": "å•†å“ä¸€è¦§",
        "search_name": "å•†å“åæ¤œç´¢",
        "search_brand": "ãƒ¡ãƒ¼ã‚«ãƒ¼åã§çµã‚Šè¾¼ã¿",
        "search_type": "å–æ‰±åŒºåˆ†ã§çµã‚Šè¾¼ã¿",
        "search_rank": "ãƒ©ãƒ³ã‚¯ã§çµã‚Šè¾¼ã¿",
        "search_code": "å•†å“ã‚³ãƒ¼ãƒ‰ / JANæ¤œç´¢",
        "all": "ã™ã¹ã¦",
        "loading_data": "ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...",
        "multi_jan": "è¤‡æ•°JANå…¥åŠ›ï¼ˆæ”¹è¡Œã¾ãŸã¯ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰"
    },
    "ä¸­æ–‡": {
        "title_order_ai": "ç®¡ç†æ”¯æŒç³»ç»Ÿ",
        "mode_select": "è¯·é€‰æ‹©æ¨¡å¼",
        "upload_csv": "ä¸Šä¼ CSV",
        "order_ai": "è®¢è´§AIåˆ¤æ–­",
        "search_item": "å•†å“ä¿¡æ¯æŸ¥è¯¢",
        "upload_item": "ä¸Šä¼ å•†å“ä¿¡æ¯CSV",
        "price_improve": "è¿›è´§ä»·æ ¼ä¼˜åŒ–æ¸…å•",
        "search_keyword": "æŒ‰å•†å“åç§°æˆ–ç¼–å·æœç´¢",
        "search_brand": "æŒ‰å“ç‰Œç­›é€‰",
        "search_type": "æŒ‰åˆ†ç±»ç­›é€‰",
        "product_list": "å•†å“åˆ—è¡¨",
        "search_name": "æŒ‰å•†å“åç§°æœç´¢",
        "search_brand": "æŒ‰åˆ¶é€ å•†ç­›é€‰",
        "search_type": "æŒ‰åˆ†ç±»ç­›é€‰",
        "search_rank": "æŒ‰ç­‰çº§ç­›é€‰",
        "search_code": "æŒ‰å•†å“ç¼–å· / æ¡ç æœç´¢",
        "all": "å…¨éƒ¨",
        "loading_data": "ğŸ“Š æ­£åœ¨è¯»å–æ•°æ®...",
        "multi_jan": "æ‰¹é‡è¾“å…¥æ¡ç ï¼ˆæ¢è¡Œæˆ–é€—å·åˆ†éš”ï¼‰"
    }
}

# åˆ—åãƒãƒƒãƒ”ãƒ³ã‚°
COLUMN_NAMES = {
    "æ—¥æœ¬èª": {
        "å•†å“ã‚³ãƒ¼ãƒ‰": "å•†å“ã‚³ãƒ¼ãƒ‰",
        "jan": "JAN",
        "ãƒ©ãƒ³ã‚¯": "ãƒ©ãƒ³ã‚¯",
        "ãƒ¡ãƒ¼ã‚«ãƒ¼å": "ãƒ¡ãƒ¼ã‚«ãƒ¼å",
        "å•†å“å": "å•†å“å",
        "å–æ‰±åŒºåˆ†": "å–æ‰±åŒºåˆ†",
        "åœ¨åº«": "åœ¨åº«",
        "åˆ©ç”¨å¯èƒ½": "åˆ©ç”¨å¯èƒ½",
        "ç™ºæ³¨æ¸ˆ": "ç™ºæ³¨æ¸ˆ",
        "ä»•å…¥ä¾¡æ ¼": "ä»•å…¥ä¾¡æ ¼",
        "ã‚±ãƒ¼ã‚¹å…¥æ•°": "ã‚±ãƒ¼ã‚¹å…¥æ•°",
        "ç™ºæ³¨ãƒ­ãƒƒãƒˆ": "ç™ºæ³¨ãƒ­ãƒƒãƒˆ",
        "é‡é‡": "é‡é‡(g)",
        "å®Ÿç¸¾åŸä¾¡": "å®Ÿç¸¾åŸä¾¡",
        "æœ€å®‰åŸä¾¡": "æœ€å®‰åŸä¾¡"
    },
    "ä¸­æ–‡": {
        "å•†å“ã‚³ãƒ¼ãƒ‰": "å•†å“ç¼–å·",
        "jan": "æ¡ç ",
        "ãƒ©ãƒ³ã‚¯": "ç­‰çº§",
        "ãƒ¡ãƒ¼ã‚«ãƒ¼å": "åˆ¶é€ å•†åç§°",
        "å•†å“å": "å•†å“åç§°",
        "å–æ‰±åŒºåˆ†": "åˆ†ç±»",
        "åœ¨åº«": "åº“å­˜",
        "åˆ©ç”¨å¯èƒ½": "å¯ç”¨åº“å­˜",
        "ç™ºæ³¨æ¸ˆ": "å·²è®¢è´­",
        "ä»•å…¥ä¾¡æ ¼": "è¿›è´§ä»·",
        "ã‚±ãƒ¼ã‚¹å…¥æ•°": "ç®±å…¥æ•°",
        "ç™ºæ³¨ãƒ­ãƒƒãƒˆ": "è®¢è´­å•ä½",
        "é‡é‡": "é‡é‡(g)",
        "å®Ÿç¸¾åŸä¾¡": "å®é™…æˆæœ¬",
        "æœ€å®‰åŸä¾¡": "æœ€ä½æˆæœ¬"
    }
}

# ğŸ” Supabaseæ¥ç¶šè¨­å®š
SUPABASE_URL_PRE = st.secrets["SUPABASE_URL"]
SUPABASE_KEY_PRE = st.secrets["SUPABASE_KEY"]
HEADERS_PRE = {
    "apikey": SUPABASE_KEY_PRE,
    "Authorization": f"Bearer {SUPABASE_KEY_PRE}",
    "Content-Type": "application/json"
}

def apply_common_search_ui(df, language: str):
    """
    å•†å“æƒ…å ±æ¤œç´¢ / è²©å£²å®Ÿç¸¾ï¼ˆç›´è¿‘1ãƒ¶æœˆï¼‰ã§å…±é€šã§ä½¿ã†æ¤œç´¢UIï¼‹ãƒ•ã‚£ãƒ«ã‚¿ã€‚
    df ã‚’çµã‚Šè¾¼ã‚“ã çµæœã‚’è¿”ã™ã€‚
    """

    # ---------- ğŸ” æ¤œç´¢UI ----------
    col1, col2 = st.columns(2)

    with col1:
        keyword_code = st.text_input(
            TEXT[language]["search_code"],
            "",
            placeholder="ä¾‹: 4515061012818"
        )
    with col2:
        keyword_name = st.text_input(
            TEXT[language]["search_name"],
            "",
            placeholder="ä¾‹: ãƒ‘ãƒ¼ãƒ•ã‚§ã‚¯ãƒˆã‚¸ã‚§ãƒ«"
        )

    jan_filter_multi = st.text_area(
        TEXT[language]["multi_jan"],
        placeholder="ä¾‹:\n4901234567890\n4987654321098",
        height=120,
    )

    maker_filter = st.selectbox(
        TEXT[language]["search_brand"],
        [TEXT[language]["all"]] +
        sorted(df.get("ãƒ¡ãƒ¼ã‚«ãƒ¼å", pd.Series(dtype=str)).dropna().unique().tolist())
    )
    # ãƒ©ãƒ³ã‚¯å€™è£œï¼ˆç©ºã¯é™¤å¤–ï¼‰
    rank_options = (
        df.get("ãƒ©ãƒ³ã‚¯", pd.Series(dtype=str))
          .astype(str).str.strip()
          .replace(["", "nan", "None", "NULL"], pd.NA)
          .dropna()
          .unique()
          .tolist()
    )
    rank_options = sorted(rank_options)
    
    # è¤‡æ•°é¸æŠï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å…¨é¸æŠï¼çµã‚Šè¾¼ã¿ç„¡ã—ã¨åŒã˜ï¼‰
    rank_filter = st.multiselect(
        TEXT[language]["search_rank"],
        options=rank_options,
        default=rank_options
    )
    type_filter = st.selectbox(
        TEXT[language]["search_type"],
        [TEXT[language]["all"]] +
        sorted(df.get("å–æ‰±åŒºåˆ†", pd.Series(dtype=str)).dropna().unique().tolist())
    )

    # ---------- çµã‚Šè¾¼ã¿ ----------
    jan_list = [j.strip() for j in re.split(r"[,\n\r]+", jan_filter_multi) if j.strip()]
    df_view = df.copy()

    # å„ªå…ˆåº¦: è¤‡æ•°JAN > ã‚³ãƒ¼ãƒ‰/JANæ¬„ > å•†å“åæ¬„
    if jan_list:
        if "jan" in df_view.columns:
            df_view = df_view[df_view["jan"].isin(jan_list)]
    elif keyword_code:
        if "å•†å“ã‚³ãƒ¼ãƒ‰" in df_view.columns and "jan" in df_view.columns:
            df_view = df_view[
                df_view["å•†å“ã‚³ãƒ¼ãƒ‰"].str.contains(keyword_code, case=False, na=False) |
                df_view["jan"].str.contains(keyword_code, case=False, na=False)
            ]
        elif "jan" in df_view.columns:
            df_view = df_view[df_view["jan"].str.contains(keyword_code, case=False, na=False)]

    if keyword_name and "å•†å“å" in df_view.columns:
        df_view = df_view[df_view["å•†å“å"].str.contains(keyword_name, case=False, na=False)]

    if maker_filter != TEXT[language]["all"] and "ãƒ¡ãƒ¼ã‚«ãƒ¼å" in df_view.columns:
        df_view = df_view[df_view["ãƒ¡ãƒ¼ã‚«ãƒ¼å"] == maker_filter]

    if "ãƒ©ãƒ³ã‚¯" in df_view.columns and rank_filter:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãŒå…¨é¸æŠãªã®ã§ã€å…¨é¸æŠã®ã¾ã¾ãªã‚‰å®Ÿè³ªå¤‰åŒ–ãªã—
        df_view = df_view[df_view["ãƒ©ãƒ³ã‚¯"].isin(rank_filter)]

    if type_filter != TEXT[language]["all"] and "å–æ‰±åŒºåˆ†" in df_view.columns:
        df_view = df_view[df_view["å–æ‰±åŒºåˆ†"] == type_filter]

    return df_view


# ğŸ“… item_master ã®æœ€æ–°æ›´æ–°æ—¥æ™‚ã‚’ JST è¡¨ç¤ºã§å–å¾—
def fetch_latest_item_update():
    url = f"{SUPABASE_URL_PRE}/rest/v1/item_master?select=updated_at&order=updated_at.desc&limit=1"
    res = requests.get(url, headers=HEADERS_PRE)
    if res.status_code == 200 and res.json():
        dt = pd.to_datetime(res.json()[0]["updated_at"], errors="coerce", utc=True)
        if pd.notnull(dt):
            dt_jst = dt.tz_convert(ZoneInfo("Asia/Tokyo"))
            return f"ï¼ˆ{dt_jst.strftime('%-m.%d update')}ï¼‰"
    return ""

def fetch_table(table_name):
    headers = {**HEADERS_PRE, "Prefer": "count=exact"}
    dfs = []
    offset = 0
    limit = 1000
    while True:
        url = f"{SUPABASE_URL_PRE}/rest/v1/{table_name}?select=*&offset={offset}&limit={limit}"
        res = requests.get(url, headers=headers)
        if res.status_code == 416 or not res.json():
            break
        if res.status_code not in [200, 206]:
            st.error(f"{table_name} ã®å–å¾—ã«å¤±æ•—: {res.status_code} / {res.text}")
            return pd.DataFrame()
        dfs.append(pd.DataFrame(res.json()))
        offset += limit
    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

item_master_update_text = fetch_latest_item_update()

# ã‚¿ã‚¤ãƒˆãƒ«è¡¨ç¤º
st.title(TEXT[language]["title_order_ai"])

MODE_KEYS = {
    "home": {
        "æ—¥æœ¬èª": "ğŸ  ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸",
        "ä¸­æ–‡": "ğŸ  ä¸»é¡µ"
    },
    "store_profit": {
        "æ—¥æœ¬èª": "ğŸª åº—èˆ—åˆ¥ ç²—åˆ©ä¸€è¦§",
        "ä¸­æ–‡": "ğŸª å„åº—é“º æ¯›åˆ©ä¸€è§ˆ"
    },
    "daily_sales": {
        "æ—¥æœ¬èª": "ğŸ“† åº—èˆ—åˆ¥ å‰æ—¥å£²ä¸Š",
        "ä¸­æ–‡": "ğŸ“† å„åº—é“º æ˜¨æ—¥é”€å”®"
    },
    "search_item": {
        "æ—¥æœ¬èª": f"ğŸ” å•†å“æƒ…å ±æ¤œç´¢<br>{item_master_update_text}",
        "ä¸­æ–‡": f"ğŸ” å•†å“ä¿¡æ¯æŸ¥è¯¢<br>{item_master_update_text}"
    },
    "monthly_sales": {
        "æ—¥æœ¬èª": "è²©å£²å®Ÿç¸¾ï¼ˆç›´è¿‘1ãƒ¶æœˆï¼‰",
        "ä¸­æ–‡": "é”€å”®ä¸šç»©ï¼ˆæœ€è¿‘ä¸€ä¸ªæœˆï¼‰"
    },
    "expiry_manage": {
        "æ—¥æœ¬èª": "ğŸ§Š è³å‘³æœŸé™ç®¡ç†",
        "ä¸­æ–‡": "ğŸ§Š ä¿è´¨æœŸç®¡ç†"
    },
    "order_ai": {
        "æ—¥æœ¬èª": "ç™ºæ³¨AIåˆ¤å®š",
        "ä¸­æ–‡": "è®¢è´§AIåˆ¤æ–­"
    },
    "rank_check": {
        "æ—¥æœ¬èª": "ãƒ©ãƒ³ã‚¯åˆ¥ç™ºæ³¨ç¢ºèª",
        "ä¸­æ–‡": "æŒ‰ç­‰çº§ç¡®è®¤è®¢å•"
    },
    "purchase_history": {
        "æ—¥æœ¬èª": "ğŸ“œ ç™ºæ³¨å±¥æ­´",
        "ä¸­æ–‡": "ğŸ“œ è®¢è´§è®°å½•"
    },
    "order": {
        "æ—¥æœ¬èª": "ğŸ“¦ ç™ºæ³¨æ›¸ä½œæˆ",
        "ä¸­æ–‡": "ğŸ“¦ è®¢å•ä¹¦ç”Ÿæˆæ¨¡å¼"
    },
    "csv_upload": {
        "æ—¥æœ¬èª": "CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        "ä¸­æ–‡": "ä¸Šä¼ CSV"
    },
    # "price_improve": {
    #     "æ—¥æœ¬èª": "ä»•å…¥ä¾¡æ ¼æ”¹å–„ãƒªã‚¹ãƒˆ",
    #     "ä¸­æ–‡": "è¿›è´§ä»·æ ¼ä¼˜åŒ–æ¸…å•"
    # },
    # "difficult_items": {
    #     "æ—¥æœ¬èª": "å…¥è·å›°é›£å•†å“",
    #     "ä¸­æ–‡": "è¿›è´§å›°éš¾å•†å“"
    # },
}


# å˜ä¸€é¸æŠãƒ¢ãƒ¼ãƒ‰ã®åˆæœŸåŒ–
if "mode" not in st.session_state:
    st.session_state["mode"] = "home"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒˆãƒƒãƒ—

# å¤šè¨€èªãƒ©ãƒ™ãƒ«å–å¾—
def local_label(mode_key: str) -> str:
    d = MODE_KEYS.get(mode_key, {})
    return d.get(language) or d.get("æ—¥æœ¬èª") or mode_key

# è¡¨ç¤ºã‚°ãƒ«ãƒ¼ãƒ—å®šç¾©ï¼ˆé †ç•ªï¼è¡¨ç¤ºé †ï¼‰
GROUPS = [
    ("ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸",      ["home"]),
    ("ã€å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã€‘",    ["store_profit", "daily_sales"]),
    ("ã€å•†å“æƒ…å ±ã€‘",      ["search_item", "monthly_sales"]),
    ("ã€è³å‘³æœŸé™ã€‘", ["expiry_manage"]),
    ("ã€ç™ºæ³¨ã€‘",          ["order_ai", "rank_check", "purchase_history", "order"]),
    ("ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€‘",  ["csv_upload"]),
]

# === ã“ã“ã‹ã‚‰ç½®ãæ›ãˆ ===
from urllib.parse import urlencode

with st.sidebar:
    st.markdown(f"### {TEXT[language]['mode_select']}")

    # ã‚¹ã‚¿ã‚¤ãƒ«
    st.markdown("""
    <style>
      .nav-group { margin: .5rem 0 .25rem; font-weight: 700; }
      a.nav-btn {
        display: block;
        width: 100%;
        text-align: left;
        padding: .45rem .6rem;
        border: 1px solid rgba(49,51,63,.2);
        border-radius: .55rem;
        margin: .18rem 0;
        background: white;
        text-decoration: none;
        color: inherit;
        transition: all 0.1s ease;
      }
      a.nav-btn:hover { background: rgba(49,51,63,.06); }
      a.nav-btn.active {
        background: rgba(14,165,233,.12);
        border-color: rgba(14,165,233,.45);
        font-weight: 600;
      }
    </style>
    """, unsafe_allow_html=True)

    # ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã« <a href="?mode=..."> ã‚’ç”Ÿæˆï¼ˆå³ã‚¯ãƒªãƒƒã‚¯ã§æ–°è¦ã‚¿ãƒ–OKï¼‰
    for group_title, keys in GROUPS:
        st.markdown(f"<div class='nav-group'>{group_title}</div>", unsafe_allow_html=True)
        for k in keys:
            label = local_label(k)  # ãƒ©ãƒ™ãƒ«ã« <br> ã‚’å«ã‚“ã§ã‚‚OK
            active_class = "active" if st.session_state["mode"] == k else ""

            # æ—¢å­˜ã®ã‚¯ã‚¨ãƒªã‚’ç¶­æŒã—ã¤ã¤ mode ã ã‘ä¸Šæ›¸ã
            params = dict(st.query_params)
            params["mode"] = k
            href = "?" + urlencode(params, doseq=True)

            st.markdown(
                f"<a class='nav-btn {active_class}' href='{href}' target='_self'>{label}</a>",
                unsafe_allow_html=True
            )
            
# URL â†’ çŠ¶æ…‹ åæ˜ ï¼ˆæ–°APIï¼‰
mode_param = st.query_params.get("mode")
if mode_param in MODE_KEYS:
    st.session_state["mode"] = mode_param

# ç¾åœ¨ãƒ¢ãƒ¼ãƒ‰
mode = st.session_state["mode"]

# çŠ¶æ…‹ â†’ URL åŒæœŸï¼ˆç›´æ‰“ã¡ã‚„åˆæœŸè¡¨ç¤ºæ™‚ã®è£œå®Œï¼‰
if st.query_params.get("mode") != mode:
    st.query_params["mode"] = mode
# === ã“ã“ã¾ã§ç½®ãæ›ãˆ ===

# ãƒ©ãƒ³ã‚¯è¨­å®š
RANK_LABELS = ("Aãƒ©ãƒ³ã‚¯", "Bãƒ©ãƒ³ã‚¯", "Cãƒ©ãƒ³ã‚¯")

def normalize_rank_base(rank) -> str:
    """
    rank: ä¾‹ "Aãƒ©ãƒ³ã‚¯", "Aãƒ©ãƒ³ã‚¯â˜…", "Bãƒ©ãƒ³ã‚¯â˜…", "TEST", None
    return: "A" / "B" / "C" / ""ï¼ˆãã‚Œä»¥å¤–ï¼‰
    """
    if rank is None:
        return ""
    s = str(rank).strip()
    if s.startswith("Aãƒ©ãƒ³ã‚¯"):
        return "A"
    if s.startswith("Bãƒ©ãƒ³ã‚¯"):
        return "B"
    if s.startswith("Cãƒ©ãƒ³ã‚¯"):
        return "C"
    return ""

def add_base_rank_column(df: pd.DataFrame, col="rank") -> pd.DataFrame:
    """
    df[col] ã‹ã‚‰ base_rank ã‚’ä½œã£ã¦è¿”ã™ï¼ˆã‚³ãƒ”ãƒ¼ã—ã¦è¿”ã™ï¼‰
    """
    if df is None or df.empty or col not in df.columns:
        return df
    out = df.copy()
    # æ­£è¦è¡¨ç¾ã§ "A/B/Cãƒ©ãƒ³ã‚¯" ã® A/B/C ã ã‘æŠœãï¼ˆâ˜…ä»˜ãã‚‚OKï¼‰
    out["base_rank"] = (
        out[col]
        .astype(str)
        .str.strip()
        .str.extract(r"^(A|B|C)ãƒ©ãƒ³ã‚¯", expand=False)
        .fillna("")
    )
    return out



# å„ãƒ¢ãƒ¼ãƒ‰ã®å‡¦ç†åˆ†å²
if mode == "home":
    st.subheader("ğŸ  " + TEXT[language]["title_order_ai"])

    if language == "æ—¥æœ¬èª":
        st.markdown("""
        #### ã”åˆ©ç”¨ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚
        å·¦ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰æ“ä½œã‚’é¸ã‚“ã§ãã ã•ã„ã€‚
        - ğŸ“¦ ç™ºæ³¨AI
        - ğŸ“¤ CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        - ğŸ” å•†å“æƒ…å ±æ¤œç´¢
        - ğŸ’° ä»•å…¥ä¾¡æ ¼æ”¹å–„ãƒªã‚¹ãƒˆ
        """)
    else:
        st.markdown("""
        #### æ„Ÿè°¢æ‚¨çš„ä½¿ç”¨ã€‚
        è¯·ä»å·¦ä¾§èœå•ä¸­é€‰æ‹©æ“ä½œæ¨¡å¼ã€‚
        - ğŸ“¦ è®¢è´§AI
        - ğŸ“¤ ä¸Šä¼ CSV
        - ğŸ” å•†å“ä¿¡æ¯æŸ¥è¯¢
        - ğŸ’° è¿›è´§ä»·æ ¼ä¼˜åŒ–æ¸…å•
        """)

elif mode == "order_ai":
    st.subheader("ğŸ“¦ ç™ºæ³¨AIãƒ¢ãƒ¼ãƒ‰")

    ai_mode = "JDãƒ¢ãƒ¼ãƒ‰"
    st.caption("âœ… ç™ºæ³¨AIã¯ JDåœ¨åº«ãƒ™ãƒ¼ã‚¹ã§è¨ˆç®—ã—ã¾ã™")


    if st.button("ğŸ¤– è¨ˆç®—ã‚’é–‹å§‹ã™ã‚‹"):
        SUPABASE_URL = st.secrets["SUPABASE_URL"]
        SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
        HEADERS = {
            "apikey": SUPABASE_KEY,
            "Authorization": f"Bearer {SUPABASE_KEY}",
            "Content-Type": "application/json"
        }

        import math
        from datetime import date, timedelta

        def fetch_table(table_name):
            headers = {**HEADERS, "Prefer": "count=exact"}
            dfs = []
            offset = 0
            limit = 1000
            while True:
                url = f"{SUPABASE_URL}/rest/v1/{table_name}?select=*&offset={offset}&limit={limit}"
                res = requests.get(url, headers=headers)
                if res.status_code == 416 or not res.json():
                    break
                if res.status_code not in [200, 206]:
                    st.error(f"{table_name} ã®å–å¾—ã«å¤±æ•—: {res.status_code} / {res.text}")
                    return pd.DataFrame()
                dfs.append(pd.DataFrame(res.json()))
                offset += limit
            return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

        def normalize_jan(x):
            try:
                return str(x).strip()
            except:
                return ""

        with st.spinner("ğŸ“¦ ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
            df_sales = fetch_table("sales")
            df_purchase = fetch_table("purchase_data")
            df_master = fetch_table("item_master")
            df_warehouse = fetch_table("warehouse_stock")  # JDå›ºå®šãªã®ã§å¸¸ã«å–å¾—


        if df_sales.empty or df_purchase.empty or df_master.empty:
            st.warning("å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
            st.stop()
        if ai_mode == "JDãƒ¢ãƒ¼ãƒ‰" and df_warehouse.empty:
            st.warning("JDãƒ¢ãƒ¼ãƒ‰ç”¨ã® warehouse_stock ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
            st.stop()

        # æ­£è¦åŒ–ãƒ»å‹æƒãˆ
        df_sales["jan"] = df_sales["jan"].apply(normalize_jan)
        df_purchase["jan"] = df_purchase["jan"].apply(normalize_jan)
        df_master["jan"] = df_master["jan"].apply(normalize_jan)
        df_sales["quantity_sold"] = pd.to_numeric(df_sales["quantity_sold"], errors="coerce").fillna(0).astype(int)
        df_sales["stock_available"] = pd.to_numeric(df_sales["stock_available"], errors="coerce").fillna(0).astype(int)

        if ai_mode == "JDãƒ¢ãƒ¼ãƒ‰":
            df_warehouse["product_code"] = df_warehouse["product_code"].apply(normalize_jan)
            df_warehouse["stock_available"] = pd.to_numeric(df_warehouse["stock_available"], errors="coerce").fillna(0).astype(int)

        # ç™ºæ³¨å±¥æ­´ï¼ˆä¸Šæµ·é™¤å¤–/ç›´è¿‘åˆ¤å®šã«ä½¿ç”¨ï¼‰
        df_history = fetch_table("purchase_history")
        if df_history.empty:
            df_history = pd.DataFrame(columns=["jan", "quantity", "memo", "order_date"])
        df_history["quantity"] = pd.to_numeric(df_history["quantity"], errors="coerce").fillna(0).astype(int)
        df_history["memo"] = df_history["memo"].astype(str).fillna("")
        df_history["jan"] = df_history["jan"].apply(normalize_jan)

        # ã€Œä¸Šæµ·ã€åˆ†ã‚’ item_master ç™ºæ³¨æ¸ˆã‹ã‚‰æ§é™¤
        df_shanghai = df_history[df_history["memo"].str.contains("ä¸Šæµ·", na=False)]
        df_shanghai_grouped = df_shanghai.groupby("jan")["quantity"].sum().reset_index(name="shanghai_quantity")
        if "ç™ºæ³¨æ¸ˆ" not in df_master.columns:
            df_master["ç™ºæ³¨æ¸ˆ"] = 0
        df_master = df_master.merge(df_shanghai_grouped, on="jan", how="left")
        df_master["shanghai_quantity"] = df_master["shanghai_quantity"].fillna(0).astype(int)
        df_master["ç™ºæ³¨æ¸ˆ_ä¿®æ­£å¾Œ"] = (pd.to_numeric(df_master["ç™ºæ³¨æ¸ˆ"], errors="coerce").fillna(0) - df_master["shanghai_quantity"]).clip(lower=0)

        # sales ã«åæ˜ 
        df_sales.drop(columns=["ç™ºæ³¨æ¸ˆ"], errors="ignore", inplace=True)
        df_sales = df_sales.merge(df_master[["jan", "ç™ºæ³¨æ¸ˆ_ä¿®æ­£å¾Œ"]], on="jan", how="left")
        df_sales["ç™ºæ³¨æ¸ˆ"] = df_sales["ç™ºæ³¨æ¸ˆ_ä¿®æ­£å¾Œ"].fillna(0).astype(int)

        # purchase_data å‹æƒãˆ
        df_purchase["order_lot"] = pd.to_numeric(df_purchase["order_lot"], errors="coerce").fillna(0).astype(int)
        df_purchase["price"] = pd.to_numeric(df_purchase["price"], errors="coerce")

        # ãƒ©ãƒ³ã‚¯å€ç‡ï¼ˆC/TESTã§ä½¿ç”¨ã€‚A/Bã¯æ–°ä»•æ§˜ã«ã‚ˆã‚Šæœªä½¿ç”¨ï¼‰
        rank_multiplier = {
            "Aãƒ©ãƒ³ã‚¯": 1.0,  # æœªä½¿ç”¨
            "Bãƒ©ãƒ³ã‚¯": 1.2,  # æœªä½¿ç”¨ï¼ˆç™ºæ³¨æ•°ã¯1.7Sã€ç™ºæ³¨ç‚¹1.2Sã«å¤‰æ›´ï¼‰
            "Cãƒ©ãƒ³ã‚¯": 1.0,
            "TEST": 1.5,
            "NEW": 1.5
        }

        with st.spinner("ğŸ¤– ç™ºæ³¨AIãŒè¨ˆç®—ã‚’ã—ã¦ã„ã¾ã™..."):
            # ç›´è¿‘ï¼ˆæœ¬æ—¥/æ˜¨æ—¥ï¼‰ç™ºæ³¨ã®é™¤å¤–
            df_history_recent = df_history.copy()
            df_history_recent["order_date"] = pd.to_datetime(df_history_recent["order_date"], errors="coerce").dt.date
            today = date.today()
            yesterday = today - timedelta(days=1)
            recent_jans = df_history_recent[df_history_recent["order_date"].isin([today, yesterday])]["jan"].dropna().astype(str).apply(normalize_jan).unique().tolist()

            results = []

            for _, row in df_sales.iterrows():
                jan = row["jan"]
                sold = row["quantity_sold"]
                ordered = row["ç™ºæ³¨æ¸ˆ"]

                # åœ¨åº«å–å¾—
                if ai_mode == "JDãƒ¢ãƒ¼ãƒ‰":
                    stock_row = df_warehouse[df_warehouse["product_code"] == jan]
                    stock = stock_row["stock_available"].values[0] if not stock_row.empty else 0
                else:
                    stock = row.get("stock_available", 0)

                # ãƒ©ãƒ³ã‚¯å–å¾—
                rank_row = df_master[df_master["jan"] == jan]
                if not rank_row.empty and ("ãƒ©ãƒ³ã‚¯" in df_master.columns):
                    rank = str(rank_row.iloc[0]["ãƒ©ãƒ³ã‚¯"]) if pd.notna(rank_row.iloc[0]["ãƒ©ãƒ³ã‚¯"]) else ""
                else:
                    rank = ""

                # â˜…å¯¾å¿œï¼šåŸºåº•ãƒ©ãƒ³ã‚¯ï¼ˆA/B/C or ""ï¼‰
                base_rank = normalize_rank_base(rank)

                # ç›´è¿‘ç™ºæ³¨ã‚¹ã‚­ãƒƒãƒ—
                if jan in recent_jans:
                    continue

                current_total = stock + ordered

                # ===== ç™ºæ³¨ç‚¹åˆ¤å®š =====
                if base_rank in ["A", "B"]:
                    # æ–°ä»•æ§˜ï¼šåœ¨åº«+ç™ºæ³¨æ¸ˆ ãŒ ceil(å®Ÿç¸¾Ã—1.2) ã‚’ã€Œä¸‹å›ã£ãŸã‚‰ã€ç™ºæ³¨
                    reorder_point = max(math.ceil(sold * 1.2), 1)
                    if current_total >= reorder_point:
                        continue  # ä¸‹å›ã£ã¦ã„ãªã„ï¼ˆ=ç™ºæ³¨ã—ãªã„ï¼‰
                else:
                    # æ—¢å­˜ä»•æ§˜ï¼ˆC/TESTï¼‰
                    if rank == "Cãƒ©ãƒ³ã‚¯":
                        reorder_point = max(math.floor(sold * 0.7), 1)
                    else:  # TEST or ãã®ä»–
                        reorder_point = max(math.floor(sold * 0.7), 1)
                    if current_total > reorder_point:
                        continue

                # ===== ç™ºæ³¨æ•°ã®åŸºæº– =====
                if base_rank in ["A", "B"]:
                    # æ–°ä»•æ§˜ï¼šç™ºæ³¨æ•° = ceil(å®Ÿç¸¾Ã—1.7)
                    base_needed = max(math.ceil(sold * 1.7), 0)
                    # ã€Œæœ€ä½1å€‹ã€ç‰¹ä¾‹ã¯ A/B ã§ã‚‚æœ‰åŠ¹ã«ã—ã¦ãŠãï¼ˆåœ¨åº«ã»ã¼ã‚¼ãƒ­ã§å®Ÿç¸¾ã‚ã‚Šã®å®‰å…¨ç­–ï¼‰
                    if stock <= 1 and sold >= 1 and base_needed <= 0:
                        base_needed = 1
                else:
                    # C/TESTï¼šä¸è¶³åˆ† = ceil(å®Ÿç¸¾Ã—å€ç‡) - åœ¨åº« - ç™ºæ³¨æ¸ˆ
                    m = rank_multiplier.get(rank, 1.0)
                    need_raw = math.ceil(sold * m) - stock - ordered
                    base_needed = 1 if (stock <= 1 and sold >= 1 and need_raw <= 0) else max(need_raw, 0)
                    if base_needed <= 0:
                        continue  # ä¸è¶³ãªã—

                # ã“ã“ã§ base_needed > 0 ãªã‚‰ç™ºæ³¨å¿…è¦

                # ä»•å…¥å€™è£œæŠ½å‡º
                options_all = df_purchase[df_purchase["jan"] == jan].copy()
                valid_options = pd.DataFrame()
                if not options_all.empty:
                    options_all["order_lot"] = pd.to_numeric(options_all["order_lot"], errors="coerce").fillna(0).astype(int)
                    options_all["price"] = pd.to_numeric(options_all["price"], errors="coerce")
                    options_lotpos = options_all[options_all["order_lot"] > 0].copy()
                    valid_options = options_lotpos[options_lotpos["price"].notna() & (options_lotpos["price"] > 0)].copy()

                # ä¾¡æ ¼ãŒç„¡ã„/ãƒ­ãƒƒãƒˆç„¡åŠ¹ â†’ ç©ºæ¬„ã§å‡ºåŠ›
                if valid_options.empty:
                    results.append({
                        "jan": jan,
                        "è²©å£²å®Ÿç¸¾": sold,
                        "åœ¨åº«": stock,
                        "ç™ºæ³¨æ¸ˆ": ordered,
                        "ç†è«–å¿…è¦æ•°": base_needed,
                        "ç™ºæ³¨æ•°": "",     # ç©ºæ¬„
                        "ãƒ­ãƒƒãƒˆ": "",     # ç©ºæ¬„
                        "æ•°é‡": "",       # ç©ºæ¬„
                        "å˜ä¾¡": "",       # ç©ºæ¬„
                        "ç·é¡": "",       # ç©ºæ¬„
                        "ä»•å…¥å…ˆ": "",     # ç©ºæ¬„
                        "ãƒ©ãƒ³ã‚¯": rank
                    })
                    continue

                # ä¾¡æ ¼ã‚ã‚Šï¼šãƒ­ãƒƒãƒˆæœ€é©åŒ–
                options = valid_options.copy()

                # A/B ã¯ base_needed=ceil(1.7S) ã‚’ãƒ­ãƒƒãƒˆã§åˆ‡ã‚Šä¸Šã’
                # C/TEST ã¯ã€Œä¸è¶³åˆ†ã€= base_needed ã‚’ãƒ­ãƒƒãƒˆã§åˆ‡ã‚Šä¸Šã’
                need_for_lot = base_needed

                if base_rank in ["A", "B"]:
                    bigger_lots = options[options["order_lot"] >= need_for_lot]
                    if not bigger_lots.empty:
                        best_option = bigger_lots.sort_values("order_lot").iloc[0]
                    else:
                        best_option = options.sort_values("order_lot", ascending=False).iloc[0]
                else:
                    # å¾“æ¥ãƒ­ã‚¸ãƒƒã‚¯
                    options["diff"] = (options["order_lot"] - need_for_lot).abs()
                    smaller_lots = options[options["order_lot"] <= need_for_lot]
                    if not smaller_lots.empty:
                        best_option = smaller_lots.loc[smaller_lots["diff"].idxmin()]
                    else:
                        near_lots = options[(options["order_lot"] > need_for_lot) & (options["order_lot"] <= need_for_lot * 1.5) & (options["order_lot"] != 1)]
                        if not near_lots.empty:
                            best_option = near_lots.loc[near_lots["diff"].idxmin()]
                        else:
                            one_lot = options[options["order_lot"] == 1]
                            if not one_lot.empty:
                                best_option = one_lot.iloc[0]
                            else:
                                best_option = options.sort_values("order_lot").iloc[0]


                lot = int(best_option["order_lot"])
                sets = math.ceil(need_for_lot / lot)
                qty = sets * lot
                total_cost = qty * float(best_option["price"])
                
                results.append({
                    "jan": jan,
                    "è²©å£²å®Ÿç¸¾": sold,
                    "åœ¨åº«": stock,
                    "ç™ºæ³¨æ¸ˆ": ordered,
                    "ç†è«–å¿…è¦æ•°": base_needed,
                    "ç™ºæ³¨æ•°": int(qty),                   # â† æ•´æ•°
                    "ãƒ­ãƒƒãƒˆ": lot,                        # â† æ•´æ•°
                    "æ•°é‡": int(sets),                    # â† æ•´æ•°
                    "å˜ä¾¡": int(best_option["price"]),    # â† æ•´æ•°
                    "ç·é¡": int(total_cost),              # â† æ•´æ•°
                    "ä»•å…¥å…ˆ": best_option.get("supplier", "ä¸æ˜"),
                    "ãƒ©ãƒ³ã‚¯": rank
                })

            # === å‡ºåŠ›æ•´å½¢ ===
            if results:
                result_df = pd.DataFrame(results)

                # å•†å“åãƒ»å–æ‰±åŒºåˆ†ã‚’çµåˆ
                if "å•†å“ã‚³ãƒ¼ãƒ‰" in df_master.columns:
                    df_master["å•†å“ã‚³ãƒ¼ãƒ‰"] = df_master["å•†å“ã‚³ãƒ¼ãƒ‰"].astype(str).str.strip()
                    result_df["jan"] = result_df["jan"].astype(str).str.strip()
                    df_temp = df_master[["å•†å“ã‚³ãƒ¼ãƒ‰", "å•†å“å", "å–æ‰±åŒºåˆ†"]].copy()
                    df_temp.rename(columns={"å•†å“ã‚³ãƒ¼ãƒ‰": "jan"}, inplace=True)
                    result_df = pd.merge(result_df, df_temp, on="jan", how="left")

                # å¼å¤©åœ¨åº«ï¼ˆè¡¨ç¤ºã®ã¿ï¼‰
                df_benten = fetch_table("benten_stock")
                if not df_benten.empty:
                    df_benten["jan"] = df_benten["jan"].astype(str).str.strip()
                    df_benten = df_benten[["jan", "stock"]].rename(columns={"stock": "å¼å¤©åœ¨åº«"})
                    result_df = pd.merge(result_df, df_benten, on="jan", how="left")
                    result_df["å¼å¤©åœ¨åº«"] = result_df["å¼å¤©åœ¨åº«"].fillna(0).astype(int)

                # åˆ—åçµ±ä¸€
                result_df.rename(columns={"åœ¨åº«": "JDåœ¨åº«"}, inplace=True)

                # è¡¨ç¤ºãƒ•ã‚£ãƒ«ã‚¿
                if "å•†å“å" in result_df.columns:
                    result_df = result_df[result_df["å•†å“å"].notna()]
                if "å–æ‰±åŒºåˆ†" in result_df.columns:
                    result_df = result_df[result_df["å–æ‰±åŒºåˆ†"] != "å–æ‰±ä¸­æ­¢"]
                else:
                    st.warning("âš ï¸ã€å–æ‰±åŒºåˆ†ã€åˆ—ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")

                # è¡¨ç¤ºé †
                column_order = ["jan", "å•†å“å", "ãƒ©ãƒ³ã‚¯", "è²©å£²å®Ÿç¸¾", "JDåœ¨åº«", "å¼å¤©åœ¨åº«", "ç™ºæ³¨æ¸ˆ",
                                "ç†è«–å¿…è¦æ•°", "ç™ºæ³¨æ•°", "ãƒ­ãƒƒãƒˆ", "æ•°é‡", "å˜ä¾¡", "ç·é¡", "ä»•å…¥å…ˆ"]
                result_df = result_df[[c for c in column_order if c in result_df.columns]]

                st.success(f"âœ… ç™ºæ³¨å¯¾è±¡: {len(result_df)} ä»¶")
                st.dataframe(result_df, use_container_width=True)

                # ä¸€æ‹¬CSV
                csv = result_df.to_csv(index=False).encode("utf-8-sig")
                st.download_button("ğŸ“¥ ç™ºæ³¨CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv, file_name="orders_available_based.csv", mime="text/csv")

                # ä»•å…¥å…ˆåˆ¥CSVï¼ˆä»•å…¥å…ˆãŒç©ºç™½ã®è¡Œã¯é™¤å¤–ï¼‰
                st.markdown("---")
                st.subheader("ğŸ“¦ ä»•å…¥å…ˆåˆ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
                if "ä»•å…¥å…ˆ" in result_df.columns:
                    for supplier, group in result_df[result_df["ä»•å…¥å…ˆ"].notna() & (result_df["ä»•å…¥å…ˆ"] != "")].groupby("ä»•å…¥å…ˆ"):
                        supplier_csv = group.to_csv(index=False).encode("utf-8-sig")
                        st.download_button(
                            label=f"ğŸ“¥ {supplier} ç”¨ ç™ºæ³¨CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=supplier_csv,
                            file_name=f"orders_{supplier}.csv",
                            mime="text/csv"
                        )
                else:
                    st.info("ä»•å…¥å…ˆåˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            else:
                st.info("ç¾åœ¨ã€ç™ºæ³¨ãŒå¿…è¦ãªå•†å“ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")



# ğŸ” å•†å“æƒ…å ±æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ -----------------------------
elif mode == "search_item":
    st.subheader("ğŸ” å•†å“æƒ…å ±æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰")

    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    HEADERS = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json"
    }

    # ---------- ãƒ‡ãƒ¼ã‚¿å–å¾— ----------
    def fetch_item_master():
        headers = {**HEADERS, "Prefer": "count=exact"}
        dfs = []
        offset, limit = 0, 1000
        while True:
            url = f"{SUPABASE_URL}/rest/v1/item_master?select=*&offset={offset}&limit={limit}"
            res = requests.get(url, headers=headers)
            if res.status_code == 416 or not res.json():
                break
            if res.status_code not in [200, 206]:
                st.error(f"item_master ã®å–å¾—ã«å¤±æ•—: {res.status_code} / {res.text}")
                return pd.DataFrame()
            dfs.append(pd.DataFrame(res.json()))
            offset += limit
        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

    def fetch_warehouse_stock():
        headers = {**HEADERS, "Prefer": "count=exact"}
        dfs = []
        offset, limit = 0, 1000
        while True:
            url = f"{SUPABASE_URL}/rest/v1/warehouse_stock?select=*&offset={offset}&limit={limit}"
            res = requests.get(url, headers=headers)
            if res.status_code == 416 or not res.json():
                break
            if res.status_code not in [200, 206]:
                st.error(f"warehouse_stock ã®å–å¾—ã«å¤±æ•—: {res.status_code} / {res.text}")
                return pd.DataFrame()
            dfs.append(pd.DataFrame(res.json()))
            offset += limit
        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

    df_master = fetch_item_master()
    df_warehouse = fetch_warehouse_stock()

    if df_master.empty:
        st.warning("å•†å“æƒ…å ±ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        st.stop()

    # å‹æ•´å½¢
    df_master["jan"] = df_master["jan"].astype(str)
    if "å•†å“ã‚³ãƒ¼ãƒ‰" in df_master.columns:
        df_master["å•†å“ã‚³ãƒ¼ãƒ‰"] = df_master["å•†å“ã‚³ãƒ¼ãƒ‰"].astype(str)
    if "å•†å“å" in df_master.columns:
        df_master["å•†å“å"] = df_master["å•†å“å"].astype(str)

    if not df_warehouse.empty:
        df_warehouse["product_code"] = df_warehouse["product_code"].astype(str)
        df_warehouse["stock_available"] = pd.to_numeric(df_warehouse["stock_available"], errors="coerce").fillna(0).astype(int)
        df_warehouse["stock_total"] = df_warehouse["stock_available"]

        # JDåœ¨åº«ï¼ˆwarehouse_stockï¼‰ã‚’çµåˆ
        df_master = df_master.merge(
            df_warehouse[["product_code", "stock_total", "stock_available"]],
            left_on="jan", right_on="product_code",
            how="left"
        )
        df_master["åœ¨åº«"] = df_master["stock_total"].fillna(0).astype(int)
        df_master["åˆ©ç”¨å¯èƒ½"] = df_master["stock_available"].fillna(0).astype(int)
    else:
        df_master["åœ¨åº«"] = 0
        df_master["åˆ©ç”¨å¯èƒ½"] = 0

    # ä¾¡æ ¼åˆ—ï¼ˆå­˜åœ¨ã—ãªã‘ã‚Œã°0ã§åŸ‹ã‚ã‚‹ï¼‰
    df_master["å®Ÿç¸¾åŸä¾¡"] = pd.to_numeric(df_master.get("average_cost", 0), errors="coerce").fillna(0).astype(int)
    df_master["æœ€å®‰åŸä¾¡"] = pd.to_numeric(df_master.get("purchase_cost", 0), errors="coerce").fillna(0).astype(int)

    # ---------- æ¤œç´¢UI ï¼‹ çµã‚Šè¾¼ã¿ï¼ˆå…±é€šé–¢æ•°ï¼‰ ----------
    df_view = apply_common_search_ui(df_master, language)

    # ---------- è¡¨ç¤º ----------
    view_cols = [
        "å•†å“ã‚³ãƒ¼ãƒ‰", "jan", "ãƒ©ãƒ³ã‚¯", "ãƒ¡ãƒ¼ã‚«ãƒ¼å", "å•†å“å", "å–æ‰±åŒºåˆ†",
        "åœ¨åº«", "ç™ºæ³¨æ¸ˆ", "å®Ÿç¸¾åŸä¾¡", "æœ€å®‰åŸä¾¡", "ã‚±ãƒ¼ã‚¹å…¥æ•°", "ç™ºæ³¨ãƒ­ãƒƒãƒˆ", "é‡é‡"
    ]
    available_cols = [c for c in view_cols if c in df_view.columns]

    display_df = (
        df_view[available_cols]
        .sort_values(by=[c for c in ["å•†å“ã‚³ãƒ¼ãƒ‰", "jan"] if c in available_cols])
    )

    row_count = len(display_df)
    h_left, h_right = st.columns([1, 0.15])
    h_left.subheader("å•†å“ä¸€è¦§")
    h_right.markdown(
        f"<h4 style='text-align:right; margin-top: 0.6em;'>{row_count:,}ä»¶</h4>",
        unsafe_allow_html=True
    )

    st.dataframe(display_df, use_container_width=True)

    # ---------- CSV DL ----------
    csv = display_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        "ğŸ“… CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv,
        file_name="item_master_filtered.csv",
        mime="text/csv",
    )


elif mode == "purchase_history":
    st.subheader("ğŸ“œ ç™ºæ³¨å±¥æ­´")

    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    HEADERS = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json"
    }

    # ---------- ğŸ” æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ  ----------
    col1, col2 = st.columns(2)

    with col1:
        # å¾“æ¥ã®å˜ä¸€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰
        jan_filter_single = st.text_input("ğŸ” JANã§æ¤œç´¢ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", "")
        order_id_filter   = st.text_input("ğŸ” Orderâ€¯IDã§æ¤œç´¢ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰", "")

    with col2:
        jan_filter_multi = st.text_area(
            TEXT[language]["multi_jan"],                # â†å‹•çš„ãƒ©ãƒ™ãƒ«
            placeholder="ä¾‹:\n4901234567890\n4987654321098",
            height=120,
        )

    @st.cache_data(ttl=60)
    def fetch_purchase_history():
        url = f"{SUPABASE_URL}/rest/v1/purchase_history?select=*"
        res = requests.get(url, headers=HEADERS)
        if res.status_code == 200:
            return pd.DataFrame(res.json())
        st.error("âŒ ç™ºæ³¨å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return pd.DataFrame()

    df = fetch_purchase_history()

    if df.empty:
        st.info("ç™ºæ³¨å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        st.stop()

    # ------------- ğŸ§¹ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° -------------
    import re

    df["jan"]        = df["jan"].astype(str)
    df["order_date"] = pd.to_datetime(df["order_date"], errors="coerce").dt.date

    # â‘  è¤‡æ•°â€¯JAN ãƒªã‚¹ãƒˆã‚’æ•´å½¢
    jan_list = [j.strip() for j in re.split(r"[,\n\r]+", jan_filter_multi) if j.strip()]

    if jan_list:  # æœ€å„ªå…ˆ
        df = df[df["jan"].isin(jan_list)]
    elif jan_filter_single:
        df = df[df["jan"].str.contains(jan_filter_single, na=False)]

    if order_id_filter:
        df = df[df["order_id"].astype(str).str.contains(order_id_filter, na=False)]

    # ------------- ğŸ“‹ è¡¨ç¤º -------------
    df_show = df[["jan", "quantity", "order_date", "order_id"]].sort_values("jan")

    st.success(f"âœ… ç™ºæ³¨å±¥æ­´ ä»¶æ•°: {len(df_show)} ä»¶")
    st.dataframe(df_show, use_container_width=True)




elif mode == "price_improve":
    st.subheader("ğŸ’° " + TEXT[language]["price_improve"])

    # èªè¨¼ç”¨ãƒ˜ãƒƒãƒ€ãƒ¼å®šç¾©
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    HEADERS = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json"
    }

    def fetch_table(table_name):
        headers = {**HEADERS, "Prefer": "count=exact"}
        dfs = []
        offset = 0
        limit = 1000
        while True:
            url = f"{SUPABASE_URL}/rest/v1/{table_name}?select=*&offset={offset}&limit={limit}"
            res = requests.get(url, headers=headers)
            if res.status_code == 416 or not res.json():
                break
            if res.status_code not in [200, 206]:
                st.error(f"{table_name} ã®å–å¾—ã«å¤±æ•—: {res.status_code} / {res.text}")
                return pd.DataFrame()
            dfs.append(pd.DataFrame(res.json()))
            offset += limit
        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

    with st.spinner("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        df_sales = fetch_table("sales")
        df_purchase = fetch_table("purchase_data")
        df_item = fetch_table("item_master")

    def normalize_jan(x):
        try:
            if re.fullmatch(r"\d+(\.0+)?", str(x)):
                return str(int(float(x)))
            else:
                return str(x).strip()
        except:
            return ""

    # æ•´å½¢
    df_sales["jan"] = df_sales["jan"].apply(normalize_jan)
    df_purchase["jan"] = df_purchase["jan"].apply(normalize_jan)
    df_item["jan"] = df_item["jan"].apply(normalize_jan)
    df_purchase["price"] = pd.to_numeric(df_purchase["price"], errors="coerce").fillna(0)

    # ç¾åœ¨ä¾¡æ ¼åˆ¤å®š
    current_prices = {}
    for _, row in df_sales.iterrows():
        jan = row["jan"]
        sold = row["quantity_sold"]
        stock = row.get("stock_available", 0)
        ordered = row.get("stock_ordered", 0)
        options = df_purchase[df_purchase["jan"] == jan].copy()
        if options.empty:
            continue

        if stock >= sold:
            need_qty = 0
        else:
            need_qty = sold - stock + math.ceil(sold * 0.5) - ordered
            need_qty = max(need_qty, 0)

        if need_qty <= 0:
            continue

        options = options[options["order_lot"] > 0]
        options["diff"] = (options["order_lot"] - need_qty).abs()

        smaller_lots = options[options["order_lot"] <= need_qty]
        if not smaller_lots.empty:
            best_option = smaller_lots.loc[smaller_lots["diff"].idxmin()]
        else:
            near_lots = options[(options["order_lot"] > need_qty) & (options["order_lot"] <= need_qty * 1.5) & (options["order_lot"] != 1)]
            if not near_lots.empty:
                best_option = near_lots.loc[near_lots["diff"].idxmin()]
            else:
                one_lot = options[options["order_lot"] == 1]
                if not one_lot.empty:
                    best_option = one_lot.iloc[0]
                else:
                    best_option = options.sort_values("order_lot").iloc[0]

        current_prices[jan] = best_option["price"]

    # æœ€å®‰å€¤å–å¾—
    min_prices = df_purchase.groupby("jan")["price"].min().to_dict()

    rows = []
    for jan, current_price in current_prices.items():
        if jan in min_prices and min_prices[jan] < current_price:
            item = df_item[df_item["jan"] == jan].head(1)
            if not item.empty:
                row = {
                    "å•†å“ã‚³ãƒ¼ãƒ‰": item.iloc[0].get("item_code", ""),
                    "JAN": jan,
                    "ãƒ¡ãƒ¼ã‚«ãƒ¼å": item.iloc[0].get("brand", ""),
                    "ç¾åœ¨ã®ä»•å…¥ä¾¡æ ¼": current_price,
                    "æœ€å®‰å€¤ã®ä»•å…¥ä¾¡æ ¼": min_prices[jan],
                    "å·®åˆ†": round(min_prices[jan] - current_price, 2)
                }
                rows.append(row)

    if rows:
        df_result = pd.DataFrame(rows)

        # âœ… å¤šè¨€èªã‚«ãƒ©ãƒ åã«å¤‰æ›
        column_translation = {
            "æ—¥æœ¬èª": {
                "å•†å“ã‚³ãƒ¼ãƒ‰": "å•†å“ã‚³ãƒ¼ãƒ‰",
                "JAN": "JAN",
                "ãƒ¡ãƒ¼ã‚«ãƒ¼å": "ãƒ¡ãƒ¼ã‚«ãƒ¼å",
                "ç¾åœ¨ã®ä»•å…¥ä¾¡æ ¼": "ç¾åœ¨ã®ä»•å…¥ä¾¡æ ¼",
                "æœ€å®‰å€¤ã®ä»•å…¥ä¾¡æ ¼": "æœ€å®‰å€¤ã®ä»•å…¥ä¾¡æ ¼",
                "å·®åˆ†": "å·®åˆ†"
            },
            "ä¸­æ–‡": {
                "å•†å“ã‚³ãƒ¼ãƒ‰": "å•†å“ç¼–å·",
                "JAN": "æ¡ç ",
                "ãƒ¡ãƒ¼ã‚«ãƒ¼å": "åˆ¶é€ å•†åç§°",
                "ç¾åœ¨ã®ä»•å…¥ä¾¡æ ¼": "å½“å‰è¿›è´§ä»·",
                "æœ€å®‰å€¤ã®ä»•å…¥ä¾¡æ ¼": "æœ€ä½è¿›è´§ä»·",
                "å·®åˆ†": "å·®é¢"
            }
        }

        df_result = df_result.rename(columns=column_translation[language])

        st.success(f"âœ… æ”¹å–„å¯¾è±¡å•†å“æ•°: {len(df_result)} ä»¶")
        st.dataframe(df_result)

        csv = df_result.to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            "ğŸ“¥ æ”¹å–„ãƒªã‚¹ãƒˆCSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv,
            file_name="price_improvement_list.csv",
            mime="text/csv",
            key="price_improve_download"  # ğŸ”‘ è¤‡æ•°å‘¼ã³å‡ºã—é˜²æ­¢
        )
    else:
        st.info("æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚‹å•†å“ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")


if mode == "csv_upload":
    st.subheader("ğŸ“„ CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰")

    def normalize_jan(x):
        try:
            return str(x).strip()
        except:
            return ""

    input_password = st.text_input("ğŸ”‘ ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", type="password")
    correct_password = st.secrets.get("UPLOAD_PASSWORD", "pass1234")

    if input_password != correct_password:
        st.warning("æ­£ã—ã„ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    HEADERS = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json"
    }

    def preprocess_csv(df, table):
        df.columns = df.columns.str.replace("ã€€", "").str.replace("\ufeff", "").str.strip()

        if table == "sales":
            st.write("ğŸ“ sales åˆ—å:", df.columns.tolist())
            item_col = None
            for col in df.columns:
                if "ã‚¢ã‚¤ãƒ†ãƒ " in col:
                    item_col = col
                    break
            if item_col:
                df.rename(columns={item_col: "jan"}, inplace=True)
            else:
                raise ValueError(f"âŒ 'ã‚¢ã‚¤ãƒ†ãƒ ' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼åˆ—å: {df.columns.tolist()}")

            df.rename(columns={
                "å–æ‰±åŒºåˆ†": "handling_type",
                "è²©å£²æ•°é‡": "quantity_sold",
                "ç¾åœ¨ã®æ‰‹æŒæ•°é‡": "stock_total",
                "ç¾åœ¨ã®åˆ©ç”¨å¯èƒ½æ•°é‡": "stock_available",
                "ç¾åœ¨ã®æ³¨æ–‡æ¸ˆæ•°é‡": "stock_ordered"
            }, inplace=True)

            for col in ["quantity_sold", "stock_total", "stock_available", "stock_ordered"]:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0).astype(int)

            df["jan"] = df["jan"].apply(normalize_jan)

        elif table == "purchase_data":
            for col in ["order_lot", "price"]:
                if col in df.columns:
                    df[col] = df[col].astype(str).str.replace(",", "")
                    df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0)
                    if col == "order_lot":
                        df[col] = df[col].round().astype(int)
            df["jan"] = df["jan"].apply(normalize_jan)

        elif table == "item_master":
            st.write("ğŸ“ item_master åˆ—å:", df.columns.tolist())
            upc_col = None
            for col in df.columns:
                if "UPC" in col:
                    upc_col = col
                    break

            if upc_col:
                df.rename(columns={upc_col: "jan"}, inplace=True)
            else:
                raise ValueError(f"âŒ 'UPCã‚³ãƒ¼ãƒ‰' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼åˆ—å: {df.columns.tolist()}")

            df.rename(columns={
                "è¡¨ç¤ºå": "å•†å“å",
                "ãƒ¡ãƒ¼ã‚«ãƒ¼å": "ãƒ¡ãƒ¼ã‚«ãƒ¼å",
                "ã‚¢ã‚¤ãƒ†ãƒ å®šç¾©åŸä¾¡": "ä»•å…¥ä¾¡æ ¼",
                "ã‚«ãƒ¼ãƒˆãƒ³å…¥æ•°": "ã‚±ãƒ¼ã‚¹å…¥æ•°",
                "ç™ºæ³¨ãƒ­ãƒƒãƒˆ": "ç™ºæ³¨ãƒ­ãƒƒãƒˆ",
                "ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸é‡é‡(g)": "é‡é‡",
                "æ‰‹æŒ": "åœ¨åº«",
                "åˆ©ç”¨å¯èƒ½": "åˆ©ç”¨å¯èƒ½",
                "æ³¨æ–‡æ¸ˆ": "ç™ºæ³¨æ¸ˆ",
                "åå‰": "å•†å“ã‚³ãƒ¼ãƒ‰",
                "å•†å“ãƒ©ãƒ³ã‚¯": "ãƒ©ãƒ³ã‚¯"
            }, inplace=True)

            df.drop(columns=["å†…éƒ¨ID"], inplace=True, errors="ignore")
            df["jan"] = df["jan"].apply(normalize_jan)

            for col in ["ã‚±ãƒ¼ã‚¹å…¥æ•°", "ç™ºæ³¨ãƒ­ãƒƒãƒˆ", "åœ¨åº«", "åˆ©ç”¨å¯èƒ½", "ç™ºæ³¨æ¸ˆ"]:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0).round().astype(int)

        return df

    def upload_file(file, table_name):
        if not file:
            return
        with st.spinner(f"ğŸ“¤ {file.name} ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­..."):
            temp_path = f"/tmp/{file.name}"
            with open(temp_path, "wb") as f:
                f.write(file.read())
            try:
                df = pd.read_csv(
                    temp_path,
                    sep=",",
                    engine="python",
                    on_bad_lines="skip",
                    encoding="utf-8-sig"
                )
                df = preprocess_csv(df, table_name)

                requests.delete(f"{SUPABASE_URL}/rest/v1/{table_name}?id=gt.0", headers=HEADERS)

                if table_name == "purchase_data":
                    df = df.drop_duplicates(subset=["jan", "supplier", "order_lot"], keep="last")
                elif table_name == "item_master":
                    df = df.drop_duplicates(subset=["å•†å“ã‚³ãƒ¼ãƒ‰"], keep="last")
                    if "id" not in df.columns:
                        df.insert(0, "id", range(1, len(df) + 1))
                else:
                    df = df.drop_duplicates(subset=["jan"], keep="last")

                df = df.replace({pd.NA: None, pd.NaT: None, float("nan"): None}).where(pd.notnull(df), None)

                for i in range(0, len(df), 500):
                    batch = df.iloc[i:i+500].to_dict(orient="records")
                    res = requests.post(
                        f"{SUPABASE_URL}/rest/v1/{table_name}",
                        headers={**HEADERS, "Prefer": "resolution=merge-duplicates"},
                        json=batch
                    )
                    if res.status_code not in [200, 201]:
                        st.error(f"âŒ {table_name} ãƒãƒƒãƒPOSTå¤±æ•—: {res.status_code} {res.text}")
                        return

                st.success(f"âœ… {table_name} ã« {len(df)} ä»¶ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")

            except Exception as e:
                st.error(f"âŒ {table_name} ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    sales_file = st.file_uploader("ğŸ“ sales.csv ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type="csv")
    if sales_file:
        upload_file(sales_file, "sales")

    purchase_file = st.file_uploader("ğŸ“¦ purchase_data.csv ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type="csv")
    if purchase_file:
        upload_file(purchase_file, "purchase_data")

    item_file = st.file_uploader("ğŸ“‹ item_master.csv ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type="csv")
    if item_file:
        upload_file(item_file, "item_master")

    # âœ… ã“ã‚Œã‚‚ãƒ¢ãƒ¼ãƒ‰å†…ã«å…¥ã‚Œã‚‹ï¼
    warehouse_file = st.file_uploader("ğŸ¢ å€‰åº«åœ¨åº«.xlsx ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["xlsx"])
    if warehouse_file:
        def preprocess_warehouse_stock(file):
            df = pd.read_excel(file, sheet_name="å€‰åº«åœ¨åº«")
            df_upload = df.iloc[:, [9, 13, 22]].copy()  # J, N, W
            df_upload.columns = ["product_code", "stock_available", "jan"]
            df_upload["product_code"] = df_upload["product_code"].astype(str).str.strip()
            df_upload["jan"] = df_upload["jan"].astype(str).str.strip()
            df_upload["stock_available"] = pd.to_numeric(df_upload["stock_available"], errors="coerce").fillna(0).round().astype(int)
            return df_upload

        def upload_warehouse_stock(df):
            try:
                requests.delete(f"{SUPABASE_URL}/rest/v1/warehouse_stock?product_code=neq.null", headers=HEADERS)
                df = df.drop_duplicates(subset=["product_code"], keep="last")
                df = df.replace({pd.NA: None, pd.NaT: None, float("nan"): None}).where(pd.notnull(df), None)

                for i in range(0, len(df), 500):
                    batch = df.iloc[i:i+500].to_dict(orient="records")
                    res = requests.post(
                        f"{SUPABASE_URL}/rest/v1/warehouse_stock",
                        headers={**HEADERS, "Prefer": "resolution=merge-duplicates"},
                        json=batch
                    )
                    if res.status_code not in [200, 201]:
                        st.error(f"âŒ warehouse_stock ãƒãƒƒãƒPOSTå¤±æ•—: {res.status_code} {res.text}")
                        return

                st.success(f"âœ… warehouse_stock ã« {len(df)} ä»¶ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")

            except Exception as e:
                st.error(f"âŒ warehouse_stock ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

        with st.spinner("ğŸ“¤ å€‰åº«åœ¨åº«.xlsx ã‚’å‡¦ç†ä¸­..."):
            df_warehouse = preprocess_warehouse_stock(warehouse_file)
            upload_warehouse_stock(df_warehouse)

    benten_file = st.file_uploader("ğŸ­ BENTENå€‰åº«åœ¨åº«ï¼ˆCSVï¼‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["csv"])
    if benten_file:
        def preprocess_benten_stock(file):
            df = pd.read_csv(file)
            df.columns = df.columns.str.replace("ã€€", "").str.replace("\ufeff", "").str.strip()

            upc_col = None
            stock_col = None
            for col in df.columns:
                if "UPC" in col:
                    upc_col = col
                if "åˆ©ç”¨å¯èƒ½" in col:
                    stock_col = col

            if not upc_col or not stock_col:
                raise ValueError(f"âŒ 'UPCã‚³ãƒ¼ãƒ‰' ã¾ãŸã¯ 'åˆ©ç”¨å¯èƒ½' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼åˆ—å: {df.columns.tolist()}")

            df = df[[upc_col, stock_col]].copy()
            df.rename(columns={upc_col: "jan", stock_col: "stock"}, inplace=True)
            df["jan"] = df["jan"].astype(str).str.strip()
            df["stock"] = pd.to_numeric(df["stock"], errors="coerce").fillna(0).round().astype(int)
            df["updated_at"] = pd.Timestamp.now().isoformat()
            return df

        def upload_benten_stock(df):
            try:
                requests.delete(f"{SUPABASE_URL}/rest/v1/benten_stock?jan=neq.null", headers=HEADERS)
                df = df.drop_duplicates(subset=["jan"], keep="last")
                df = df.replace({pd.NA: None, pd.NaT: None, float("nan"): None}).where(pd.notnull(df), None)

                for i in range(0, len(df), 500):
                    batch = df.iloc[i:i+500].to_dict(orient="records")
                    res = requests.post(
                        f"{SUPABASE_URL}/rest/v1/benten_stock",
                        headers={**HEADERS, "Prefer": "resolution=merge-duplicates"},
                        json=batch
                    )
                    if res.status_code not in [200, 201]:
                        st.error(f"âŒ benten_stock ãƒãƒƒãƒPOSTå¤±æ•—: {res.status_code} {res.text}")
                        return

                st.success(f"âœ… benten_stock ã« {len(df)} ä»¶ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")

            except Exception as e:
                st.error(f"âŒ benten_stock ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

        with st.spinner("ğŸ“¤ BENTENå€‰åº«CSV ã‚’å‡¦ç†ä¸­..."):
            df_benten = preprocess_benten_stock(benten_file)
            upload_benten_stock(df_benten)



# ğŸ†• è²©å£²å®Ÿç¸¾ï¼ˆç›´è¿‘1ãƒ¶æœˆï¼‰ãƒ¢ãƒ¼ãƒ‰ -----------------------------
elif mode == "monthly_sales":
    st.subheader("ğŸ“Š è²©å£²å®Ÿç¸¾ï¼ˆç›´è¿‘1ãƒ¶æœˆï¼‰")

    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    HEADERS = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json"
    }

    # âœ… ãƒ‡ãƒ¼ã‚¿å–å¾—é–¢æ•°
    def fetch_data(table_name):
        headers = {**HEADERS, "Prefer": "count=exact"}
        dfs = []
        offset, limit = 0, 1000
        while True:
            url = f"{SUPABASE_URL}/rest/v1/{table_name}?select=*&offset={offset}&limit={limit}"
            res = requests.get(url, headers=headers)
            if res.status_code == 416 or not res.json():
                break
            if res.status_code not in [200, 206]:
                st.error(f"{table_name} ã®å–å¾—ã«å¤±æ•—: {res.status_code} / {res.text}")
                return pd.DataFrame()
            dfs.append(pd.DataFrame(res.json()))
            offset += limit
        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()

    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    df_master = fetch_data("item_master")
    df_sales = fetch_data("sales")
    df_warehouse = fetch_data("warehouse_stock")

    if df_master.empty or df_sales.empty or df_warehouse.empty:
        st.warning("å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        st.stop()

    # item_master æ•´å½¢
    df_master["jan"] = df_master["jan"].astype(str)
    df_master["å•†å“ã‚³ãƒ¼ãƒ‰"] = df_master["å•†å“ã‚³ãƒ¼ãƒ‰"].astype(str)
    df_master = df_master.rename(columns={"jan": "JAN"})

    # sales æ•´å½¢
    df_sales["å•†å“ã‚³ãƒ¼ãƒ‰"] = df_sales["jan"].astype(str)
    df_sales.rename(columns={"quantity_sold": "è²©å£²æ•°"}, inplace=True)

    # warehouse_stock æ•´å½¢
    df_warehouse["product_code"] = df_warehouse["product_code"].astype(str)
    df_warehouse = df_warehouse.rename(columns={
        "product_code": "å•†å“ã‚³ãƒ¼ãƒ‰",
        "stock_available": "åˆ©ç”¨å¯èƒ½åœ¨åº«"
    })

    # --- ãƒãƒ¼ã‚¸ ---
    df_joined = pd.merge(df_sales, df_master, on="å•†å“ã‚³ãƒ¼ãƒ‰", how="left")
    df_joined = pd.merge(df_joined, df_warehouse[["å•†å“ã‚³ãƒ¼ãƒ‰", "åˆ©ç”¨å¯èƒ½åœ¨åº«"]], on="å•†å“ã‚³ãƒ¼ãƒ‰", how="left")

    # --- JAN ---
    if "JAN" in df_joined.columns:
        df_joined["jan"] = df_joined["JAN"]
    else:
        st.warning("âš ï¸ item_master å´ã‹ã‚‰JANãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    # --- æ•°å€¤åˆ— ---
    df_joined["è²©å£²æ•°"] = pd.to_numeric(df_joined["è²©å£²æ•°"], errors="coerce").fillna(0).astype(int)
    df_joined["ç™ºæ³¨æ¸ˆ"] = pd.to_numeric(df_joined.get("stock_ordered", 0), errors="coerce").fillna(0).astype(int)
    df_joined["åˆ©ç”¨å¯èƒ½"] = df_joined["åˆ©ç”¨å¯èƒ½åœ¨åº«"].fillna(0).astype(int)
    df_joined.drop(columns=["åˆ©ç”¨å¯èƒ½åœ¨åº«"], inplace=True)

    # è²©å£²æ•° > 0 ã®ã¿
    df_joined = df_joined[df_joined["è²©å£²æ•°"] > 0]

    # ---------- ğŸ” æ¤œç´¢UI ï¼‹ çµã‚Šè¾¼ã¿ï¼ˆå•†å“æƒ…å ±æ¤œç´¢ã¨å…±é€šï¼‰ ----------
    df_view = apply_common_search_ui(df_joined, language)

    # ---------- ğŸ“‹ è¡¨ç¤º ----------
    view_cols = [
        "å•†å“ã‚³ãƒ¼ãƒ‰", "jan", "ãƒ©ãƒ³ã‚¯", "ãƒ¡ãƒ¼ã‚«ãƒ¼å",
        "å•†å“å", "å–æ‰±åŒºåˆ†", "è²©å£²æ•°", "åˆ©ç”¨å¯èƒ½", "ç™ºæ³¨æ¸ˆ"
    ]
    available_cols = [c for c in view_cols if c in df_view.columns]

    display_df = (
        df_view[available_cols]
        .sort_values(by="å•†å“ã‚³ãƒ¼ãƒ‰")
        .rename(columns=COLUMN_NAMES[language])
    )

    row_count = len(display_df)
    h_left, h_right = st.columns([1, 0.15])
    h_left.subheader(TEXT[language]["product_list"])
    h_right.markdown(
        f"<h4 style='text-align:right; margin-top: 0.6em;'>{row_count:,}ä»¶</h4>",
        unsafe_allow_html=True
    )

    st.dataframe(display_df, use_container_width=True)

    csv = display_df.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        "ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv,
        file_name="monthly_sales_filtered.csv",
        mime="text/csv",
    )



elif mode == "rank_check":
    st.subheader("ğŸ“Œ ãƒ©ãƒ³ã‚¯å•†å“ç¢ºèªãƒ¢ãƒ¼ãƒ‰")

    # =========================
    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    # =========================
    df_item = fetch_table("item_master")
    df_sales = fetch_table("sales")
    df_stock = fetch_table("warehouse_stock")
    df_benten = fetch_table("benten_stock")
    df_history = fetch_table("purchase_history")

    if df_item.empty or df_sales.empty or df_stock.empty or df_benten.empty or df_history.empty:
        st.warning("å¿…è¦ãªãƒ†ãƒ¼ãƒ–ãƒ«ãŒç©ºã§ã™")
        st.stop()

    # =========================
    # ãƒ‡ãƒ¼ã‚¿æ•´å½¢ï¼ˆitem_masterï¼‰
    # =========================
    df_item["jan"] = df_item["jan"].astype(str).str.strip()
    df_item.loc[df_item["jan"].isin(["", "nan", "None", "NULL"]), "jan"] = None
    df_item["ãƒ©ãƒ³ã‚¯"] = df_item["ãƒ©ãƒ³ã‚¯"].astype(str).str.strip()
    df_item["å•†å“å"] = df_item["å•†å“å"].astype(str)
    df_item["ãƒ¡ãƒ¼ã‚«ãƒ¼å"] = df_item["ãƒ¡ãƒ¼ã‚«ãƒ¼å"].astype(str)

    # =========================
    # ç™ºæ³¨æ¸ˆï¼ˆä¸Šæµ·é™¤å¤–ï¼‰
    # =========================
    df_history["quantity"] = pd.to_numeric(df_history["quantity"], errors="coerce").fillna(0).astype(int)
    df_history["memo"] = df_history["memo"].astype(str).fillna("")
    df_history["jan"] = df_history["jan"].astype(str).str.strip()

    df_shanghai = df_history[df_history["memo"].str.contains("ä¸Šæµ·", na=False)]
    df_shanghai_grouped = (
        df_shanghai.groupby("jan")["quantity"]
        .sum()
        .reset_index(name="shanghai_quantity")
    )

    df_item["ç™ºæ³¨æ¸ˆ"] = pd.to_numeric(df_item["ç™ºæ³¨æ¸ˆ"], errors="coerce").fillna(0).astype(int)
    df_item = df_item.merge(df_shanghai_grouped, on="jan", how="left")
    df_item["shanghai_quantity"] = df_item["shanghai_quantity"].fillna(0).astype(int)
    df_item["ç™ºæ³¨æ¸ˆ"] = (df_item["ç™ºæ³¨æ¸ˆ"] - df_item["shanghai_quantity"]).clip(lower=0)

    # =========================
    # å¯¾è±¡å•†å“ï¼ˆJANã‚ã‚Šã¯å¿…é ˆã€‚ãƒ©ãƒ³ã‚¯ã¯å…¨éƒ¨OKï¼šTESTå«ã‚€ï¼‰
    # =========================
    df_ab = df_item[df_item["jan"].notnull()].copy()

    df_ab["JAN"] = df_ab["jan"].astype(str).str.strip()
    df_ab = df_ab.drop_duplicates(subset=["JAN"])

    # =========================
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼UIï¼ˆãƒ©ãƒ³ã‚¯å€™è£œã‚’è‡ªå‹•ç”Ÿæˆï¼‰
    # =========================
    col1, col2 = st.columns(2)

    with col1:
        name_filter = st.text_input("ğŸ” å•†å“åã§çµã‚Šè¾¼ã¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰")

    with col2:
        maker_list = sorted(df_ab["ãƒ¡ãƒ¼ã‚«ãƒ¼å"].dropna().unique().tolist())
        selected_maker = st.selectbox(
            "ğŸ­ ãƒ¡ãƒ¼ã‚«ãƒ¼åã§çµã‚Šè¾¼ã¿",
            ["ã™ã¹ã¦"] + maker_list
        )

    # ãƒ©ãƒ³ã‚¯å€™è£œï¼ˆè‡ªå‹•ç”Ÿæˆï¼šç©ºã‚„nanã¯é™¤å¤–ï¼‰
    rank_options = (
        df_ab["ãƒ©ãƒ³ã‚¯"].astype(str).str.strip()
        .replace(["", "nan", "None", "NULL"], pd.NA)
        .dropna()
        .unique()
        .tolist()
    )
    rank_options = sorted(rank_options)

    if rank_options:
        selected_ranks = st.multiselect(
            "ğŸ“Œ è¡¨ç¤ºã™ã‚‹ãƒ©ãƒ³ã‚¯ï¼ˆè‡ªå‹•ï¼‰",
            rank_options,
            default=rank_options
        )
    else:
        st.warning("âš ï¸ ãƒ©ãƒ³ã‚¯ãŒç™»éŒ²ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒ©ãƒ³ã‚¯ãƒ•ã‚£ãƒ«ã‚¿ã‚’è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")
        selected_ranks = []


    # =========================
    # sales â†’ JANï¼ˆå®Ÿç¸¾30æ—¥ï¼‰
    # =========================
    df_sales["JAN"] = df_sales["jan"].astype(str).str.strip()

    df_sales_30 = (
        df_sales.groupby("JAN", as_index=False)["quantity_sold"]
        .sum()
        .rename(columns={"quantity_sold": "å®Ÿç¸¾ï¼ˆ30æ—¥ï¼‰"})
    )

    # =========================
    # åœ¨åº«
    # =========================
    df_stock["JAN"] = df_stock["jan"].astype(str).str.strip()
    df_stock = df_stock.rename(columns={"stock_available": "JDåœ¨åº«"})

    df_benten["JAN"] = df_benten["jan"].astype(str).str.strip()
    df_benten = df_benten.rename(columns={"stock": "å¼å¤©åœ¨åº«"})

    # =========================
    # ç™ºæ³¨æ¸ˆ
    # =========================
    df_item_sub = df_item[["jan", "ç™ºæ³¨æ¸ˆ"]].copy()
    df_item_sub["JAN"] = df_item_sub["jan"].astype(str).str.strip()
    df_item_sub = df_item_sub[["JAN", "ç™ºæ³¨æ¸ˆ"]]

    # =========================
    # ãƒãƒ¼ã‚¸
    # =========================
    base_cols = [
        "JAN",
        "å•†å“å",
        "ãƒ¡ãƒ¼ã‚«ãƒ¼å",
        "ãƒ©ãƒ³ã‚¯",
        "ã‚±ãƒ¼ã‚¹å…¥æ•°",
        "ç™ºæ³¨ãƒ­ãƒƒãƒˆ"
    ]

    if "purchase_cost" in df_ab.columns:
        base_cols.append("purchase_cost")

    df_merged = (
        df_ab[base_cols]
        .rename(columns={"purchase_cost": "æœ€å®‰åŸä¾¡"})
        .merge(df_sales_30, on="JAN", how="left")
        .merge(df_item_sub, on="JAN", how="left")
        .merge(df_stock[["JAN", "JDåœ¨åº«"]], on="JAN", how="left")
        .merge(df_benten[["JAN", "å¼å¤©åœ¨åº«"]], on="JAN", how="left")
    )

    # =========================
    # æ¬ æè£œå®Œ
    # =========================
    df_merged["å®Ÿç¸¾ï¼ˆ30æ—¥ï¼‰"] = df_merged["å®Ÿç¸¾ï¼ˆ30æ—¥ï¼‰"].fillna(0)
    df_merged["ç™ºæ³¨æ¸ˆ"] = df_merged["ç™ºæ³¨æ¸ˆ"].fillna(0).astype(int)
    df_merged["JDåœ¨åº«"] = df_merged["JDåœ¨åº«"].fillna(0)
    df_merged["å¼å¤©åœ¨åº«"] = df_merged["å¼å¤©åœ¨åº«"].fillna(0)
    df_merged["æœ€å®‰åŸä¾¡"] = pd.to_numeric(df_merged.get("æœ€å®‰åŸä¾¡"), errors="coerce")

    # =========================
    # ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ
    # =========================
    df_merged["ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.0"] = df_merged["å®Ÿç¸¾ï¼ˆ30æ—¥ï¼‰"] > (
        df_merged["JDåœ¨åº«"] + df_merged["å¼å¤©åœ¨åº«"] + df_merged["ç™ºæ³¨æ¸ˆ"]
    )

    df_merged["ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.2"] = (df_merged["å®Ÿç¸¾ï¼ˆ30æ—¥ï¼‰"] * 1.2) > (
        df_merged["JDåœ¨åº«"] + df_merged["å¼å¤©åœ¨åº«"] + df_merged["ç™ºæ³¨æ¸ˆ"]
    )

    # =========================
    # æ¡ä»¶ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    # =========================
    check_1_0 = st.checkbox("âœ… ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.0ã®ã¿è¡¨ç¤º", value=False)
    check_1_2 = st.checkbox("âœ… ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.2ã®ã¿è¡¨ç¤º", value=False)

    df_result = df_merged[df_merged["ãƒ©ãƒ³ã‚¯"].isin(selected_ranks)].copy()

    if name_filter:
        df_result = df_result[df_result["å•†å“å"].str.contains(name_filter, case=False, na=False)]

    if selected_maker != "ã™ã¹ã¦":
        df_result = df_result[df_result["ãƒ¡ãƒ¼ã‚«ãƒ¼å"] == selected_maker]

    if check_1_0:
        df_result = df_result[df_result["ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.0"]]

    if check_1_2:
        df_result = df_result[df_result["ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.2"]]

    # =========================
    # å‡ºåŠ›
    # =========================
    st.dataframe(df_result[[
        "JAN",
        "å•†å“å",
        "ãƒ¡ãƒ¼ã‚«ãƒ¼å",
        "ãƒ©ãƒ³ã‚¯",
        "ã‚±ãƒ¼ã‚¹å…¥æ•°",
        "ç™ºæ³¨ãƒ­ãƒƒãƒˆ",
        "å®Ÿç¸¾ï¼ˆ30æ—¥ï¼‰",
        "JDåœ¨åº«",
        "å¼å¤©åœ¨åº«",
        "ç™ºæ³¨æ¸ˆ",
        "æœ€å®‰åŸä¾¡",
        "ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.0",
        "ç™ºæ³¨ã‚¢ãƒ©ãƒ¼ãƒˆ1.2"
    ]])




elif mode == "difficult_items":
    st.subheader("ğŸš« å…¥è·å›°é›£å•†å“ãƒ¢ãƒ¼ãƒ‰")

    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    HEADERS = {
        "apikey": SUPABASE_KEY,
        "Authorization": f"Bearer {SUPABASE_KEY}",
        "Content-Type": "application/json"
    }

    df = fetch_table("difficult_items")
    if not df.empty:
        df["created_at"] = pd.to_datetime(df["created_at"]).dt.strftime("%Y-%m-%d %H:%M:%S")
        df["updated_at"] = pd.to_datetime(df["updated_at"]).dt.strftime("%Y-%m-%d %H:%M:%S")
        df["é¸æŠ"] = False

        cols = ["é¸æŠ", "item_key", "reason", "note", "created_at", "updated_at", "id"]
        df = df[cols]

        st.write("### ğŸ“‹ ç¾åœ¨ã®å…¥è·å›°é›£ãƒªã‚¹ãƒˆ")

        edited_df = st.data_editor(
            df,
            use_container_width=True,
            num_rows="dynamic",
            column_config={
                "é¸æŠ": st.column_config.CheckboxColumn("é¸æŠ")
            },
            disabled=[
                "item_key", "reason", "note", "created_at", "updated_at"
            ]
        )

        selected_df = edited_df[edited_df["é¸æŠ"]]
        selected_ids = selected_df["id"].tolist()

        # âœ… ãƒœã‚¿ãƒ³ç„¡åŠ¹åŒ–ç®¡ç†
        delete_btn_disabled = False

        if st.button("âœ… é¸æŠã—ãŸè¡Œã‚’å‰Šé™¤"):
            if selected_ids:
                for _id in selected_ids:
                    record = df[df["id"] == _id].copy().to_dict(orient="records")[0]
                    record.pop("é¸æŠ", None)
                    record.pop("created_at", None)
                    record.pop("updated_at", None)
                    record["item_id"] = record["id"]
                    record.pop("id")
                    record["action"] = "delete"
                    record["action_at"] = datetime.datetime.now(ZoneInfo("Asia/Tokyo")).isoformat()
        
                    res1 = requests.post(
                        f"{SUPABASE_URL}/rest/v1/difficult_items_history",
                        headers={**HEADERS, "Prefer": "return=representation"},
                        json=record
                    )
        
                    res2 = requests.delete(
                        f"{SUPABASE_URL}/rest/v1/difficult_items?id=eq.{_id}",
                        headers=HEADERS
                    )
        
                st.success("âœ… å‰Šé™¤å®Œäº†ï¼")
                st.rerun()
            else:
                st.warning("âš ï¸ è¡ŒãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“")

    with st.form("add_difficult_item"):
        item_key = st.text_input("ãƒ–ãƒ©ãƒ³ãƒ‰ / å•†å“å / JAN ãªã©")
        reason = st.text_input("å…¥è·å›°é›£ç†ç”±")
        note = st.text_area("å‚™è€ƒ")

        submitted = st.form_submit_button("ç™»éŒ²ã™ã‚‹")
        if submitted:
            payload = {
                "item_key": item_key,
                "reason": reason,
                "note": note
            }

            res = requests.post(
                f"{SUPABASE_URL}/rest/v1/difficult_items",
                headers={**HEADERS, "Prefer": "return=representation"},
                json=payload
            )
            st.write("ç™»éŒ²POST:", res.status_code, res.text)

            if res.status_code in [200, 201]:
                record = res.json()[0]
                record["item_id"] = record["id"]
                record.pop("id")
                record.pop("created_at", None)  # â† å¿˜ã‚Œãšè¿½åŠ ï¼
                record.pop("updated_at", None)  # â† å¿˜ã‚Œãšè¿½åŠ ï¼
                record["action"] = "insert"
                record["action_at"] = datetime.datetime.now(ZoneInfo("Asia/Tokyo")).isoformat()

                res2 = requests.post(
                    f"{SUPABASE_URL}/rest/v1/difficult_items_history",
                    headers={**HEADERS, "Prefer": "return=representation"},
                    json=record
                )

                st.success("âœ… ç™»éŒ²ã—ã¾ã—ãŸï¼")
                st.rerun()
            else:
                st.error(f"ç™»éŒ²å¤±æ•—: {res.text}")

    df_history = fetch_table("difficult_items_history")
    
    if not df_history.empty:
        one_week_ago = datetime.datetime.now(ZoneInfo("Asia/Tokyo")) - datetime.timedelta(days=7)
        df_history["action_at"] = pd.to_datetime(df_history["action_at"], utc=True)
        df_history = df_history[df_history["action_at"] >= one_week_ago]
    
        # ğŸ”¥ JSTã«å¤‰æ›
        df_history["action_at"] = df_history["action_at"].dt.tz_convert("Asia/Tokyo")
        df_history["action_at"] = df_history["action_at"].dt.strftime("%Y-%m-%d %H:%M:%S")
    
        st.write("ğŸ“œ **å±¥æ­´ï¼ˆç›´è¿‘7æ—¥åˆ†ï¼‰**")
        st.dataframe(df_history, use_container_width=True)
    else:
        st.write("ğŸ“œ **å±¥æ­´ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“**")


# parse_items_fixed ã¯ä»Šã®ã¾ã¾åˆ©ç”¨OK

elif mode == "order":
    import numpy as np
    from datetime import datetime, date

    st.subheader("ğŸ“¦ ç™ºæ³¨æ›¸ä½œæˆãƒ¢ãƒ¼ãƒ‰")

    option = st.radio("å…¥åŠ›æ–¹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„", ["ãƒ†ã‚­ã‚¹ãƒˆè²¼ã‚Šä»˜ã‘", "CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"])
    df_order = None

    # ---------- ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé…å¸ƒï¼ˆCSVã®ã¿ï¼‰ ----------
    def provide_template():
        template = pd.DataFrame({
            "jan": [],
            "æ•°é‡": [],
            "å˜ä¾¡": []
        })
        csv_temp = template.to_csv(index=False, encoding="utf-8-sig")
        st.download_button(
            "ğŸ“ å…¥åŠ›ç”¨CSVãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_temp,
            mime="text/csv",
            file_name="order_template.csv",
            help="å¿…é ˆåˆ—: jan, æ•°é‡, å˜ä¾¡ï¼ˆãƒ­ãƒƒãƒˆÃ—æ•°é‡ã§ã‚‚å¯ï¼‰"
        )

    provide_template()

    # ---------- å…¥åŠ› ----------
    if option == "ãƒ†ã‚­ã‚¹ãƒˆè²¼ã‚Šä»˜ã‘":
        input_text = st.text_area("æ³¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’è²¼ã‚Šä»˜ã‘", height=300)
        if st.button("ãƒ†ã‚­ã‚¹ãƒˆã‚’å¤‰æ›"):
            if not input_text.strip():
                st.warning("âš  ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            else:
                df_order = parse_items_fixed(input_text)
                if df_order is not None and not df_order.empty and "å“ç•ª" in df_order.columns:
                    df_order = df_order[df_order["å“ç•ª"] != "åˆè¨ˆ"]
                else:
                    st.warning("âš  å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’æ­£ã—ãå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

    elif option == "CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
        uploaded_file = st.file_uploader("æ³¨æ–‡CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["csv", "xlsx"])
        if uploaded_file is not None:
            try:
                if uploaded_file.name.endswith(".xlsx"):
                    df_order = pd.read_excel(uploaded_file)
                else:
                    df_order = pd.read_csv(uploaded_file, encoding="utf-8-sig")
            except UnicodeDecodeError:
                df_order = pd.read_csv(uploaded_file, encoding="shift_jis")

            df_order.columns = df_order.columns.str.strip().str.lower()

            rename_map = {
                "janã‚³ãƒ¼ãƒ‰": "jan", "ï¼ªï¼¡ï¼®": "jan", "jan": "jan", "JAN": "jan",
                "æ•°é‡": "æ•°é‡", "æ•°": "æ•°é‡", "qty": "æ•°é‡",
                "ãƒ­ãƒƒãƒˆÃ—æ•°é‡": "ãƒ­ãƒƒãƒˆÃ—æ•°é‡",
                "å˜ä¾¡": "å˜ä¾¡", "ä¾¡æ ¼": "å˜ä¾¡", "price": "å˜ä¾¡"
            }
            df_order.rename(columns={k.lower(): v for k, v in rename_map.items() if k.lower() in df_order.columns}, inplace=True)

            if "jan" not in df_order.columns:
                st.error("âŒ CSV/ã‚¨ã‚¯ã‚»ãƒ«ã« 'jan' åˆ—ãŒã‚ã‚Šã¾ã›ã‚“")
                df_order = None

    # ---------- ç™ºæ³¨æƒ…å ± ----------
    suppliers = [
        "0020 ã‚¨ãƒ³ãƒ‘ã‚¤ãƒ¤è‡ªå‹•è»Šæ ªå¼ä¼šç¤¾ï¼ˆKONNGUâ€™Sï¼‰", "0025 æ ªå¼ä¼šç¤¾ã‚ªãƒ³ãƒ€","0029 Kãƒ»BLUEæ ªå¼ä¼šç¤¾",
        "0072 æ–°å¯Œå£«ãƒãƒ¼ãƒŠãƒ¼æ ªå¼ä¼šç¤¾", "0073 æ ªå¼ä¼šç¤¾ã€€ã‚¨ã‚£ãƒãƒ»ã‚±ã‚¤", "0077 å¤§åˆ†å…±å’Œæ ªå¼ä¼šç¤¾",
        "0085 ä¸­å¤®ç‰©ç”£æ ªå¼ä¼šç¤¾", "0106 è¥¿å·æ ªå¼ä¼šç¤¾", "0197 å¤§æœ¨åŒ–ç²§å“æ ªå¼ä¼šç¤¾", "0201 ç¾é‡‘ä»•å…¥ã‚Œ",
        "0202 ãƒˆãƒ©ã‚¹ã‚³ä¸­å±±æ ªå¼ä¼šç¤¾", "0256 æ ªå¼ä¼šç¤¾ ã‚°ãƒ©ãƒ³ã‚¸ã‚§", "0258 æ ªå¼ä¼šç¤¾ ãƒ•ã‚¡ã‚¤ãƒ³",
        "0263 æ ªå¼ä¼šç¤¾ãƒ¡ãƒ‡ã‚£ãƒ•ã‚¡ã‚¤ãƒ³", "0285 æœ‰é™ä¼šç¤¾ã‚ªãƒ¼ã‚¶ã‚¤é¦–è—¤", "0343 æ ªå¼ä¼šç¤¾æ£®ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ",
        "0376 è…é‡æ ªå¼ä¼šç¤¾", "0402 ãƒãƒªãƒå…±å’Œç‰©ç”£æ ªå¼ä¼šç¤¾", "0411 æ ªå¼ä¼šç¤¾ãƒ©ã‚¯ãƒ¼ãƒ³ã‚³ãƒãƒ¼ã‚¹ï¼ˆã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒ‡ãƒªãƒãƒªãƒ¼ï¼‰",
        "0435 æ ªå¼ä¼šç¤¾ æµä¹…å•†äº‹", "0444 ãƒãƒŠãƒ¢ãƒ³ãƒ¯ãƒ¼ã‚¯ã‚¹ åˆåŒä¼šç¤¾", "0445 å¯Œæ£®å•†äº‹ æ ªå¼ä¼šç¤¾",
        "0457 ã‚«ãƒã‚¤ã‚·æ ªå¼ä¼šç¤¾", "0468 ç‹å­å›½éš›è²¿æ˜“æ ªå¼ä¼šç¤¾", "0469 æ ªå¼ä¼šç¤¾ æ–°æ—¥é…è–¬å“",
        "0474 æ ªå¼ä¼šç¤¾ã€€äº”æ´²", "0475 æ ªå¼ä¼šç¤¾ã‚·ã‚²ãƒãƒ„", "0476 ã‚«ãƒ¼ãƒ‰ä»•å…¥ã‚Œ",
        "0479 ã‚¹ã‚±ãƒ¼ã‚¿ãƒ¼æ ªå¼ä¼šç¤¾", "0482 é¢¨é›²å•†äº‹æ ªå¼ä¼šç¤¾", "0484 ZSAå•†äº‹æ ªå¼ä¼šç¤¾",
        "0486 Maple Internationalæ ªå¼ä¼šç¤¾", "0490 NEW WINDæ ªå¼ä¼šç¤¾", "0491 ã‚¢ãƒ—ãƒ©ã‚¤ãƒ‰æ ªå¼ä¼šç¤¾",
        "0504 äº¬æµœå•†äº‹æ ªå¼ä¼šç¤¾", "0510 æ ªå¼ä¼šç¤¾ã‚¿ã‚¸ãƒãƒ¤",
    ]
    employees = ["079 éš‹è‰¶å‰", "031 æ–è—¤è£•å²", "037 ç±³æ¾¤å’Œæ•", "043 å¾è¶Š"]
    departments = ["è¼¸å‡ºäº‹æ¥­éƒ¨ : è¼¸å‡ºï¼ˆASEANï¼‰", "è¼¸å‡ºäº‹æ¥­éƒ¨ : è¼¸å‡ºï¼ˆä¸­å›½ï¼‰", "è¼¸å‡ºäº‹æ¥­éƒ¨"]
    locations = ["JD-ç‰©æµ-åƒè‘‰", "å¼å¤©å€‰åº«"]

    col1, col2, col3 = st.columns(3)
    with col1:
        external_id = datetime.now().strftime("%Y%m%d%H%M%S")
        st.text_input("å¤–éƒ¨ID", value=external_id, disabled=True)
        supplier = st.selectbox("ä»•å…¥å…ˆ", suppliers)
    with col2:
        order_date = st.date_input("æ—¥ä»˜", value=date.today())
        employee = st.selectbox("å¾“æ¥­å“¡", employees)
    with col3:
        department = st.selectbox("éƒ¨é–€", departments)
        location = st.selectbox("å ´æ‰€", locations)

    memo = st.text_input("ãƒ¡ãƒ¢", "")

    # ---------- ç™ºæ³¨æ›¸ç”Ÿæˆ ----------
    if df_order is not None and not df_order.empty:
        df_item = fetch_table("item_master")
        df_item.columns = df_item.columns.str.strip().str.lower()

        # JANæ•´å½¢ï¼ˆå…ˆé ­00000ã‚’å‰Šé™¤ï¼‰
        df_order["jan"] = df_order["jan"].astype(str).str.strip().str.replace(r"^0{5,}", "", regex=True)
        df_item["jan"] = df_item["jan"].astype(str).str.strip().str.replace(r"^0{5,}", "", regex=True)

        # ç¨ç‡åˆ¤å®šé–¢æ•°
        def get_tax_rate(schedule):
            if not schedule or pd.isna(schedule): return 0.0
            if "10" in schedule: return 0.10
            if "8" in schedule: return 0.08
            return 0.0

        df_item["tax_rate"] = df_item.get("ç´ç¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«", "").apply(get_tax_rate)

        df = df_order.merge(df_item, on="jan", how="left")

        # æ¬ æJANã®è¡¨ç¤º
        missing = df[df["å•†å“å"].isna()]
        if not missing.empty:
            st.warning(f"âš  {len(missing)} ä»¶ã®JANãŒ item_master ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            st.dataframe(missing[["jan"]])

        # æ•°å€¤å¤‰æ›
        qty_col = "ãƒ­ãƒƒãƒˆÃ—æ•°é‡" if "ãƒ­ãƒƒãƒˆÃ—æ•°é‡" in df.columns else "æ•°é‡"
        df["æ•°é‡"] = pd.to_numeric(df[qty_col], errors="coerce").fillna(0).astype(int)
        df["å˜ä¾¡"] = pd.to_numeric(df["å˜ä¾¡"], errors="coerce").fillna(0).astype(int)

        # é‡‘é¡ãƒ»ç¨é¡ãƒ»ç·é¡
        df["é‡‘é¡"] = df["å˜ä¾¡"] * df["æ•°é‡"]
        df["ç¨é¡"] = np.floor(df["é‡‘é¡"] * df["tax_rate"]).fillna(0).astype(int)
        df["ç·é¡"] = df["é‡‘é¡"] + df["ç¨é¡"]

        order_date_str = order_date.strftime("%Y/%m/%d")

        df_out = pd.DataFrame({
            "å¤–éƒ¨ID": external_id,
            "ä»•å…¥å…ˆ": supplier,
            "æ—¥ä»˜": order_date_str,
            "å¾“æ¥­å“¡": employee,
            "éƒ¨é–€": department,
            "ãƒ¡ãƒ¢": memo,
            "å ´æ‰€": location,
            "ã‚¢ã‚¤ãƒ†ãƒ ": df.get("å•†å“ã‚³ãƒ¼ãƒ‰", "").astype(str) + " " + df.get("å•†å“å", ""),
            "æ•°é‡": df["æ•°é‡"],
            "å˜ä¾¡/ç‡": df["å˜ä¾¡"],
            "é‡‘é¡": df["é‡‘é¡"],
            "ç¨é¡": df["ç¨é¡"],
            "ç·é¡": df["ç·é¡"]
        })

        st.subheader("ğŸ“‘ ç™ºæ³¨æ›¸ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
        st.dataframe(df_out)

        csv_out = df_out.to_csv(index=False, encoding="utf-8-sig")
        st.download_button(
            label="ğŸ“¥ ç™ºæ³¨æ›¸CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_out,
            file_name=f"ç™ºæ³¨æ›¸_{external_id}.csv",
            mime="text/csv"
        )
        
elif mode == "store_profit":
    st.subheader("ğŸª åº—èˆ—åˆ¥ç²—åˆ©ä¸€è¦§")

    # Supabase ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
    df = fetch_table("store_profit_lines")

    if df is None or df.empty:
        st.warning("store_profit_lines ãŒç©ºã‹ã€èª­ã¿å‡ºã›ã¦ã„ã¾ã›ã‚“ã€‚")
        st.info("æ¬¡ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š\n"
                "1) Supabaseã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ï¼ˆSQLã§ SELECT count(*)ï¼‰\n"
                "2) RLSãŒæœ‰åŠ¹ãªã‚‰ã€åŒ¿åã‚­ãƒ¼(anon)ã«å¯¾ã—ã¦SELECTè¨±å¯ãƒãƒªã‚·ãƒ¼ãŒã‚ã‚‹ã‹\n"
                "3) fetch_table ãŒ 'store_profit_lines' ã‚’æ­£ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«å‘ã‘ã¦ã„ã‚‹ã‹")
        st.stop()

    # åˆ—ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    required_cols = {"report_period","line_type","store","qty","revenue","defined_cost","gross_profit","original_line"}
    missing = required_cols - set(df.columns)
    if missing:
        st.error(f"å¿…è¦åˆ—ãŒè¶³ã‚Šã¾ã›ã‚“: {missing}")
        st.stop()

    # æœŸé–“é¸æŠ
    periods = sorted(df["report_period"].dropna().unique())
    if len(periods) == 0:
        st.warning("report_period ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã® period ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    sel_period = st.selectbox("å¯¾è±¡æœŸé–“ã‚’é¸æŠ", periods, index=len(periods)-1)

    dfp = df[df["report_period"] == sel_period]

    # åº—èˆ—åˆ¥é›†è¨ˆï¼ˆdetailã®ã¿ï¼‰
    dfd = dfp[dfp["line_type"] == "detail"].copy()
    if dfd.empty:
        st.warning("ã“ã®æœŸé–“ã®æ˜ç´°è¡Œï¼ˆline_type='detail'ï¼‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚CSVã®å–ã‚Šè¾¼ã¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # æ•°å€¤å‹ã«å¤‰æ›ï¼ˆå¿µã®ãŸã‚ï¼‰
    for c in ["qty","revenue","defined_cost","gross_profit"]:
        dfd[c] = pd.to_numeric(dfd[c], errors="coerce").fillna(0).astype(int)

    grouped = (
        dfd.groupby("store", as_index=False)
           .agg(qty=("qty","sum"),
                revenue=("revenue","sum"),
                defined_cost=("defined_cost","sum"),
                gross_profit=("gross_profit","sum"))
    )
    grouped["gross_margin"] = (grouped["gross_profit"] / grouped["revenue"] * 100).fillna(0).round(2)

        # ---- åˆè¨ˆï¼ˆå…¨åº—ï¼‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å…ˆã«è¡¨ç¤ºã™ã‚‹ ---------------------------------
    # è¡¨ç¤ºãƒ©ãƒ™ãƒ«ï¼ˆæ—¥æœ¬èª/ä¸­å›½èªï¼‰
    LABELS = {
        "æ—¥æœ¬èª": {
            "store": "åº—èˆ—",
            "qty": "æ•°é‡",
            "revenue": "å£²ä¸Š",
            "defined_cost": "å®šç¾©åŸä¾¡",
            "gross_profit": "ç²—åˆ©",
            "gross_margin": "ç²—åˆ©ç‡",
        },
        "ä¸­æ–‡": {
            "store": "åº—é“º",
            "qty": "æ•°é‡",
            "revenue": "é”€å”®é¢",
            "defined_cost": "å®šä¹‰æˆæœ¬",
            "gross_profit": "æ¯›åˆ©",
            "gross_margin": "æ¯›åˆ©ç‡",
        },
    }
    TOTAL_LABELS = {
        "æ—¥æœ¬èª": {
            "title": "ğŸ§® åˆè¨ˆï¼ˆå…¨åº—ï¼‰",
            "period": "å¯¾è±¡æœŸé–“",
            "qty": "åˆè¨ˆæ•°é‡",
            "revenue": "å£²ä¸Šåˆè¨ˆ",
            "defined_cost": "å®šç¾©åŸä¾¡åˆè¨ˆ",
            "gross_profit": "ç²—åˆ©åˆè¨ˆ",
            "gross_margin": "ç²—åˆ©ç‡",
            "download": "ğŸ“¥ åˆè¨ˆï¼ˆå…¨åº—ï¼‰ã‚’CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        },
        "ä¸­æ–‡": {
            "title": "ğŸ§® åˆè®¡ï¼ˆå…¨åº—ï¼‰",
            "period": "æœŸé—´",
            "qty": "åˆè®¡æ•°é‡",
            "revenue": "é”€å”®é¢åˆè®¡",
            "defined_cost": "å®šä¹‰æˆæœ¬åˆè®¡",
            "gross_profit": "æ¯›åˆ©åˆè®¡",
            "gross_margin": "æ¯›åˆ©ç‡",
            "download": "ğŸ“¥ ä¸‹è½½åˆè®¡ï¼ˆå…¨åº—ï¼‰CSV",
        },
    }
    labels = LABELS.get(language, LABELS["æ—¥æœ¬èª"])
    tlabels = TOTAL_LABELS.get(language, TOTAL_LABELS["æ—¥æœ¬èª"])

    # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿
    def fmt_int(x):
        try:
            return f"{int(x):,}"
        except:
            return x
    def fmt_money(x):
        try:
            return f"{int(round(float(x))):,}"
        except:
            return x
    def fmt_pct(x):
        try:
            return f"{float(x):.2f}%"
        except:
            return x

    # åˆè¨ˆå€¤
    total_qty     = int(grouped["qty"].sum())
    total_rev     = int(grouped["revenue"].sum())
    total_cost    = int(grouped["defined_cost"].sum())
    total_gp      = int(grouped["gross_profit"].sum())
    total_margin  = round((total_gp / total_rev * 100) if total_rev else 0.0, 2)

    df_total = pd.DataFrame([{
        tlabels["period"]: sel_period,
        tlabels["qty"]: fmt_int(total_qty),
        tlabels["revenue"]: fmt_money(total_rev),
        tlabels["defined_cost"]: fmt_money(total_cost),
        tlabels["gross_profit"]: fmt_money(total_gp),
        tlabels["gross_margin"]: fmt_pct(total_margin),
    }])

    st.markdown(f"### {tlabels['title']}")
    st.dataframe(df_total, use_container_width=True)

    st.download_button(
        tlabels["download"],
        df_total.to_csv(index=False).encode("utf-8-sig"),
        file_name=f"store_profit_total_{sel_period}.csv",
        mime="text/csv",
    )


    # ---- è¡¨ç¤ºãƒ©ãƒ™ãƒ«ï¼ˆæ—¥æœ¬èª/ä¸­å›½èªï¼‰ã¨è¨€èªåˆ¥ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ ----
    LABELS = {
        "æ—¥æœ¬èª": {
            "store": "åº—èˆ—",
            "qty": "æ•°é‡",
            "revenue": "å£²ä¸Š",
            "defined_cost": "å®šç¾©åŸä¾¡",
            "gross_profit": "ç²—åˆ©",
            "gross_margin": "ç²—åˆ©ç‡",
        },
        "ä¸­æ–‡": {
            "store": "åº—é“º",
            "qty": "æ•°é‡",
            "revenue": "é”€å”®é¢",
            "defined_cost": "å®šä¹‰æˆæœ¬",
            "gross_profit": "æ¯›åˆ©",
            "gross_margin": "æ¯›åˆ©ç‡",
        },
    }
    labels = LABELS.get(language, LABELS["æ—¥æœ¬èª"])

    # 0é™¤ç®—ãªã©ã®NaN/infã¯0ã«
    grouped.replace([float("inf"), float("-inf")], 0, inplace=True)
    grouped.fillna(0, inplace=True)

    # è¡¨ç¤ºç”¨ã«åˆ—åã‚’ç¿»è¨³
    display_df = grouped.rename(columns={
        "store": labels["store"],
        "qty": labels["qty"],
        "revenue": labels["revenue"],
        "defined_cost": labels["defined_cost"],
        "gross_profit": labels["gross_profit"],
        "gross_margin": labels["gross_margin"],
    })

    # æ•°å­—ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼šã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š / ç²—åˆ©ç‡ã¯%ä»˜ãï¼ˆå°æ•°2æ¡ï¼‰
    def fmt_int(x): 
        try: return f"{int(x):,}"
        except: return x
    def fmt_money(x):
        try: return f"{int(round(float(x))):,}"
        except: return x
    def fmt_pct(x):
        try: return f"{float(x):.2f}%"
        except: return x

    display_df[labels["qty"]] = display_df[labels["qty"]].map(fmt_int)
    display_df[labels["revenue"]] = display_df[labels["revenue"]].map(fmt_money)
    display_df[labels["defined_cost"]] = display_df[labels["defined_cost"]].map(fmt_money)
    display_df[labels["gross_profit"]] = display_df[labels["gross_profit"]].map(fmt_money)
    display_df[labels["gross_margin"]] = display_df[labels["gross_margin"]].map(fmt_pct)

    st.write("### åº—èˆ—åˆ¥é›†è¨ˆ")
    # ç²—åˆ©ã®å¤§ãã„é †ã§è¦‹ã‚„ã™ã
    display_df = display_df.sort_values(by=labels["gross_profit"], ascending=False, key=lambda s: s.str.replace(",", "", regex=False).astype(int))
    st.dataframe(display_df, use_container_width=True)

    # ---- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆé›†è¨ˆ / åŸæ–‡ï¼‰----
    # é›†è¨ˆCSVã¯è¦‹ãŸç›®ã®åˆ—åã§å‡ºåŠ›ï¼ˆæ•°å€¤ã¯ç”Ÿãƒ‡ãƒ¼ã‚¿ã®ã¾ã¾ãŒè‰¯ã‘ã‚Œã° 'grouped' ã‚’ä½¿ã£ã¦ãã ã•ã„ï¼‰
    grouped_l10n = grouped.rename(columns={
        "store": labels["store"],
        "qty": labels["qty"],
        "revenue": labels["revenue"],
        "defined_cost": labels["defined_cost"],
        "gross_profit": labels["gross_profit"],
        "gross_margin": labels["gross_margin"],
    })
    st.download_button(
        "ğŸ“¥ åº—èˆ—åˆ¥é›†è¨ˆã‚’CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        grouped_l10n.to_csv(index=False).encode("utf-8-sig"),
        file_name=f"store_profit_summary_{sel_period}.csv",
        mime="text/csv",
    )

    csv_original = "\n".join(dfp["original_line"].tolist())
    st.download_button(
        "ğŸ“¥ å…ƒCSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå®Œå…¨å¾©å…ƒï¼‰",
        csv_original,
        file_name=f"store_profit_original_{sel_period}.csv",
        mime="text/csv",
    )

elif mode == "daily_sales":
    st.subheader("ğŸ“† åº—èˆ—åˆ¥å‰æ—¥å£²ä¸Šï¼ˆæœ€æ–°æ—¥ï¼‰")

    df = fetch_table("store_profit_daily_lines")
    if df is None or df.empty:
        st.warning("store_profit_daily_lines ãŒç©ºã‹ã€èª­ã¿å‡ºã›ã¦ã„ã¾ã›ã‚“ã€‚")
        st.stop()

    required = {"report_date","line_type","store","item","qty","revenue","defined_cost","gross_profit"}
    missing = required - set(df.columns)
    if missing:
        st.error(f"å¿…è¦åˆ—ãŒè¶³ã‚Šã¾ã›ã‚“: {missing}")
        st.stop()

    df["report_date"] = pd.to_datetime(df["report_date"], errors="coerce").dt.date
    for c in ["qty","revenue","defined_cost","gross_profit"]:
        df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0).astype(int)

    # æœ€æ–°æ—¥ã ã‘
    latest_date = df["report_date"].max()
    cur = df[df["report_date"] == latest_date].copy()

    # detailã®ã¿ + ã€Œåˆè¨ˆ/ç·è¨ˆ/è¨ˆã§å§‹ã¾ã‚‹ç–‘ä¼¼æ˜ç´°ã€ã‚„ EMPTY ã‚’é™¤å¤–
    pat_agg = r"^(åˆè¨ˆ|ç·è¨ˆ|è¨ˆ)\b"
    cur_detail = cur[cur["line_type"] == "detail"].copy()
    cur_detail = cur_detail[
        ~cur_detail["item"].astype(str).str.match(pat_agg, na=False) &
        ~cur_detail["item_name"].astype(str).str.fullmatch(r"\s*EMPTY\s*", na=False)
    ]

    # â”€ å…¨åº—åˆè¨ˆï¼šdetail ã®ã¿ã‹ã‚‰ç®—å‡ºï¼ˆé‡è¤‡ãªã—ï¼‰ â”€
    tot_qty  = int(cur_detail["qty"].sum())
    tot_rev  = int(cur_detail["revenue"].sum())
    tot_cost = int(cur_detail["defined_cost"].sum())
    tot_gp   = int(cur_detail["gross_profit"].sum())
    tot_mgn  = round((tot_gp / tot_rev * 100) if tot_rev else 0.0, 2)

    def fmt_int(x):   return f"{int(x):,}"
    def fmt_money(x): return f"{int(round(float(x))):,}"
    def fmt_pct(x):   return f"{float(x):.2f}%"

    df_total = pd.DataFrame([{
        "å¯¾è±¡æ—¥": latest_date,
        "åˆè¨ˆæ•°é‡": fmt_int(tot_qty),
        "å£²ä¸Šåˆè¨ˆ": fmt_money(tot_rev),
        "å®šç¾©åŸä¾¡åˆè¨ˆ": fmt_money(tot_cost),
        "ç²—åˆ©åˆè¨ˆ": fmt_money(tot_gp),
        "ç²—åˆ©ç‡": fmt_pct(tot_mgn),
    }])

    st.markdown("### ğŸ§® åˆè¨ˆï¼ˆå…¨åº—ï¼‰")
    st.dataframe(df_total, use_container_width=True)
    st.download_button(
        "ğŸ“¥ åˆè¨ˆï¼ˆå…¨åº—ï¼‰CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        df_total.to_csv(index=False).encode("utf-8-sig"),
        file_name=f"daily_sales_total_{latest_date}.csv",
        mime="text/csv",
    )

    # â”€ åº—èˆ—åˆ¥ï¼šdetail ã®ã¿ã‹ã‚‰é›†è¨ˆ â”€
    cur_g = (cur_detail.groupby("store", as_index=False)
                .agg(qty=("qty","sum"),
                     revenue=("revenue","sum"),
                     defined_cost=("defined_cost","sum"),
                     gross_profit=("gross_profit","sum")))
    cur_g["gross_margin"] = (
        (cur_g["gross_profit"] / cur_g["revenue"].replace({0: pd.NA}) * 100)
        .astype(float).round(2).fillna(0.0)
    )

    LABELS = {
        "æ—¥æœ¬èª": {"store":"åº—èˆ—","qty":"æ•°é‡","revenue":"å£²ä¸Š","defined_cost":"å®šç¾©åŸä¾¡","gross_profit":"ç²—åˆ©","gross_margin":"ç²—åˆ©ç‡"},
        "ä¸­æ–‡":   {"store":"åº—é“º","qty":"æ•°é‡","revenue":"é”€å”®é¢","defined_cost":"å®šä¹‰æˆæœ¬","gross_profit":"æ¯›åˆ©","gross_margin":"æ¯›åˆ©ç‡"},
    }
    labels = LABELS.get(language, LABELS["æ—¥æœ¬èª"])

    disp = cur_g.rename(columns={
        "store": labels["store"],
        "qty": labels["qty"],
        "revenue": labels["revenue"],
        "defined_cost": labels["defined_cost"],
        "gross_profit": labels["gross_profit"],
        "gross_margin": labels["gross_margin"],
    }).copy()

    disp[labels["qty"]]          = disp[labels["qty"]].map(fmt_int)
    disp[labels["revenue"]]      = disp[labels["revenue"]].map(fmt_money)
    disp[labels["defined_cost"]] = disp[labels["defined_cost"]].map(fmt_money)
    disp[labels["gross_profit"]] = disp[labels["gross_profit"]].map(fmt_money)
    disp[labels["gross_margin"]] = disp[labels["gross_margin"]].map(fmt_pct)

    st.write("### åº—èˆ—åˆ¥")
    disp = disp.sort_values(
        by=labels["revenue"],
        ascending=False,
        key=lambda s: s.str.replace(",", "", regex=False).astype(int)
    )
    st.dataframe(disp, use_container_width=True)

    st.download_button(
        "ğŸ“¥ åº—èˆ—åˆ¥ï¼ˆæ•°å€¤ãã®ã¾ã¾ï¼‰CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        cur_g.to_csv(index=False).encode("utf-8-sig"),
        file_name=f"daily_sales_by_store_{latest_date}.csv",
        mime="text/csv",
    )


elif mode == "expiry_manage":
    # =========================
    # ğŸ§Š è³å‘³æœŸé™ç®¡ç†ãƒ¢ãƒ¼ãƒ‰
    # =========================
    st.subheader("ğŸ§Š è³å‘³æœŸé™ç®¡ç†" if language == "æ—¥æœ¬èª" else "ğŸ§Š ä¿è´¨æœŸç®¡ç†")

    # --- Supabaseï¼ˆã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸Šéƒ¨ã§å®šç¾©æ¸ˆã¿ï¼šSUPABASE_URL_PRE / HEADERS_PRE ã‚’ä½¿ã†ï¼‰ ---
    SUPABASE_URL = SUPABASE_URL_PRE
    HEADERS = HEADERS_PRE

    # --- Lark Sheets è¨­å®šï¼ˆst.secrets æ¨å¥¨ï¼‰ ---
    # secrets.toml ä¾‹ï¼š
    # LARK_APP_ID="xxxx"
    # LARK_APP_SECRET="xxxx"
    # LARK_SPREADSHEET_TOKEN="O6VQsoFDOhOPV7t3qSslkoSEg3b"
    # LARK_SHEET_ID="91fd41"
    try:
        LARK_APP_ID = st.secrets["LARK_APP_ID"]
        LARK_APP_SECRET = st.secrets["LARK_APP_SECRET"]
        LARK_SPREADSHEET_TOKEN = st.secrets.get("LARK_SPREADSHEET_TOKEN", "O6VQsoFDOhOPV7t3qSslkoSEg3b")
        LARK_SHEET_ID = st.secrets.get("LARK_SHEET_ID", "91fd41")
    except Exception:
        st.error("âŒ Lark ã®èªè¨¼æƒ…å ±ãŒ st.secrets ã«ã‚ã‚Šã¾ã›ã‚“ï¼ˆLARK_APP_ID / LARK_APP_SECRETï¼‰")
        st.stop()

    # ---------- ãƒ©ãƒ™ãƒ« ----------
    LABEL = {
        "æ—¥æœ¬èª": {
            "sync": "ğŸ”„ Larkã‹ã‚‰åŒæœŸï¼ˆæ‰‹å‹•ï¼‰",
            "synced": "âœ… åŒæœŸå®Œäº†",
            "syncing": "åŒæœŸä¸­...",
            "err": "âš ï¸ ã‚¨ãƒ©ãƒ¼è¡Œ",
            "filters": "ğŸ” ãƒ•ã‚£ãƒ«ã‚¿",
            "status": "çŠ¶æ…‹",
            "days": "æ®‹ã‚Šæ—¥æ•°",
            "expiry": "æœ€çŸ­è³å‘³æœŸé™",
            "download": "ğŸ“¥ CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            "limit": "è¡¨ç¤ºä»¶æ•°ä¸Šé™",
            "keyword": "JAN / å•†å“å æ¤œç´¢",
            "only_with_expiry": "è³å‘³æœŸé™ã‚ã‚Šã®ã¿",
            "only_no_expiry": "æœªç™»éŒ²ã®ã¿",
        },
        "ä¸­æ–‡": {
            "sync": "ğŸ”„ ä»LarkåŒæ­¥ï¼ˆæ‰‹åŠ¨ï¼‰",
            "synced": "âœ… åŒæ­¥å®Œæˆ",
            "syncing": "åŒæ­¥ä¸­...",
            "err": "âš ï¸ é”™è¯¯è¡Œ",
            "filters": "ğŸ” ç­›é€‰",
            "status": "çŠ¶æ€",
            "days": "å‰©ä½™å¤©æ•°",
            "expiry": "æœ€çŸ­ä¿è´¨æœŸ",
            "download": "ğŸ“¥ ä¸‹è½½CSV",
            "limit": "æ˜¾ç¤ºæ¡æ•°ä¸Šé™",
            "keyword": "æœç´¢ï¼šæ¡ç  / å•†å“å",
            "only_with_expiry": "ä»…æ˜¾ç¤ºå·²ç™»è®°",
            "only_no_expiry": "ä»…æ˜¾ç¤ºæœªç™»è®°",
        }
    }[language]

    # =========================
    # Lark API
    # =========================
    def lark_get_tenant_token(app_id: str, app_secret: str) -> str:
        url = "https://open.larksuite.com/open-apis/auth/v3/tenant_access_token/internal/"
        r = requests.post(url, json={"app_id": app_id, "app_secret": app_secret}, timeout=30)
        r.raise_for_status()
        j = r.json()
        if j.get("code") != 0:
            raise RuntimeError(f"Lark token error: {j}")
        return j["tenant_access_token"]

    def lark_read_sheet_values(
        tenant_token: str,
        spreadsheet_token: str,
        sheet_id: str,
        rng: str = "A1:G5000"
    ):
        url = (
            "https://open.larksuite.com/open-apis/"
            f"sheets/v2/spreadsheets/{spreadsheet_token}/values_batch_get"
        )
        headers = {
            "Authorization": f"Bearer {tenant_token}"
        }
        params = {
            "ranges": f"{sheet_id}!{rng}"
        }
    
        r = requests.get(url, headers=headers, params=params, timeout=30)
        r.raise_for_status()
        j = r.json()
    
        if j.get("code") != 0:
            raise RuntimeError(j)
    
        # â† ã“ã“ãŒãƒã‚¤ãƒ³ãƒˆ
        return j["data"]["valueRanges"][0]["values"]




    # =========================
    # ãƒ‘ãƒ¼ã‚¹
    # =========================
    def normalize_jan_cell(x) -> str | None:
        if x is None:
            return None
        s = str(x).strip()
        if not s:
            return None
        # æ•°å­—ä»¥å¤–é™¤å»ï¼ˆãƒã‚¤ãƒ•ãƒ³ã‚„ã‚¹ãƒšãƒ¼ã‚¹æ··å…¥å¯¾ç­–ï¼‰
        s = re.sub(r"\D", "", s)
        return s if s else None

    def parse_date_cell(x):
        """
        Lark Sheets ã®æ—¥ä»˜ã¯
        - 'YYYY/MM/DD' ç­‰ã®æ–‡å­—åˆ—
        - Excelã‚·ãƒªã‚¢ãƒ«å€¤ï¼ˆä¾‹: 46326ï¼‰
        ã®ä¸¡æ–¹ãŒæ¥ã‚‹
        """
        if x is None:
            return None
    
        # æ•°å€¤ï¼ˆExcelã‚·ãƒªã‚¢ãƒ«ï¼‰
        if isinstance(x, (int, float)):
            base = datetime.date(1899, 12, 30)
            return (base + datetime.timedelta(days=int(x))).isoformat()
    
        s = str(x).strip()
        if not s:
            return None
    
        # æ–‡å­—åˆ—æ—¥ä»˜
        for fmt in ("%Y/%m/%d", "%Y-%m-%d", "%Y.%m.%d", "%Y%m%d"):
            try:
                return datetime.datetime.strptime(s, fmt).date().isoformat()
            except ValueError:
                pass
    
        raise ValueError(f"æ—¥ä»˜ã¨ã—ã¦è§£é‡ˆã§ãã¾ã›ã‚“: {s}")


    def min_date_iso(*isos):
        ds = [d for d in isos if d]
        return min(ds) if ds else None

    # =========================
    # Supabase upsertï¼ˆRESTï¼‰
    # =========================
    def supabase_truncate_item_expiry():
        url = f"{SUPABASE_URL}/rest/v1/rpc/truncate_item_expiry"
        r = requests.post(url, headers=HEADERS, json={}, timeout=60)
        if r.status_code not in [200, 204]:
            raise RuntimeError(f"Supabase truncate failed: {r.status_code} {r.text}")

    def supabase_upsert_item_expiry(rows: list[dict]) -> int:
        if not rows:
            return 0
        url = f"{SUPABASE_URL}/rest/v1/item_expiry"
        # resolution=merge-duplicates ã§ upsertï¼ˆPK=janæƒ³å®šï¼‰
        headers = {**HEADERS, "Prefer": "resolution=merge-duplicates,return=representation"}
        r = requests.post(url, headers=headers, json=rows, timeout=60)
        if r.status_code not in [200, 201]:
            raise RuntimeError(f"Supabase upsert failed: {r.status_code} {r.text}")
        data = r.json()
        return len(data) if isinstance(data, list) else len(rows)

    def sync_lark_to_supabase() -> dict:
        supabase_truncate_item_expiry()
        tenant = lark_get_tenant_token(LARK_APP_ID, LARK_APP_SECRET)
        values = lark_read_sheet_values(
            tenant_token=tenant,
            spreadsheet_token=LARK_SPREADSHEET_TOKEN,
            sheet_id=LARK_SHEET_ID,
            rng="A1:G5000"
        )

        if not values or len(values) < 2:
            return {"upserted": 0, "errors": []}

        upserts = []
        errors = []

        # 1è¡Œç›®ãƒ˜ãƒƒãƒ€ãƒ¼æƒ³å®šã€2è¡Œç›®ã‹ã‚‰
        for row_idx, row in enumerate(values[1:], start=2):
            try:
                a = row[0] if len(row) > 0 else None  # JAN
                b = row[1] if len(row) > 1 else None  # å•†å“å
                c = row[2] if len(row) > 2 else None
                d = row[3] if len(row) > 3 else None
                e = row[4] if len(row) > 4 else None
                f = row[5] if len(row) > 5 else None
                g = row[6] if len(row) > 6 else None

                jan = normalize_jan_cell(a)
                if not jan:
                    continue

                expiry_1 = parse_date_cell(c)
                expiry_2 = parse_date_cell(d)
                expiry_3 = parse_date_cell(e)
                expiry_4 = parse_date_cell(f)
                expiry_5 = parse_date_cell(g)
                expiry_min = min_date_iso(expiry_1, expiry_2, expiry_3, expiry_4, expiry_5)

                upserts.append({
                    "jan": jan,
                    "name": str(b).strip() if b is not None else None,
                    "expiry_1": expiry_1,
                    "expiry_2": expiry_2,
                    "expiry_3": expiry_3,
                    "expiry_4": expiry_4,
                    "expiry_5": expiry_5,
                    "expiry_min": expiry_min,
                    "updated_at": datetime.datetime.utcnow().isoformat()
                })

            except Exception as ex:
                errors.append({"row": row_idx, "raw": row, "error": str(ex)})

        # 500ä»¶ãšã¤
        upserted_total = 0
        for i in range(0, len(upserts), 500):
            upserted_total += supabase_upsert_item_expiry(upserts[i:i+500])

        return {"upserted": upserted_total, "errors": errors}

    # =========================
    # UI: åŒæœŸ
    # =========================
    st.markdown("### " + LABEL["sync"])
    if st.button(LABEL["sync"], key="expiry_sync_btn"):
        with st.spinner(LABEL["syncing"]):
            try:
                result = sync_lark_to_supabase()
                st.success(f"{LABEL['synced']}: {result['upserted']} ä»¶")
                if result["errors"]:
                    st.warning(f"{LABEL['err']}: {len(result['errors'])} ä»¶")
                    df_err = pd.DataFrame(result["errors"]).copy()
                
                    # raw(list) ã‚’æ–‡å­—åˆ—ã«ã—ã¦ Arrow å¤‰æ›ã‚¨ãƒ©ãƒ¼å›é¿
                    if "raw" in df_err.columns:
                        df_err["raw"] = df_err["raw"].apply(lambda x: " | ".join(map(str, x)) if isinstance(x, (list, tuple)) else str(x))
                
                    st.dataframe(df_err, use_container_width=True)

            except Exception as e:
                st.error(f"âŒ åŒæœŸå¤±æ•—: {e}")

    st.markdown("---")

    # =========================
    # ä¸€è¦§å–å¾—ï¼ˆSupabase â†’ pandasï¼‰
    # =========================
    @st.cache_data(ttl=5)
    def fetch_item_expiry():
        # å…¨ä»¶ï¼ˆå¿…è¦ãªã‚‰å¾Œã§ range / pagination è¿½åŠ ï¼‰
        url = f"{SUPABASE_URL}/rest/v1/item_expiry?select=*"
        r = requests.get(url, headers=HEADERS, timeout=60)
        if r.status_code != 200:
            st.error(f"item_expiry ã®å–å¾—ã«å¤±æ•—: {r.status_code} / {r.text}")
            return pd.DataFrame()
        return pd.DataFrame(r.json())
        
    @st.cache_data(ttl=5)  # TTLã¯å¥½ã¿ã§ 5ã€œ30ç§’
    def fetch_warehouse_stock():
        # jan ã¨ stock_available ã ã‘å–ã‚‹ï¼ˆè»½é‡åŒ–ï¼‰
        url = f"{SUPABASE_URL}/rest/v1/warehouse_stock?select=jan,stock_available"
        r = requests.get(url, headers=HEADERS, timeout=60)
        if r.status_code != 200:
            st.error(f"warehouse_stock ã®å–å¾—ã«å¤±æ•—: {r.status_code} / {r.text}")
            return pd.DataFrame()
        return pd.DataFrame(r.json())

    df = fetch_item_expiry()

    df_stock = fetch_warehouse_stock()

    if not df_stock.empty:
        # jan ã‚’æ–‡å­—åˆ—ã§æƒãˆã‚‹
        df_stock["jan"] = df_stock["jan"].astype(str).str.strip()
        # stock_available ã‚’æ•°å€¤åŒ–ï¼ˆç©ºã‚„æ–‡å­—ãŒæ··ã–ã£ã¦ã‚‚è½ã¡ãªã„ã‚ˆã†ã«ï¼‰
        df_stock["stock_available"] = pd.to_numeric(df_stock["stock_available"], errors="coerce").fillna(0).astype(int)
    
        # item_expiry å´ã‚‚ jan ã‚’æƒãˆã‚‹ï¼ˆå¾Œã§æ—¢ã«ã‚„ã£ã¦ã‚‹ãªã‚‰ã“ã“ã¯çœç•¥ã—ã¦OKï¼‰
        df["jan"] = df["jan"].astype(str).str.strip()
    
        # left joinï¼šitem_expiry ã‚’ä¸»ã«ã—ã¦åœ¨åº«ã‚’ä»˜ä¸
        df = df.merge(df_stock[["jan", "stock_available"]], on="jan", how="left")
    
    # åœ¨åº«ãŒç„¡ã„ï¼ˆæœªå–å¾—/NULLï¼‰å ´åˆã¯ 0 æ‰±ã„ã«
    df["stock_available"] = pd.to_numeric(df.get("stock_available"), errors="coerce").fillna(0).astype(int)


    # =========================
    # è¡¨ç¤ºç”¨åŠ å·¥
    # =========================
    if df.empty:
        st.info("item_expiry ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã«åŒæœŸã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # å‹æ•´å½¢
    # ä¾‹: Larkå´ã®ãƒ˜ãƒƒãƒ€åã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ã­
    # ã“ã“ã§ã¯åˆ—åãŒ jan, name, expiry_1..expiry_5 ã§æ¥ã¦ã‚‹å‰æ
    
    # --- æ–‡å­—åˆ—åˆ— ---
    df["jan"] = df["jan"].astype(str).str.strip()
    df["name"] = df["name"].astype(str).fillna("").str.strip()
    
    # --- æ—¥ä»˜åˆ—ï¼ˆç©ºæ¬„ã¯NaTâ†’Noneã«ã™ã‚‹ï¼‰---
    expiry_cols = ["expiry_1", "expiry_2", "expiry_3", "expiry_4", "expiry_5"]
    for c in expiry_cols:
        if c in df.columns:
            df[c] = pd.to_datetime(df[c], errors="coerce").dt.date
    
    # --- expiry_min ã‚’è¨ˆç®— ---
    if set(expiry_cols).issubset(df.columns):
        df["expiry_min"] = pd.to_datetime(df[expiry_cols].stack(), errors="coerce").groupby(level=0).min().dt.date
    else:
        df["expiry_min"] = None
    
    # --- updated_at ---
    df["updated_at"] = datetime.datetime.now(ZoneInfo("Asia/Tokyo")).isoformat()
    
    # --- JSONåŒ–ã®ãŸã‚ NaT/NaN ã‚’ None ã« ---
    df = df.where(pd.notnull(df), None)


    # expiry_min ã‚’ datetime ã«ï¼ˆtzãªã—ï¼‰
    df["expiry_min_dt"] = pd.to_datetime(df.get("expiry_min"), errors="coerce")
    
    # ä»Šæ—¥ã‚‚ tzãªã—ï¼ˆdateåŸºæº–ï¼‰ã«ã™ã‚‹
    today = pd.Timestamp.today().normalize()
    
    # æ®‹ã‚Šæ—¥æ•°
    df["æ®‹ã‚Šæ—¥æ•°"] = (
        (df["expiry_min_dt"] - today)
        .dt.days
        .astype("Int64")   # â† ã“ã‚Œ
    )


    def status(days):
        if pd.isna(days):
            return "æœªç™»éŒ²"
        if days < 0:
            return "æœŸé™åˆ‡ã‚Œ"
        if days <= 60:
            return "60æ—¥ä»¥å†…"
        return "ä½™è£•ã‚ã‚Š"

    df["çŠ¶æ…‹"] = df["æ®‹ã‚Šæ—¥æ•°"].apply(status)


    # =========================
    # ãƒ•ã‚£ãƒ«ã‚¿
    # =========================
    st.markdown("### " + LABEL["filters"])
    c1, c2, c3, c4 = st.columns([1.2, 1.0, 1.0, 0.8])

    with c1:
        kw = st.text_input(LABEL["keyword"], value="", key="expiry_kw")

    with c2:
        statuses = ["æœŸé™åˆ‡ã‚Œ", "60æ—¥ä»¥å†…", "ä½™è£•ã‚ã‚Š", "æœªç™»éŒ²"]
        default_status = ["æœŸé™åˆ‡ã‚Œ", "60æ—¥ä»¥å†…"]
        sel_status = st.multiselect(LABEL["status"], statuses, default=default_status, key="expiry_status")

    with c3:
        only_with = st.checkbox(LABEL["only_with_expiry"], value=False, key="expiry_only_with")
        only_no = st.checkbox(LABEL["only_no_expiry"], value=False, key="expiry_only_no")
        only_in_stock = st.checkbox("åœ¨åº«ã‚ã‚Šã®ã¿ï¼ˆåœ¨åº«0ã¯éè¡¨ç¤ºï¼‰", value=True, key="expiry_only_in_stock")
        only_zero_stock = st.checkbox("åœ¨åº«0ã®ã¿", value=False, key="expiry_only_zero_stock")
        
    with c4:
        limit = st.number_input(LABEL["limit"], min_value=50, max_value=5000, value=500, step=50, key="expiry_limit")

    df_view = df.copy()

    if kw:
        kw_s = kw.strip()
        # jan or name
        cond = df_view["jan"].str.contains(kw_s, na=False)
        if "name" in df_view.columns:
            cond = cond | df_view["name"].astype(str).str.contains(kw_s, na=False)
        df_view = df_view[cond]

    if sel_status:
        df_view = df_view[df_view["çŠ¶æ…‹"].isin(sel_status)]

    if only_with and not only_no:
        df_view = df_view[df_view["expiry_min_dt"].notna()]
    if only_no and not only_with:
        df_view = df_view[df_view["expiry_min_dt"].isna()]
    if only_zero_stock:
        df_view = df_view[df_view["stock_available"] <= 0]
    
    # åœ¨åº«ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šåœ¨åº«0ã‚’éè¡¨ç¤ºï¼‰
    if "stock_available" in df_view.columns:
        if st.session_state.get("expiry_only_in_stock", True) and not st.session_state.get("expiry_show_zero_stock", False):
            df_view = df_view[df_view["stock_available"] > 0]
    
        
    # =========================
    # è¡¨ç¤º
    # =========================
    df_view = df_view.sort_values(by=["expiry_min_dt", "jan"], ascending=[True, True])

    # è¡¨ç¤ºåˆ—ï¼ˆexpiry_1..5ã‚‚å‡ºã™ï¼‰
    cols = ["jan", "name", "stock_available", "expiry_min", "æ®‹ã‚Šæ—¥æ•°", "çŠ¶æ…‹",
            "expiry_1", "expiry_2", "expiry_3", "expiry_4", "expiry_5"]
    cols = [c for c in cols if c in df_view.columns]

    row_count = len(df_view)
    h_left, h_right = st.columns([1, 0.15])
    h_left.subheader(f"{LABEL['expiry']} / {LABEL['days']}")
    h_right.markdown(
        f"<h4 style='text-align:right; margin-top: 0.6em;'>{row_count:,}ä»¶</h4>",
        unsafe_allow_html=True
    )

    def highlight_status(row):
        if row["çŠ¶æ…‹"] == "æœŸé™åˆ‡ã‚Œ":
            return ["background-color: #ffcccc"] * len(row)
        if row["çŠ¶æ…‹"] == "60æ—¥ä»¥å†…":
            return ["background-color: #ffe599"] * len(row)
        return [""] * len(row)
    
    st.dataframe(
        df_view.head(int(limit))[cols].style.apply(highlight_status, axis=1),
        use_container_width=True
    )

    # CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    csv = df_view[cols].to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        LABEL["download"],
        data=csv,
        file_name="item_expiry_filtered.csv",
        mime="text/csv",
        key="expiry_download"
    )

    test_jan = "4901085632505"
    r = requests.get(
        f"{SUPABASE_URL}/rest/v1/warehouse_stock?select=jan,product_code,stock_available&jan=eq.{test_jan}",
        headers=HEADERS
    )
    st.write("DEBUG warehouse_stock", r.json())
    st.write("DEBUG SUPABASE_URL", SUPABASE_URL)

    st.write("DEBUG columns", df.columns.tolist())
    st.write(
        "DEBUG stock rows",
        df[df["jan"] == "4901085632505"][[
            c for c in df.columns if "stock" in c
        ]]
    )
    
